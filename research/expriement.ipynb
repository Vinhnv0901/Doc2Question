{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45786,
     "status": "ok",
     "timestamp": 1756904361413,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "1SUSc76IT0Kw",
    "outputId": "84f6ca80-96b4-48c0-c736-bf375e322c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install langchain langchain_community -q\n",
    "# #For API Calls\n",
    "# !pip install huggingface_hub -q\n",
    "# !pip install transformers -q\n",
    "# !pip install accelerate -q\n",
    "# !pip install  bitsandbytes -q\n",
    "# !pip install langchain-huggingface -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26429,
     "status": "ok",
     "timestamp": 1756904641250,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "nmJrhQLsqtOF",
    "outputId": "aa501051-ca0c-4ee7-e2dd-1d99f834f711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qBzmDC8T9QS"
   },
   "source": [
    "# set up env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8051,
     "status": "ok",
     "timestamp": 1756902855958,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "BIZecifuG_nc",
    "outputId": "e767abf0-526b-4c4c-8d12-b1f31302f678"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not token:\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEDRx8YeUQpC"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5289,
     "status": "ok",
     "timestamp": 1756902532140,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "6v5Mp6h4VPmA",
    "outputId": "de03c50f-f1c6-4838-8292-314d3ba1578f"
   },
   "outputs": [],
   "source": [
    "# !pip install PyPDF2 pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1756902663928,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "N94q_fkoG_nd"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1756898666120,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "oLc4wO09G_ne",
    "outputId": "a1f90d33-9418-4a74-c777-0673c54a7505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\d\\\\generative AI\\\\Doc2Question\\\\research'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kNpWjzKmG_ne",
    "outputId": "e017bf09-d2bf-42b0-9292-4ece50027ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\d\\generative AI\\Doc2Question\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3THCgy9dG_nf",
    "outputId": "7ab3921c-2160-4ff4-b95b-4aea7188fb16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\d\\\\generative AI\\\\Doc2Question'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1756902889186,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "mROeCFOlG_nf"
   },
   "outputs": [],
   "source": [
    "file_path = \"data/shrimp_docs.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 609,
     "status": "ok",
     "timestamp": 1756902895610,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "2cvjKwkDG_ng",
    "outputId": "6686edca-1100-4c4c-e820-1bb6d9c5c493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='Citation: Duan, H.; Wang, J.; Zhang,\\nY.; Wu, X.; Peng, T.; Liu, X.; Deng, D.\\nShrimp Larvae Counting Based on\\nImproved YOLOv5 Model with\\nRegional Segmentation. Sensors 2024,\\n24, 6328. https://doi.org/10.3390/\\ns24196328\\nAcademic Editor: Yongwha Chung\\nReceived: 10 September 2024\\nRevised: 26 September 2024\\nAccepted: 27 September 2024\\nPublished: 30 September 2024\\nCopyright: © 2024 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nsensors\\nArticle\\nShrimp Larvae Counting Based on Improved YOLOv5 Model\\nwith Regional Segmentation\\nHongchao Duan, Jun Wang, Yuan Zhang *, Xiangyu Wu, Tao Peng, Xuhao Liu and Delong Deng\\nCentre for Optical and Electromagnetic Research, South China Academy of Advanced Optoelectronics,\\nSouth China Normal University, Guangzhou 510006, China; 2021024069@m.scnu.edu.cn (H.D.);\\n2020023787@m.scnu.edu.cn (J.W.); 2022024121@m.scnu.edu.cn (X.W.); 2023024193@m.scnu.edu.cn (T.P .);\\n2023024174@m.scnu.edu.cn (D.D.)\\n* Correspondence: yuan.zhang@coer-scnu.org\\nAbstract: Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and\\nhigh density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely\\npacked shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through\\na regional segmentation approach. First, the C2f and convolutional block attention modules are used\\nto improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a\\nregional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp\\ncounter’s detection performance. Finally, a strategy for stitching and deduplication is implemented\\nto tackle the problem of double counting across various segments. The findings from the experiments\\nindicate that the suggested algorithm surpasses several other shrimp counting techniques in terms\\nof accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an\\naccuracy exceeding 98%.\\nKeywords: shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat\\nshrimp removal\\n1. Introduction\\nShrimp farming has become a vital economic sector within aquaculture, contributing\\nsignificantly to the growth of the fish industry [1]. Counting shrimp larvae is an essential\\ntask in the shrimp farming process, as it assists farmers in determining the reproductive\\nrate and accurately estimating the production potential [2]. In addition, it helps evaluate\\nfertility, control the density of cultivation, and manage transport sales [ 3,4]. However,\\ndue to the tiny size, great flexibility, and dense numbers of shrimp larvae, counting them\\npresents a significant challenge [5]. Currently, the process is predominantly carried out by\\nhand, making it a time-consuming, labor-intensive, and somewhat imprecise task [6]. Thus,\\ndeveloping an innovative approach to overcome the current difficulties in counting shrimp\\nlarvae is of the utmost importance.\\nThe progress in the counting of shrimp larvae consists primarily of manual count-\\ning [7], photoelectric detector counting [ 8–11], counting based on conventional image\\nprocessing [12], and deep learning-based counting [13]. The first technique is highly depen-\\ndent on the operator’s expertise, resulting in substantial differences in the accuracy and\\nefficiency of the counting process. In addition, the process of manual counting may harm\\nshrimp larvae. Photoelectric devices for counting shrimp larvae work by shining light on\\nthe larvae and determining their number based on the light reflected back to the sensors [8].\\nAlthough this method significantly improves both the speed and accuracy, these counters\\ncan be affected by various environmental factors, such as lighting conditions and water\\nquality, resulting in considerable counting errors. Furthermore, photoelectric devices are\\ngenerally effective for shrimp larvae of certain sizes, which reduces their accuracy when\\ndealing with larvae of different sizes.\\nSensors 2024, 24, 6328. https://doi.org/10.3390/s24196328 https://www.mdpi.com/journal/sensors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='Sensors 2024, 24, 6328 2 of 19\\nRecently, computer vision technology has been applied in various fields [ 14], such\\nas target detection, image super-resolution, and population counting [15]. Regarding the\\ncounting of shrimp larvae [16–19], traditional image processing techniques primarily utilize\\nimage segmentation and object detection methods to recognize and count target images.\\nKesvarakul et al. counted shrimp larvae in images by converting them into binary images\\nwith a threshold [20]. However, this technique is only effective when dealing with a small\\nquantity of larvae and when their images are clear. Thai et al. used image segmentation\\nand contour tracking methods for the counting of shrimp larvae [21]. This method is able\\nto count individual shrimp larvae, although it struggles to accurately distinguish shrimp\\nlarvae which are stuck together. For populations with fewer shrimp larvae, counting was\\nperformed using algorithms, such as segmentation paired with the Canny edge detection\\nmethod and blob processing approaches [22]. Although this technique offers an enhance-\\nment over previous algorithms, it still does not address the issue of shrimp and larvae\\nadhering to each other. Traditional image processing-based counting techniques require\\nprior knowledge to manually adjust image features, leading to poor accuracy in scenarios\\nwith complex backgrounds. Furthermore, the lack of generalization in these models hinders\\nthe accurate detection and counting of shrimp larvae in different scenarios.\\nSubsequently, deep learning-based [23–26] counting methods use trained object recog-\\nnition models to identify targets within images. Armalivia et al. used the You Only Look\\nOnce version 3 (YOLOv3) algorithm to count shrimp larvae [ 27], achieving an average\\ncounting accuracy of 96% in low-density populations of around 100 larvae (in a circular\\narea with a diameter of 40 cm), but a larger number of larvae could not be identified. Hu\\net al. used deep learning and a density map of shrimp larvae to estimate the populations\\nof approximately 1000 larvae (in a 24 cm × 33 cm area) [ 13]. However, this approach\\ndoes not accurately identify each individual larva. Hence, devising an automated and\\naccurate method for counting shrimp larvae in densely populated areas remains a difficult\\nchallenge. Object detection algorithms based on deep learning mainly include Regions\\nwith Convolutional Neural Networks (R-CNN) and multiple versions of YOLO, among\\nwhich YOLOv5 is a mature algorithm among target recognition algorithms which has\\nthe advantages of being lightweight and having a fast inference ability. However, in the\\ndetection and recognition of small targets, YOLOv5 is not effective, and thus we need to\\nimprove YOLOv5.\\nIn this study, we investigate the enhancement of deep learning techniques to address\\nthe challenge of counting shrimp larvae in highly populated environments. In particular,\\nwe propose an algorithm which uses regional segmentation along with an improved\\nYOLOv5 model to accurately count shrimp larvae, especially in highly dense environments.\\nThe remainder of this paper is structured as follows. In Section 2, we give the related\\nwork to highlight our motivation. In Section 3, we describe the details of the proposed\\nshrimp larvae counting method. Section 4 presents the experimental results and further\\nvalidates the superiority of the algorithm through controlled experiments. Section 5 gives\\nour conclusions.\\nThe main contributions are described as follows:\\n• W e build an automatic shrimp collecting platform to obtain the corresponding dataset and\\nprovide a counting algorithm based on improved YOLOv5 with region segmentation.\\n• The C2f and attention mechanism are embedded in the YOLOv5 model, improving\\nthe detection ability for small shrimp.\\n• The segmentation of regions aims to boost the proportion of pixels representing shrimp\\nlarvae in a visual image.\\n• Experimental results prove that the proposed algorithm can identify shrimp larvae\\nwith high accuracy under the difficult circumstance of overlap between large numbers\\nof shrimp and different light intensities.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='Sensors 2024, 24, 6328 3 of 19\\n2. Related Work\\nThere are many challenges in the counting of shrimp larvae. First, shrimp larvae are\\nsmall in size, making them relatively difficult to detect. In addition, shrimp larvae move\\nrapidly, and their pixel values in images differ depending on their depth in the water.\\nConventional threshold segmentation techniques may easily overlook shrimp larvae which\\nare at the bottom of the water.\\nAs shown in the Figure 1, there are three states, including isolated, clumped, and\\noverlapping shrimp larvae. Isolated shrimp larvae remain unaffected by the presence\\nof other larvae and can be detected through conventional image processing approaches\\nor deep learning methods. Clumped shrimp larvae consist of several larvae attached\\nto each other, resulting in an underestimated count when using connected component\\nanalysis. This problem can be resolved through the application of watershed algorithms\\nand distance transformation. However, these techniques necessitate precise threshold\\nconfigurations and do not generalize well. Overlapping shrimp larvae appear when larvae\\nat varying water depths align at the same spot in the image. The aforementioned watershed\\nalgorithm is ineffective in addressing this scenario. Utilizing concave point algorithms\\nfor separation might erroneously fragment the shrimp tails, causing overcounting of the\\nshrimp. Consequently, conventional image processing techniques prove inadequate for\\nidentifying these instances, necessitating the use of deep learning approaches to accurately\\ndiscern and identify the various states of shrimp larvae.\\nFigure 1. The states of shrimp larvae. ( a) Isolated shrimp larvae. ( b) Clumped shrimp larvae.\\n(c) Overlapping shrimp larvae.\\nMinimizing the dimensions of the input image can enhance the accuracy of receptive\\nfield detection [28]. Two widely used techniques for decreasing the size of an image are\\nimage scaling and cropping. Image scaling often employs downsampling [29], which may\\nlead to data loss. Another approach is image segmentation, which can be performed either\\nrandomly or in a consistent manner. Random segmentation employs random functions\\nto determine the coordinates for the top-left and bottom-left sections of the local image\\nof shrimp larvae [30]. Then, the coordinates for the top-right and bottom-right sections\\nare determined according to the dimensions of the local image. In addition, the cropping\\nfunction is applied to extract the local image. Beginning in the upper-left corner of the\\noriginal image, fixed segmentation continues with sequential segmentation based on the\\ndimensions of the local image. For regions along the edges which are smaller than the\\nsegmented dimensions, the true size of the image is maintained. Random segmentation\\nmight result in repeated recognition of the same regions, thereby increasing the training\\nburden on the model. Consequently, this research uses fixed segmentation to handle the\\ninitial images.\\nThe estimation of density maps is also a commonly used counting method [23] which\\nestimates the number of objects by generating a density map. This is often applied in\\ncrowd counting [31]. However, this method cannot accurately provide the geometric and\\npositional information of objects. When the detection of small objects does not require\\npositional information, the method can provide good counting results. In shrimp larvae\\ncounting, when the larvae density is relatively low, positional and shape information may\\nnot be critical, and both density map estimation and object detection can achieve accurate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='Sensors 2024, 24, 6328 4 of 19\\ncounts. However, as the larvae density increases, with overlapping larvae at different water\\ndepths, positional and shape information become important. In high-density areas, density\\nmap estimation is prone to misjudgments, leading to a decrease in count accuracy. By\\nincorporating position and shape information, object detection methods can achieve better\\ncounting accuracy. In terms of runtime, density map estimation models are more complex\\nand require a longer processing time, while object detection methods are faster, making\\nthem more suitable for quick shrimp larvae counting.\\nYOLO [32] is considered one of the best choices for object detection models due to its\\nspeed and superior accuracy compared with R-CNN [33]. YOLO pioneered an innovative\\napproach by utilizing convolutional neural networks on a complete image to predict object\\nclasses and bounding boxes via direct feature regression. Over the years, YOLO has evolved\\ninto several versions. YOLOv1 uses a unified detection approach for object localization\\nand classification tasks [ 32]. Subsequently, YOLOv2 improves on version 1 by having\\nbetter accuracy, a faster speed, and the ability to recognize more objects [ 34]. YOLOv3\\nimproves the object detection speed by implementing multiscale prediction [35], optimizing\\nthe core network, and refining the loss function. YOLOv4 introduces a fast and effective\\nobject detection model which substantially cuts down computational expenses, making\\nit more compatible with general-purpose devices and those with hardware constraints.\\nYOLOv5 brings significant advancements, including the CSPDarknet backbone and mosaic\\naugmentation, balancing speed and accuracy [ 36–42]. YOLOv7 improves the structure\\nof the extended efficient layer aggregation network, revises the model architecture, and\\nintroduces an efficient label assignment strategy, which increases detection performance\\nwhile decreasing the number of model parameters [43]. Currently, researchers are constantly\\nexploring the architectural design of YOLO, with the latest version being YOLOv10 [44]. It\\nhas achieved state-of-the-art performance and efficiency on various model scales. In the\\nexperimental section, we describe the performance of YOLOv10 and conduct shrimp larvae\\ncounting tests using this model. However, the YOLOv10 model is complex and has high\\nhardware resource requirements, making its hardware deployment challenging. One of\\nthe primary objectives of this study is to deploy the algorithm on hardware to achieve an\\nintegrated system for shrimp larvae image acquisition and counting. YOLOv10 does not\\nmeet the requirements of our design. YOLOv5 stands out from its predecessors with its\\nlighter architecture, faster inference times, and more developed ecosystem which offers\\nenhanced compatibility [45]. Moreover, YOLOv5 has low hardware resource requirements,\\nis relatively mature in terms of hardware deployment, and is easier to implement. Therefore,\\nwe selected YOLOv5 as the baseline for our study. However, in scenarios which involve\\nthe detection of small objects, such as shrimp larvae, the use of several convolutional layers\\nmay lead to missing small targets, resulting in reduced pixel occupancy for small objects. To\\naddress this problem, we upgraded the YOLOv5 backbone network to improve its ability\\nto identify small targets.\\n3. Materials and Methods\\nFor a more precise counting of shrimp larvae in situations with large numbers and a\\nhigh density, we suggest an algorithm based primarily on regional segmentation and an\\nimproved YOLOv5 model. The flow chart of the algorithm can be viewed in Figure 2.\\nTo begin with, we divide the whole original image into smaller segments using a\\nregion-based segmentation algorithm. Subsequently, we employ an enhanced YOLOv5\\nmodel to recognize shrimp larvae in each of these isolated images separately. Then, all\\nsegmented image blocks are reassembled into a complete image. A repeat count removal\\nmethod is proposed to address the issue of repeat counting in the stitching position,\\nensuring that accurate counting of shrimp larvae can be achieved throughout the image.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='Sensors 2024, 24, 6328 5 of 19\\nFigure 2. The flow chart of the proposed counting algorithm.\\n3.1. Image Collection\\nThe quality of the original images, which is one of the key factors for computer vision,\\nwill affect the accuracy of the counting of shrimp larvae. For the best accuracy in counting\\nthe larvae, we designed and fabricated a shrimp larvae image acquisition device as shown\\nin Figure 3.\\nShrimp larvae are placed in a semi-transparent plastic square bucket, and white LEDs\\nwith light guide plates are added around the side wall and the bottom to provide uniform\\nillumination. Shrimp larvae images are captured using an industrial camera (MV-CE060-\\n10UM (Hikvision, Hangzhou, China)) with an 8 mm lens under supplementary white light.\\nThe resolution of the camera is 3072× 2048 pixels for a field of view of about40 cm × 30 cm,\\nand the effective area we used for the shrimp larvae was 30 cm × 30 cm.\\nFigure 3. Shrimp larvae acquisition device. ( a) Schematic diagram. ( b) Photographs of the\\nactual device.\\nThe shrimp larvae dataset used in this experiment was collected from on-site photog-\\nraphy at the Hongkai Shrimp Larvae Farm in Zhuhai City, Guangdong Province, China.\\nFigure 4a shows the real shrimp larvae farming environment. The staff used a fishing net to\\nextract shrimp larvae from the pool (as shown in Figure 4b) and then placed the collected\\nlarvae in our custom-built shrimp larvae image acquisition device as shown in Figure 4c.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='Sensors 2024, 24, 6328 6 of 19\\nFigure 4. Acquisition of shrimp larvae datasets in a real aquaculture scenario. ( a) Shrimp culture\\ntank. (b) Shrimp larvae collection. (c) Acquisition device.\\n3.2. Region Segmentation\\nThe receptive field refers to the size of the region in the input image which corresponds\\nto each pixel in the output feature map [ 46]; that is, a single point on the feature map\\ncorresponds to a specific region on the input image. The size of the area of the receptive\\nfield directly affects the detection accuracy [47,48]. The formula for calculating the receptive\\nfield is as follows:\\nRi = (Ri+1 − 1) × Sti + Ksi (1)\\nwhere Ri represents the receptive field on the ith convolutional layer, Ri+1 is the receptive\\nfield on the i + 1th layer, Sti is the stride of the convolution, and KS is the size of the\\nconvolutional kernel for the current layer.\\nWhen the original image of shrimp larvae ( 3072 × 2048 pixels) is used as the input\\nimage, the corresponding receptive field size is38 ×38 pixels. This indicates that each point\\non the feature map corresponds to a area of 38 × 38 pixels on the original image, which is\\nsignificantly larger than the size of the shrimp larvae (approximately 10 × 10 pixels). As a\\nresult, information on shrimp larvae would be overlooked or covered, greatly reducing the\\ndetection accuracy.\\nTo address this issue, it is necessary to reduce the size of the receptive field to improve\\nthe shrimp larvae detection performance. Therefore, we adopted the region segmentation\\nmethod to divide the original image, reducing the size of the input image and thereby\\ndecreasing the receptive field area. For example, the original image of shrimp larvae (as\\nshown in Figure 5a) was divided into multiple image blocks (100 × 100 pixels for each, as\\nshown in Figure 5b) for subsequent labeling and training.\\nIn the whole image, the proportion of shrimp larvae in terms of pixels in the whole\\nimage is extremely small. Therefore, we used segmentation technology to obtain multiple\\nlocal regions of shrimp larvae, making feature extraction easier.\\nFigure 6 shows the feature maps selected from the 17th layer of the YOLOv5 model\\nfor both the full image and the local image. To facilitate comparison, we extracted the same\\nregion from the full image as that in the local image. From Figure 6, it is evident that the\\nfull image did not accurately identify local regions, while the local image allowed a more\\nprecise extraction of features from the shrimp larvae.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='Sensors 2024, 24, 6328 7 of 19\\nFigure 5. Image segmentation. (a) Original shrimp larvae image. (b) Segmented image.\\nFigure 6. The feature map referring to the shrimp larvae image.\\n3.3. The Improved YOLOv5\\nThe core idea of YOLOv5 is to convert the target detection problem into a regression\\nproblem, predicting the boundary boundaries and categories of the target in the image.\\nIt has high accuracy and a fast inference ability, making it one of the best-performing\\ntarget detection models available today. However, the tiny size of the shrimp larvae, along\\nwith various body positions such as their overlap, leads to a decrease in the identification\\naccuracy of the YOLOv5 model. Therefore, the recognition accuracy can be improved by\\nincreasing the classification performance of features.\\nAs shown in Figure 7, the C2f module (Figure 8) was used to replace the C3 module. In\\nthe backbone network of the YOLOv5 algorithm, the main function of the C3 module is to\\nextract image features and enhance the learning capacity of the convolutional network. The\\nC3 module cannot meet the requirements for the detection of small targets such as shrimp\\nlarvae and needs further improvement to enhance the feature extraction capabilities of the\\nmodel. The C2f module processes the input data using two convolutional layers, which\\nassist in extracting features at varying levels and degrees of abstraction. This improves the\\nfeature extraction efficiency and simultaneously reduces the network weight, facilitating\\nmore abundant gradient flow information [ 49]. In contrast to the C3 module, the C2f\\nmodule is more lightweight, has reduced computational demands, and demonstrates\\nrobust feature extraction capabilities.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='Sensors 2024, 24, 6328 8 of 19\\nFigure 7. The structure of the improved YOLOv5 model.\\nFigure 8. Structure of the C2f network.\\nTo further enhance the recognition capability of the model, the convolutional block\\nattention module (CBAM) [ 50] is introduced into the YOLOv5 framework. The CBAM\\nconsists of the channel attention and spatial attention modules, as shown in Figure 9. For\\nan input F, the global average and maximum pooling operations are first applied to obtain\\nglobal information for each channel. Subsequently, two fully connected layers are utilized\\nto produce the channel attention vector. These layers apply weights to the input feature F\\nper channel, which yields the refined feature F1. In the spatial attention module, the feature\\nmap is combined to capture spatial information. Furthermore, the feature F1 undergoes a\\nconvolution layer 3 × 3, resulting in the creation of a spatial attention map via the sigmoid\\nfunction. The spatial attention map is subsequently used to enhance the feature F1, leading\\nto production of the fused feature F2. The attention module improves the depiction of\\nshrimp larvae features across different conditions.\\nFigure 10 displays the feature map of both the enhanced YOLOv5 network and the\\noriginal YOLOv5 network. Clearly, the proposed approach provides more precise counting\\noutcomes even in instances of densely packed shrimp larvae.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='Sensors 2024, 24, 6328 9 of 19\\nFigure 9. Structure illustration of CBAM.\\nFigure 10. Feature map comparison. ( a) The original YOLOv5 network. ( b) The improved\\nYOLOv5 network.\\n3.4. Repeat Count Removal via Stitching\\nThe method described in Section 3.2 allows for segmentation of the entire image. The\\nmethod described in Section 3.3 can identify shrimp larvae in each segmented image block.\\nHowever, identical shrimp larvae can be segmented into neighboring image blocks and\\ndetected at the same time (in Figure 11), resulting in reduced counting accuracy. To address\\nthe issue of duplicate counts, an additional detection model was developed.\\nFigure 11. Segmentation of same shrimp larvae in adjacent image blocks (dashed lines indicate the\\nstitching positions). (a) Vertical segmentation. (b) Horizontal segmentation. (c) Both horizontal and\\nvertical segmentation.\\nFigure 11a shows a shrimp larva divided vertically into two sections and identified\\nin two separate subimages. Figure 11b presents two shrimp larvae segmented horizon-\\ntally into two subimages and identified. Figure 11c shows shrimp larvae cut horizontally\\nand vertically, resulting in four distinct subimages. Figure 12 shows a schematic illustra-\\ntion of the segmentation of shrimp larvae, helping in the evaluation of the segmentation\\nconfigurations.\\nIn Figure 12, the red boxes represent the detection boundary boxes of the detection\\noutput, and the black lines represent the cropping boundaries of the image. Taking into'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='Sensors 2024, 24, 6328 10 of 19\\naccount the spatial relationship between the detection boundary boxes and the image\\ncropping boundaries in the output, nine distinct scenarios can be identified: undivided\\nshrimp larvae (N), top edge touching (U), bottom edge touching (D), right edge touching\\n(R), left edge touching (L), right and bottom edges touching (RD), right and top edges\\ntouching (RU), left and bottom edges touching(LD), and left and top edges touching(LU).\\nA set S, where S = {N, D, U, R, L, RD, RU, LD, LU}, was defined to store all detection\\noutput. By traversing the detection outputs and checking the boundaries, the detection\\nresults were placed in the corresponding sets.\\nFigure 12. Schematic diagram of shrimp larvae segmentation. (a) Vertical segmentation. (b) Horizon-\\ntal segmentation. (c) Both horizontal and vertical segmentation.\\nTo avoid duplication, we examined the shrimp larva detection boxes in neighboring\\nregions to determine whether the detected shrimp larvae were identical. If several detection\\nboxes corresponded to each other, as illustrated in Figure 13, then(xi, yi) are the coordinates\\nof the vertices of the detection frame, and the detection box at the bottom which intersects\\nthe cropping boundary coincides with the two detection boxes at the top, which also\\nintersect with the cropping boundary. Therefore, an overlap ratio must be implemented\\nto facilitate the selection process. We define a variable Ro to represent the percentage of\\noverlap between detection frames, whose formula is as follows:\\nRo = x4 − x1\\nx2 − x3\\n(2)\\nwhere x1, x2, x3, x4 are the horizontal coordinates of the detection frame.\\nFigure 13. Multiple detection box matching.\\nBy calculating the overlap ratio, the detection box with the highest overlap ratio\\nis selected. After determining the matching object, a minimum bounding rectangle is\\nused to completely enclose the two detection boxes, thus identifying the deduplication\\narea (indicated in Figure 14a, for example). The two red detection boxes in Figure 14b'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='Sensors 2024, 24, 6328 11 of 19\\ncorrespond to a typical repeat count case (according to Figure 11b). We determined whether\\nthey were connected by examining the coordinates of the two detection boxes. Once the\\nconnected boxes were identified, we used a minimum enclosing box (indicated by the green\\nrectangle in Figure 14a,c) to completely cover the two red detection boxes and define the\\ndeduplication area.\\nFigure 14. The deduplication area. (a) Schematic diagram of how to find the deduplication area (the\\ndashed line indicates region segmentation positions), where the two red rectangles are the detection boxes\\nand the corresponding minimum enclosing rectangle is drawn in green. (b) Original detection result with\\ntwo detection boxes detecting the same larva. (c) Minimum enclosing rectangle for the case in (b).\\nOnce the deduplication area is identified, it is fed back into the enhanced YOLOv5\\nnetwork mentioned in Section 3.3 to accurately count the shrimp larvae in this region. This\\nprocess ensures that duplicate counts are removed from the entire composite image of\\nshrimp larvae.\\n4. Experimental Results and Analysis\\n4.1. Implementation Details\\nIn the experiment, we collected 20,000 valid local images of shrimp larvae, with\\n15,000 allocated for training and 5000 for testing. For annotating the dataset, the conven-\\ntional labeling approach (Figure 15a) involves describing all the shrimp larvae, yet this\\ncan introduce redundant details and impede the model’s ability to learn the features of\\nshrimp larvae. We marked the heads of the shrimp larvae as shown in Figure 15b. This\\nannotation method concentrates on the feature of the shrimp larvae, making it easier to\\ndistinguish overlapping larvae. The comparison algorithms included the YOLOv5 algo-\\nrithm (which means using YOLOv5 without regional segmentation), the Mask R-CNN\\nalgorithm [17], and popular commercial software (Smart Shrimp Farm V2.1.3 [ 51]). In\\naddition, the proposed algorithm was performed on an NVIDIA RTX 3080Ti (NVIDIA,\\nSanta Clara, CA, USA). A total of 500 epochs were trained, with the learning rate set to\\n0.001. The other parameters were the default YOLOv5 parameters. The optimizer was the\\nstochastic gradient descent.\\nFigure 15. Dataset annotation. (a) The traditional labeling method. (b) The annotation methodwe used.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='Sensors 2024, 24, 6328 12 of 19\\nWe used images of shrimp larvae of different densities to evaluate the performance\\nof the proposed method. Under the fixed field of view provided (30 cm × 30 cm), we\\nexamined four different densities of shrimp larvae: approximately 1000 larvae for density\\nlevel 1, approximately 2300 larvae for density level 2, approximately 4000 larvae for density\\nlevel 3, and roughly 5000 larvae for density level 4. We chose 100 × 100 pixels for the\\nsegmentation image size in the counting experiments hereafter. The dataset and codes can\\nbe requested by email (Hongchao Duan: 2021024069@m.scnu.edu.cn).\\n4.2. Evaluation Criterion\\nTo quantitatively evaluate the performance of the proposed algorithm, we define A as\\nthe measurement for the counting accuracy of the formula as follows:\\nA = Ac − |Id − Ac|\\nAc\\n× 100% (3)\\nwhere Ac represents the actual number of shrimp larvae and Id denotes the number of\\nshrimp larvae.\\nFor accurate results, 15 images of shrimp larvae were analyzed at each density level,\\nwith the mean count accuracy serving as the statistical result for each level.\\n4.3. Experiment Comparison\\nTo verify the effectiveness of the above proposed method, we collected a large number\\nof shrimp images of different densities (four densities mentioned in Section 4.1) for the\\nvalidation experiment. Figure 16a–d shows the recognition effect of the proposed method\\nunder different densities. It can be seen in Figure 16 that the proposed algorithm had a\\ngood recognition effect for different densities, especially in places where shrimp larvae\\nwere densely populated.\\nFigure 16. Counting results at different densities. ( a) Density 1. ( b) Density 2. ( c) Density 3.\\n(d) Density 4.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='Sensors 2024, 24, 6328 13 of 19\\nFigure 17a shows a typical local image of the original high-density shrimp larvae\\n(approximately 5000 shrimp larvae in a 30 cm × 30 cm area), and Figure 17b shows the\\ncorresponding identification results. It shows that the proposed method exhibited high\\nrecognition accuracy even when the density of the shrimp larvae was high and there were\\noverlaps between larvae. This indicates that the regional segmentation approach adopted\\nin this study makes the receptive field more accurate, which makes the improved YOLOv5\\nmodel more accurate in the recognition of shrimp larvae targets.\\nFigure 17. A typical local area of Figure 16. (a) Original image. (b) Detection results.\\nAmong the various comparison algorithms, Smart Shrimp Farm only provided the\\nfinal count results without the detection images. We tested high-density shrimp larvae\\nimages (approximately 5000 shrimp larvae in a 30 cm × 30 cm area) using the remaining\\nalgorithms, and the comparison of the test results is shown in Figure 18. For Density Map\\nRegression, we provide the corresponding density map in Figure 18f.\\nFigure 18. Test result images for different algorithms. ( a) The proposed method. ( b) YOLOv5.\\n(c) Mask R-CNN. (d) YOLOv10. (e) Shrimpseed_Net. (f) Density map regression.\\nTable 1 presents the statistical results of the average counting accuracy A for the\\ncomparison algorithm.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='Sensors 2024, 24, 6328 14 of 19\\nTable 1. Average counting accuracy A for different shrimp larvae densities.\\nMethod Density 1 Density 2 Density 3 Density 4\\nThe Proposed Method 99.32 % 99.17% 98.53% 98.23%\\nSmart Shrimp Farm [51] 91.43% 90.35% 88.25% 82.99%\\nYOLOv5 [52] 87.79% 85.46% 83.66% 81.03%\\nMask R-CNN [17] 86.63% 84.52% 81.25% 80.32%\\nYOLOv10 [44] 91.41% 90.68% 89.29% 88.75%\\nShrimpseed_Net [53] 91.65% 90.23% 89.47% 87.62%\\nDensity Map Regression [23] 92.33% 91.58% 90.75% 88.61%\\nThe results clearly indicate that, compared with commercial software such as YOLOv5,\\nYOLOv10, Shrimpseed_Net, Density Map Regression, and Mask R-CNN, the proposed\\nalgorithm excelled in counting shrimp larvae at various densities, particularly in high-\\ndensity scenarios. Although the accuracy decreased somewhat as the density increased, the\\noverall recognition accuracy of the proposed method remained above 98%, significantly\\noutperforming the other three cases (just above 80%). Moreover, the accuracy, which\\nexceeded 98%, demonstrates that the proposed algorithm could potentially substitute the\\nmanual shrimp counting process on farms.\\n4.4. Ablation Study\\nTo validate the superiority of the improved YOLOv5 model proposed in this article,\\nwe compared the identification results in the segmentation images of100 × 100 pixels using\\nboth the original YOLOv5 model and the improved YOLOv5 model in Section 3.3 (typical\\nresults are shown in Figure 19).\\nFigure 19. Recognition performance of different algorithm models. ( a) Original YOLOv5 model.\\n(b) Improved YOLOv5 model.\\nAs shown in Figure 19, the improved YOLOv5 model demonstrated better recognition\\nperformance when the shrimp larvae were densely packed. To quantitatively analyze the\\ndifferences between the two models, we used the counting algorithm on all images, and\\nthe results of the average counting accuracy for the four density levels are summarized in\\nTable 2. For density 4, we collected images of shrimp larvae under three different lighting\\nconditions (bright, normal brightness, and dim) for testing. The test results are shown in\\nFigure 20, and the data are summarized in Table 2.\\nThe statistical results in Table 2 show that the improved YOLOv5 model achieved an\\napproximately 2% higher counting accuracy compared with the original YOLOv5 model,\\nvalidating the effectiveness of the proposed method. Under different lighting conditions,\\nthe counting accuracy of our proposed algorithm exceeded 98%, outperforming the original\\nYOLOv5 algorithm.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='Sensors 2024, 24, 6328 15 of 19\\nFigure 20. Test results under three different lighting conditions. (a) Bright. (b) Normal brightness.\\n(c) Dim.\\nTable 2. Average counting accuracy A for the two YOLOv5 models.\\nDensity The Original YOLOv5 The Improved YOLOv5\\nDensity 1 98.13% 99.32%\\nDensity 2 97.85% 99.17%\\nDensity 3 96.81% 98.53%\\nDensity 4 (Bright) 96.26% 98.23%\\nDensity 4 (Normal Brightness) 94.66% 98.21%\\nDensity 4 (Dim) 93.52% 98.17%\\nThe original image of the larvae was divided into many local areas to decrease the size\\nof the receptive field in Section 3.2. To evaluate the impact of segmentation image sizes on\\nthe accuracy of shrimp larvae detection, we cropped the original shrimp larvae images into\\nfive different sizes: 600 × 600 pixels, 300 × 300 pixels, 200 × 200 pixels, 100 × 100 pixels,\\nand 50 × 50 pixels (as shown in Figure 21). Subsequently, we used the improved YOLOv5\\nmodel to train and test these cropped images of different sizes separately.\\nFigure 21. Recognition results for different segmentation image sizes: ( a) 600 × 600 pixels,\\n(b) 300 × 3000 pixels, (c) 200 × 200 pixels, (d) 100 × 100 pixels, and (e) 50 × 50 pixels.\\nAs shown in Figure 21, when the segmentation image size was 600 × 600 pixels, many\\nshrimp larvae were not identified. As the segmentation size decreased, the number of\\nundetected shrimp larvae also decreased. When the segmentation image size was reduced'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16'}, page_content='Sensors 2024, 24, 6328 16 of 19\\nto 100 × 100 pixels and 50 × 50 pixels, all shrimp larvae in the local images were detected.\\nTo quantitatively analyze the differences between the various segmentation image sizes,\\nwe tested the counting accuracy using images with four different densities of shrimp larvae\\nmentioned in Section 4.1, and the statistical results of the average counting accuracy A for\\neach level of density of shrimp larvae are presented in Table 3.\\nTable 3. Average counting accuracy A for different segmentation sizes.\\nDensity 600 × 600\\nPixels\\n300 × 300\\nPixels\\n200 × 200\\nPixels\\n100 × 100\\nPixels 50 × 50 Pixels\\nDensity 1 91.69% 94.27% 97.79% 99.32% 99.34%\\nDensity 2 90.83% 93.85% 97.24% 99.17% 99.20%\\nDensity 3 88.72% 93.31% 96.38% 98.53% 98.87%\\nDensity 4 88.15% 92.56% 95.93% 98.23% 98.21%\\nThe results indicate that the size of the image segmentation had a significant impact on\\nthe accuracy of the counting of shrimp larvae. As the segmentation image size decreased,\\nthe accuracy of the shrimp larvae count clearly increased. When the segmentation image\\nsize was reduced to 50 × 50 pixels, the counting accuracy was the best, but we also realize\\nthat the cost of computing increased exponentially. Considering the calculation cost and\\nthe counting accuracy, we chose 100 × 100 pixels for the segmentation image size in the\\ncounting experiments hereafter.\\n4.5. Discussion\\nComparative experiments showed that the proposed region segmentation algorithm\\nimproved the accuracy of counting shrimp larvae. In contrast to the standard YOLOv5\\nmodel, the enhanced YOLOv5 variant provides more accurate detection of small objects\\nsuch as shrimp larvae. Validation experiments indicated that the proposed algorithm\\nperformed better, with a counting accuracy of 98% even under conditions with high\\ndensities and large volumes of shrimp larvae. However, there are still aspects which need\\nto be improved. To begin with, the dataset for this experiment was gathered using an\\noptical platform which we designed. To improve the generalizability of the algorithm,\\nit is essential to include images of shrimp larvae obtained from various environments\\nand a range of imaging devices in the training dataset. In addition, the stitching and\\ndeduplication process occasionally overlooks a few shrimp larvae due to stitching errors,\\nsuggesting that this algorithm needs further improvement. In conclusion, the proposed\\nalgorithm can be integrated into shrimp larvae imaging devices to develop a comprehensive\\nshrimp larvae counting system.\\nThe image segmentation approach and the improved YOLOv5 model proposed in\\nthis paper not only solve the problem of shrimp larvae counting but also have significant\\nresearch potential in various fields, such as dense crowd counting and medical image\\nanalysis. Using segmentation algorithms, the proportion of the pixel of the target objects\\nin the input images increased, thereby improving the detection accuracy. However, the\\nalgorithm needs to be adjusted according to the specific problem at hand.\\n5. Conclusions\\nWe introduced a method for counting shrimp larvae which uses region segmentation\\nalong with an improved YOLOv5 model. By segmenting the regions, we can reduce the\\nreceptive field area, thereby enhancing the detection accuracy of shrimp larvae using the\\nimproved YOLOv5 model. Furthermore, the deduplication model can tackle the problem\\nof repeated counts. Based on this, the experimental results show that the number of larvae\\nwas about 5000, and the counting accuracy of our algorithm remained above 98%, which\\nwould be suitable for replacing the work of manually counting shrimp larvae.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='Sensors 2024, 24, 6328 17 of 19\\nAuthor Contributions: Conceptualization, Y.Z.; methodology, Y.Z., J.W. and H.D.; software, J.W. and\\nH.D.; validation, X.W., T.P ., X.L. and D.D.; formal analysis, J.W. and H.D.; investigation, J.W., X.W.\\nand H.D.; resources, T.P .; data curation, D.D.; writing—original draft preparation, H.D.; writing—\\nreview and editing, Y.Z.; visualization, Y.Z.; supervision, Y.Z.; project administration, Y.Z.; funding\\nacquisition, Y.Z. All authors have read and agreed to the published version of the manuscript.\\nFunding: This research received no external funding.\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: Data are available from the authors on request.\\nConflicts of Interest: The authors declare no conflicts of interest.\\nReferences\\n1. Racotta, I.S.; Palacios, E.; Ibarra, A.M. Shrimp larval quality in relation to broodstock condition. Aquaculture 2003, 227, 107–130.\\n[CrossRef]\\n2. Li, D.; Miao, Z.; Peng, F.; Wang, L.; Hao, Y.; Wang, Z.; Chen, T.; Li, H.; Zheng, Y. Automatic counting methods in aquaculture: A\\nreview. J. World Aquac. Soc.2021, 52, 269–283. [CrossRef]\\n3. Tierney, T.W.; Fleckenstein, L.J.; Ray, A.J. The effects of density and artificial substrate on intensive shrimp Litopenaeus vannamei\\nnursery production. Aquac. Eng.2020, 89, 102063. [CrossRef]\\n4. Naegel, L.C.; Gómez-Humarán, I.M. Effect of sample volume and population density on precision of larval population estimates.\\nAquac. Eng.1998, 17, 11–19. [CrossRef]\\n5. Hsieh, Y.K.; Hsieh, J.W.; Hu, W.C.; Tseng, Y.C. AIoT-Based Shrimp Larvae Counting System Using Scaled Multilayer Feature\\nFusion Network. IEEE Internet Things J.2024, early Access. [CrossRef]\\n6. Zhang, L.; Li, W.; Liu, C.; Zhou, X.; Duan, Q. Automatic fish counting method using image density grading and local regression.\\nComput. Electron. Agric.2020, 179, 105844. [CrossRef]\\n7. Yeh, C.T.; Ling, M.S. Portable Device for Ornamental Shrimp Counting Using Unsupervised Machine Learning. Sens. Mater.\\n2021, 33, 3027–3036. [CrossRef]\\n8. Martinez-Palacios, C.; Novoa, M.O.; Chavez-Martinez, C. A simple apparatus for self-separation of post-larval prawns,\\nMacrobrachium spp., in mixed larval rearing tanks. Aquac. Res.1985, 16, 341–348. [CrossRef]\\n9. Work, H.P .S. Photoelectric Sensors for Counting and Classifying Vehicles. InTransportation Research Record 1311; Transportation\\nResearch Board: Washington, DC, USA, 1991; p. 79.\\n10. Carmichael, H. Photoelectric Detection II. In An Open Systems Approach to Quantum Optics: Lectures Presented at the Université Libre\\nde Bruxelles October 28 to November 4, 1991; Springer: Berlin/Heidelberg, Germany, 1993; pp. 93–112.\\n11. Spratt, M.D. Preliminary results of a computer imaging system applied to estimating the quantity of larvae and fingerling fish for\\naquaculture. In Fish Quality Control by Computer Vision; Routledge: New York, NY, USA, 2017; pp. 263–282.\\n12. Awalludin, E.; Muhammad, W.W.; Arsad, T.; Yussof, W.H.W. Fish larvae counting system using image processing techniques. In\\nProceedings of the Journal of Physics Conference Series; IOP Publishing: Bandung, Indonesia, 2020; Volume 1529, p. 052040.\\n13. Hu, W.C.; Chen, L.B.; Hsieh, M.H.; Ting, Y.K. A deep-learning-based fast counting methodology using density estimation for\\ncounting shrimp larvae. IEEE Sens. J.2022, 23, 527–535. [CrossRef]\\n14. Sun, Y.; Lin, Y.; Zhao, G.; Svanberg, S. Identification of flying insects in the spatial, spectral, and time domains with focus on\\nmosquito imaging. Sensors 2021, 21, 3329. [CrossRef]\\n15. Patwal, A.; Diwakar, M.; Tripathi, V .; Singh, P . Crowd counting analysis using deep learning: A critical review.Procedia Comput.\\nSci. 2023, 218, 2448–2458. [CrossRef]\\n16. Kaewchote, J.; Janyong, S.; Limprasert, W. Image recognition method using Local Binary Pattern and the Random forest classifier\\nto count post larvae shrimp. Agric. Nat. Resour.2018, 52, 371–376. [CrossRef]\\n17. Nguyen, K.T.; Nguyen, C.N.; Wang, C.Y.; Wang, J.C. Two-phase instance segmentation for whiteleg shrimp larvae counting. In\\nProceedings of the 2020 IEEE International Conference on Consumer Electronics (ICCE), Las Vegas, NV , USA, 4–6 January 2020;\\npp. 1–3.\\n18. Qu, Y.; Jiang, S.; Li, D.; Zhong, P .; Shen, Z. SLCOBNet: Shrimp larvae counting network with overlapping splitting and\\nBayesian-DM-count loss. Biosyst. Eng.2024, 244, 200–210. [CrossRef]\\n19. Zhang, J.; Yang, G.; Sun, L.; Zhou, C.; Zhou, X.; Li, Q.; Bi, M.; Guo, J. Shrimp egg counting with fully convolutional regression\\nnetwork and generative adversarial network. Aquac. Eng.2021, 94, 102175. [CrossRef]\\n20. Kesvarakul, R.; Chianrabutra, C.; Chianrabutra, S. Baby shrimp counting via automated image processing. In Proceedings of the\\n9th International Conference on Machine Learning and Computing, Singapore, 24–26 February 2017; pp. 352–356.\\n21. Thai, T.T.N.; Nguyen, T.S.; Pham, V .C. Computer vision based estimation of shrimp population density and size. In Proceedings\\nof the 2021 International Symposium on Electrical and Electronics Engineering (ISEE), Ho Chi Minh, Vietnam, 15–16 April 2021;\\npp. 145–148.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='Sensors 2024, 24, 6328 18 of 19\\n22. Awalludin, E.A.; Yaziz, M.M.; Rahman, N.A.; Yussof, W.N.J.H.W.; Hitam, M.S.; Arsad, T.T. Combination of canny edge detection\\nand blob processing techniques for shrimp larvae counting. In Proceedings of the 2019 IEEE International Conference on Signal\\nand Image Processing Applications (ICSIPA), Kuala Lumpur, Malaysia, 17–19 September 2019; pp. 308–313.\\n23. Zhou, C.; Yang, G.; Sun, L.; Wang, S.; Song, W.; Guo, J. Counting, locating, and sizing of shrimp larvae based on density map\\nregression. Aquac. Int.2024, 32, 3147–3168. [CrossRef]\\n24. Zhang, L.; Zhou, X.; Li, B.; Zhang, H.; Duan, Q. Automatic shrimp counting method using local images and lightweight YOLOv4.\\nBiosyst. Eng.2022, 220, 39–54. [CrossRef]\\n25. Bereciartua-Pérez, A.; Gómez, L.; Picón, A.; Navarra-Mestre, R.; Klukas, C.; Eggers, T. Insect counting through deep learning-\\nbased density maps estimation. Comput. Electron. Agric.2022, 197, 106933. [CrossRef]\\n26. Yu, X.; Wang, J.; Wen, S.; Yang, J.; Zhang, F. A deep learning based feature extraction method on hyperspectral images for\\nnondestructive prediction of TVB-N content in Pacific white shrimp ( Litopenaeus vannamei). Biosyst. Eng. 2019, 178, 244–255.\\n[CrossRef]\\n27. Armalivia, S.; Zainuddin, Z.; Achmad, A.; Wicaksono, M.A. Automatic counting shrimp larvae based you only look once (YOLO).\\nIn Proceedings of the 2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS), Bandung,\\nIndonesia, 28–30 April 2021; pp. 1–4.\\n28. Liu, S.; Huang, D. Receptive field block net for accurate and fast object detection. In Proceedings of the European conference on\\nComputer Vision (ECCV), Munich, Germany, 8–14 September 2018; pp. 385–400.\\n29. Zhang, Y.; Zhao, D.; Zhang, J.; Xiong, R.; Gao, W. Interpolation-dependent image downsampling. IEEE Trans. Image Process.2011,\\n20, 3291–3296. [CrossRef]\\n30. Lan, Y.h.; Zhang, Y.; Li, C.h.; Zhao, X.f. A novel image segmentation method based on random walk. In Proceedings of the 2009\\nAsia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA), Wuhan, China, 28–29 November\\n2009; Volume 1, pp. 207–210.\\n31. Lin, W.; Chan, A.B. Optimal transport minimization: Crowd localization on density maps for semi-supervised counting. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 18–22 June 2023;\\npp. 21663–21673.\\n32. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV , USA, 27–30 June 2016; pp. 779–788.\\n33. Han, X.; Chang, J.; Wang, K. You only look once: Unified, real-time object detection. Procedia Comput. Sci.2021, 183, 61–72.\\n[CrossRef]\\n34. Redmon, J.; Farhadi, A. YOLO9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 7263–7271.\\n35. Redmon, J. Yolov3: An incremental improvement. arXiv 2018, arXiv:1804.02767.\\n36. Hussain, M. YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision. arXiv 2024, arXiv:2407.02988.\\n37. Fu, H.; Song, G.; Wang, Y. Improved YOLOv4 marine target detection combined with CBAM. Symmetry 2021, 13, 623. [CrossRef]\\n38. Wang, Z.; Jin, L.; Wang, S.; Xu, H. Apple stem/calyx real-time recognition using YOLO-v5 algorithm for fruit automatic loading\\nsystem. Postharvest Biol. Technol.2022, 185, 111808. [CrossRef]\\n39. Kim, J.H.; Kim, N.; Park, Y.W.; Won, C.S. Object detection and classification based on YOLO-V5 with improved maritime dataset.\\nJ. Mar. Sci. Eng.2022, 10, 377. [CrossRef]\\n40. Yang, R.; Li, W.; Shang, X.; Zhu, D.; Man, X. KPE-YOLOv5: An improved small target detection algorithm based on YOLOv5.\\nElectronics 2023, 12, 817. [CrossRef]\\n41. Wang, L.; Liu, X.; Ma, J.; Su, W.; Li, H. Real-time steel surface defect detection with improved multi-scale YOLO-v5. Processes\\n2023, 11, 1357. [CrossRef]\\n42. Sun, Q.; Li, P .; He, C.; Song, Q.; Chen, J.; Kong, X.; Luo, Z. A Lightweight and High-Precision Passion Fruit YOLO Detection\\nModel for Deployment in Embedded Devices. Sensors 2024, 24, 4942. [CrossRef]\\n43. Wang, C.Y.; Bochkovskiy, A.; Liao, H.Y.M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada,\\n18–22 June 2023; pp. 7464–7475.\\n44. Wang, A.; Chen, H.; Liu, L.; Chen, K.; Lin, Z.; Han, J.; Ding, G. Yolov10: Real-time end-to-end object detection. arXiv 2024,\\narXiv:2405.14458.\\n45. Dong, X.; Yan, S.; Duan, C. A lightweight vehicles detection network model based on YOLOv5. Eng. Appl. Artif. Intell.2022,\\n113, 104914. [CrossRef]\\n46. Chai, E.; Ta, L.; Ma, Z.; Zhi, M. ERF-YOLO: A YOLO algorithm compatible with fewer parameters and higher accuracy. Image Vis.\\nComput. 2021, 116, 104317. [CrossRef]\\n47. Kim, B.J.; Choi, H.; Jang, H.; Lee, D.G.; Jeong, W.; Kim, S.W. Dead pixel test using effective receptive field. Pattern Recognit. Lett.\\n2023, 167, 149–156. [CrossRef]\\n48. Wang, Q.; Qian, Y.; Hu, Y.; Wang, C.; Ye, X.; Wang, H. M2YOLOF: Based on effective receptive fields and multiple-in-single-out\\nencoder for object detection. Expert Syst. Appl.2023, 213, 118928. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-30T15:27:00+08:00', 'author': 'Hongchao Duan, Jun Wang,Yuan Zhang, Xiangyu Wu,Tao Peng,Xuhao Liu and Delong Deng', 'keywords': 'shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal', 'moddate': '2024-09-30T09:33:33+02:00', 'subject': \"Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter's detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%.\", 'title': 'Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation', 'source': 'data/shrimp_docs.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='Sensors 2024, 24, 6328 19 of 19\\n49. Chen, Y.; Zhan, S.; Cao, G.; Li, J.; Wu, Z.; Chen, X. C2f-Enhanced YOLOv5 for Lightweight Concrete Surface Crack Detection.\\nIn Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications, Wuhan, China,\\n18–20 November 2023; pp. 60–64.\\n50. Wang, W.; Tan, X.; Zhang, P .; Wang, X. A CBAM based multiscale transformer fusion approach for remote sensing image change\\ndetection. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.2022, 15, 6817–6825. [CrossRef]\\n51. Smart Shrimp Farm. 2024. Available online: http://m.ux6.com/app/222810.html (accessed on 1 March 2024).\\n52. Zhang, Y.; Guo, Z.; Wu, J.; Tian, Y.; Tang, H.; Guo, X. Real-time vehicle detection based on improved yolo v5. Sustainability 2022,\\n14, 12274. [CrossRef]\\n53. Liu, D.; Xu, B.; Cheng, Y.; Chen, H.; Dou, Y.; Bi, H.; Zhao, Y. Shrimpseed_Net: Counting of shrimp seed using deep learning on\\nsmartphones for aquaculture. IEEE Access2023, 11, 85441–85450. [CrossRef]\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1756902897773,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "DrgvKJTuG_ng",
    "outputId": "7affdfdc-3614-44bb-c7c7-ef188f024119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756902898423,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "LkOmp7B7G_ng"
   },
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1756902900005,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "JaHTAlQ5G_ng",
    "outputId": "e9d68835-e5a2-441d-a96f-428015a65e50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Citation: Duan, H.; Wang, J.; Zhang,\\nY.; Wu, X.; Peng, T.; Liu, X.; Deng, D.\\nShrimp Larvae Counting Based on\\nImproved YOLOv5 Model with\\nRegional Segmentation. Sensors 2024,\\n24, 6328. https://doi.org/10.3390/\\ns24196328\\nAcademic Editor: Yongwha Chung\\nReceived: 10 September 2024\\nRevised: 26 September 2024\\nAccepted: 27 September 2024\\nPublished: 30 September 2024\\nCopyright: © 2024 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nsensors\\nArticle\\nShrimp Larvae Counting Based on Improved YOLOv5 Model\\nwith Regional Segmentation\\nHongchao Duan, Jun Wang, Yuan Zhang *, Xiangyu Wu, Tao Peng, Xuhao Liu and Delong Deng\\nCentre for Optical and Electromagnetic Research, South China Academy of Advanced Optoelectronics,\\nSouth China Normal University, Guangzhou 510006, China; 2021024069@m.scnu.edu.cn (H.D.);\\n2020023787@m.scnu.edu.cn (J.W.); 2022024121@m.scnu.edu.cn (X.W.); 2023024193@m.scnu.edu.cn (T.P .);\\n2023024174@m.scnu.edu.cn (D.D.)\\n* Correspondence: yuan.zhang@coer-scnu.org\\nAbstract: Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and\\nhigh density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely\\npacked shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through\\na regional segmentation approach. First, the C2f and convolutional block attention modules are used\\nto improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a\\nregional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp\\ncounter’s detection performance. Finally, a strategy for stitching and deduplication is implemented\\nto tackle the problem of double counting across various segments. The findings from the experiments\\nindicate that the suggested algorithm surpasses several other shrimp counting techniques in terms\\nof accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an\\naccuracy exceeding 98%.\\nKeywords: shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat\\nshrimp removal\\n1. Introduction\\nShrimp farming has become a vital economic sector within aquaculture, contributing\\nsignificantly to the growth of the fish industry [1]. Counting shrimp larvae is an essential\\ntask in the shrimp farming process, as it assists farmers in determining the reproductive\\nrate and accurately estimating the production potential [2]. In addition, it helps evaluate\\nfertility, control the density of cultivation, and manage transport sales [ 3,4]. However,\\ndue to the tiny size, great flexibility, and dense numbers of shrimp larvae, counting them\\npresents a significant challenge [5]. Currently, the process is predominantly carried out by\\nhand, making it a time-consuming, labor-intensive, and somewhat imprecise task [6]. Thus,\\ndeveloping an innovative approach to overcome the current difficulties in counting shrimp\\nlarvae is of the utmost importance.\\nThe progress in the counting of shrimp larvae consists primarily of manual count-\\ning [7], photoelectric detector counting [ 8–11], counting based on conventional image\\nprocessing [12], and deep learning-based counting [13]. The first technique is highly depen-\\ndent on the operator’s expertise, resulting in substantial differences in the accuracy and\\nefficiency of the counting process. In addition, the process of manual counting may harm\\nshrimp larvae. Photoelectric devices for counting shrimp larvae work by shining light on\\nthe larvae and determining their number based on the light reflected back to the sensors [8].\\nAlthough this method significantly improves both the speed and accuracy, these counters\\ncan be affected by various environmental factors, such as lighting conditions and water\\nquality, resulting in considerable counting errors. Furthermore, photoelectric devices are\\ngenerally effective for shrimp larvae of certain sizes, which reduces their accuracy when\\ndealing with larvae of different sizes.\\nSensors 2024, 24, 6328. https://doi.org/10.3390/s24196328 https://www.mdpi.com/journal/sensorsSensors 2024, 24, 6328 2 of 19\\nRecently, computer vision technology has been applied in various fields [ 14], such\\nas target detection, image super-resolution, and population counting [15]. Regarding the\\ncounting of shrimp larvae [16–19], traditional image processing techniques primarily utilize\\nimage segmentation and object detection methods to recognize and count target images.\\nKesvarakul et al. counted shrimp larvae in images by converting them into binary images\\nwith a threshold [20]. However, this technique is only effective when dealing with a small\\nquantity of larvae and when their images are clear. Thai et al. used image segmentation\\nand contour tracking methods for the counting of shrimp larvae [21]. This method is able\\nto count individual shrimp larvae, although it struggles to accurately distinguish shrimp\\nlarvae which are stuck together. For populations with fewer shrimp larvae, counting was\\nperformed using algorithms, such as segmentation paired with the Canny edge detection\\nmethod and blob processing approaches [22]. Although this technique offers an enhance-\\nment over previous algorithms, it still does not address the issue of shrimp and larvae\\nadhering to each other. Traditional image processing-based counting techniques require\\nprior knowledge to manually adjust image features, leading to poor accuracy in scenarios\\nwith complex backgrounds. Furthermore, the lack of generalization in these models hinders\\nthe accurate detection and counting of shrimp larvae in different scenarios.\\nSubsequently, deep learning-based [23–26] counting methods use trained object recog-\\nnition models to identify targets within images. Armalivia et al. used the You Only Look\\nOnce version 3 (YOLOv3) algorithm to count shrimp larvae [ 27], achieving an average\\ncounting accuracy of 96% in low-density populations of around 100 larvae (in a circular\\narea with a diameter of 40 cm), but a larger number of larvae could not be identified. Hu\\net al. used deep learning and a density map of shrimp larvae to estimate the populations\\nof approximately 1000 larvae (in a 24 cm × 33 cm area) [ 13]. However, this approach\\ndoes not accurately identify each individual larva. Hence, devising an automated and\\naccurate method for counting shrimp larvae in densely populated areas remains a difficult\\nchallenge. Object detection algorithms based on deep learning mainly include Regions\\nwith Convolutional Neural Networks (R-CNN) and multiple versions of YOLO, among\\nwhich YOLOv5 is a mature algorithm among target recognition algorithms which has\\nthe advantages of being lightweight and having a fast inference ability. However, in the\\ndetection and recognition of small targets, YOLOv5 is not effective, and thus we need to\\nimprove YOLOv5.\\nIn this study, we investigate the enhancement of deep learning techniques to address\\nthe challenge of counting shrimp larvae in highly populated environments. In particular,\\nwe propose an algorithm which uses regional segmentation along with an improved\\nYOLOv5 model to accurately count shrimp larvae, especially in highly dense environments.\\nThe remainder of this paper is structured as follows. In Section 2, we give the related\\nwork to highlight our motivation. In Section 3, we describe the details of the proposed\\nshrimp larvae counting method. Section 4 presents the experimental results and further\\nvalidates the superiority of the algorithm through controlled experiments. Section 5 gives\\nour conclusions.\\nThe main contributions are described as follows:\\n• W e build an automatic shrimp collecting platform to obtain the corresponding dataset and\\nprovide a counting algorithm based on improved YOLOv5 with region segmentation.\\n• The C2f and attention mechanism are embedded in the YOLOv5 model, improving\\nthe detection ability for small shrimp.\\n• The segmentation of regions aims to boost the proportion of pixels representing shrimp\\nlarvae in a visual image.\\n• Experimental results prove that the proposed algorithm can identify shrimp larvae\\nwith high accuracy under the difficult circumstance of overlap between large numbers\\nof shrimp and different light intensities.Sensors 2024, 24, 6328 3 of 19\\n2. Related Work\\nThere are many challenges in the counting of shrimp larvae. First, shrimp larvae are\\nsmall in size, making them relatively difficult to detect. In addition, shrimp larvae move\\nrapidly, and their pixel values in images differ depending on their depth in the water.\\nConventional threshold segmentation techniques may easily overlook shrimp larvae which\\nare at the bottom of the water.\\nAs shown in the Figure 1, there are three states, including isolated, clumped, and\\noverlapping shrimp larvae. Isolated shrimp larvae remain unaffected by the presence\\nof other larvae and can be detected through conventional image processing approaches\\nor deep learning methods. Clumped shrimp larvae consist of several larvae attached\\nto each other, resulting in an underestimated count when using connected component\\nanalysis. This problem can be resolved through the application of watershed algorithms\\nand distance transformation. However, these techniques necessitate precise threshold\\nconfigurations and do not generalize well. Overlapping shrimp larvae appear when larvae\\nat varying water depths align at the same spot in the image. The aforementioned watershed\\nalgorithm is ineffective in addressing this scenario. Utilizing concave point algorithms\\nfor separation might erroneously fragment the shrimp tails, causing overcounting of the\\nshrimp. Consequently, conventional image processing techniques prove inadequate for\\nidentifying these instances, necessitating the use of deep learning approaches to accurately\\ndiscern and identify the various states of shrimp larvae.\\nFigure 1. The states of shrimp larvae. ( a) Isolated shrimp larvae. ( b) Clumped shrimp larvae.\\n(c) Overlapping shrimp larvae.\\nMinimizing the dimensions of the input image can enhance the accuracy of receptive\\nfield detection [28]. Two widely used techniques for decreasing the size of an image are\\nimage scaling and cropping. Image scaling often employs downsampling [29], which may\\nlead to data loss. Another approach is image segmentation, which can be performed either\\nrandomly or in a consistent manner. Random segmentation employs random functions\\nto determine the coordinates for the top-left and bottom-left sections of the local image\\nof shrimp larvae [30]. Then, the coordinates for the top-right and bottom-right sections\\nare determined according to the dimensions of the local image. In addition, the cropping\\nfunction is applied to extract the local image. Beginning in the upper-left corner of the\\noriginal image, fixed segmentation continues with sequential segmentation based on the\\ndimensions of the local image. For regions along the edges which are smaller than the\\nsegmented dimensions, the true size of the image is maintained. Random segmentation\\nmight result in repeated recognition of the same regions, thereby increasing the training\\nburden on the model. Consequently, this research uses fixed segmentation to handle the\\ninitial images.\\nThe estimation of density maps is also a commonly used counting method [23] which\\nestimates the number of objects by generating a density map. This is often applied in\\ncrowd counting [31]. However, this method cannot accurately provide the geometric and\\npositional information of objects. When the detection of small objects does not require\\npositional information, the method can provide good counting results. In shrimp larvae\\ncounting, when the larvae density is relatively low, positional and shape information may\\nnot be critical, and both density map estimation and object detection can achieve accurateSensors 2024, 24, 6328 4 of 19\\ncounts. However, as the larvae density increases, with overlapping larvae at different water\\ndepths, positional and shape information become important. In high-density areas, density\\nmap estimation is prone to misjudgments, leading to a decrease in count accuracy. By\\nincorporating position and shape information, object detection methods can achieve better\\ncounting accuracy. In terms of runtime, density map estimation models are more complex\\nand require a longer processing time, while object detection methods are faster, making\\nthem more suitable for quick shrimp larvae counting.\\nYOLO [32] is considered one of the best choices for object detection models due to its\\nspeed and superior accuracy compared with R-CNN [33]. YOLO pioneered an innovative\\napproach by utilizing convolutional neural networks on a complete image to predict object\\nclasses and bounding boxes via direct feature regression. Over the years, YOLO has evolved\\ninto several versions. YOLOv1 uses a unified detection approach for object localization\\nand classification tasks [ 32]. Subsequently, YOLOv2 improves on version 1 by having\\nbetter accuracy, a faster speed, and the ability to recognize more objects [ 34]. YOLOv3\\nimproves the object detection speed by implementing multiscale prediction [35], optimizing\\nthe core network, and refining the loss function. YOLOv4 introduces a fast and effective\\nobject detection model which substantially cuts down computational expenses, making\\nit more compatible with general-purpose devices and those with hardware constraints.\\nYOLOv5 brings significant advancements, including the CSPDarknet backbone and mosaic\\naugmentation, balancing speed and accuracy [ 36–42]. YOLOv7 improves the structure\\nof the extended efficient layer aggregation network, revises the model architecture, and\\nintroduces an efficient label assignment strategy, which increases detection performance\\nwhile decreasing the number of model parameters [43]. Currently, researchers are constantly\\nexploring the architectural design of YOLO, with the latest version being YOLOv10 [44]. It\\nhas achieved state-of-the-art performance and efficiency on various model scales. In the\\nexperimental section, we describe the performance of YOLOv10 and conduct shrimp larvae\\ncounting tests using this model. However, the YOLOv10 model is complex and has high\\nhardware resource requirements, making its hardware deployment challenging. One of\\nthe primary objectives of this study is to deploy the algorithm on hardware to achieve an\\nintegrated system for shrimp larvae image acquisition and counting. YOLOv10 does not\\nmeet the requirements of our design. YOLOv5 stands out from its predecessors with its\\nlighter architecture, faster inference times, and more developed ecosystem which offers\\nenhanced compatibility [45]. Moreover, YOLOv5 has low hardware resource requirements,\\nis relatively mature in terms of hardware deployment, and is easier to implement. Therefore,\\nwe selected YOLOv5 as the baseline for our study. However, in scenarios which involve\\nthe detection of small objects, such as shrimp larvae, the use of several convolutional layers\\nmay lead to missing small targets, resulting in reduced pixel occupancy for small objects. To\\naddress this problem, we upgraded the YOLOv5 backbone network to improve its ability\\nto identify small targets.\\n3. Materials and Methods\\nFor a more precise counting of shrimp larvae in situations with large numbers and a\\nhigh density, we suggest an algorithm based primarily on regional segmentation and an\\nimproved YOLOv5 model. The flow chart of the algorithm can be viewed in Figure 2.\\nTo begin with, we divide the whole original image into smaller segments using a\\nregion-based segmentation algorithm. Subsequently, we employ an enhanced YOLOv5\\nmodel to recognize shrimp larvae in each of these isolated images separately. Then, all\\nsegmented image blocks are reassembled into a complete image. A repeat count removal\\nmethod is proposed to address the issue of repeat counting in the stitching position,\\nensuring that accurate counting of shrimp larvae can be achieved throughout the image.Sensors 2024, 24, 6328 5 of 19\\nFigure 2. The flow chart of the proposed counting algorithm.\\n3.1. Image Collection\\nThe quality of the original images, which is one of the key factors for computer vision,\\nwill affect the accuracy of the counting of shrimp larvae. For the best accuracy in counting\\nthe larvae, we designed and fabricated a shrimp larvae image acquisition device as shown\\nin Figure 3.\\nShrimp larvae are placed in a semi-transparent plastic square bucket, and white LEDs\\nwith light guide plates are added around the side wall and the bottom to provide uniform\\nillumination. Shrimp larvae images are captured using an industrial camera (MV-CE060-\\n10UM (Hikvision, Hangzhou, China)) with an 8 mm lens under supplementary white light.\\nThe resolution of the camera is 3072× 2048 pixels for a field of view of about40 cm × 30 cm,\\nand the effective area we used for the shrimp larvae was 30 cm × 30 cm.\\nFigure 3. Shrimp larvae acquisition device. ( a) Schematic diagram. ( b) Photographs of the\\nactual device.\\nThe shrimp larvae dataset used in this experiment was collected from on-site photog-\\nraphy at the Hongkai Shrimp Larvae Farm in Zhuhai City, Guangdong Province, China.\\nFigure 4a shows the real shrimp larvae farming environment. The staff used a fishing net to\\nextract shrimp larvae from the pool (as shown in Figure 4b) and then placed the collected\\nlarvae in our custom-built shrimp larvae image acquisition device as shown in Figure 4c.Sensors 2024, 24, 6328 6 of 19\\nFigure 4. Acquisition of shrimp larvae datasets in a real aquaculture scenario. ( a) Shrimp culture\\ntank. (b) Shrimp larvae collection. (c) Acquisition device.\\n3.2. Region Segmentation\\nThe receptive field refers to the size of the region in the input image which corresponds\\nto each pixel in the output feature map [ 46]; that is, a single point on the feature map\\ncorresponds to a specific region on the input image. The size of the area of the receptive\\nfield directly affects the detection accuracy [47,48]. The formula for calculating the receptive\\nfield is as follows:\\nRi = (Ri+1 − 1) × Sti + Ksi (1)\\nwhere Ri represents the receptive field on the ith convolutional layer, Ri+1 is the receptive\\nfield on the i + 1th layer, Sti is the stride of the convolution, and KS is the size of the\\nconvolutional kernel for the current layer.\\nWhen the original image of shrimp larvae ( 3072 × 2048 pixels) is used as the input\\nimage, the corresponding receptive field size is38 ×38 pixels. This indicates that each point\\non the feature map corresponds to a area of 38 × 38 pixels on the original image, which is\\nsignificantly larger than the size of the shrimp larvae (approximately 10 × 10 pixels). As a\\nresult, information on shrimp larvae would be overlooked or covered, greatly reducing the\\ndetection accuracy.\\nTo address this issue, it is necessary to reduce the size of the receptive field to improve\\nthe shrimp larvae detection performance. Therefore, we adopted the region segmentation\\nmethod to divide the original image, reducing the size of the input image and thereby\\ndecreasing the receptive field area. For example, the original image of shrimp larvae (as\\nshown in Figure 5a) was divided into multiple image blocks (100 × 100 pixels for each, as\\nshown in Figure 5b) for subsequent labeling and training.\\nIn the whole image, the proportion of shrimp larvae in terms of pixels in the whole\\nimage is extremely small. Therefore, we used segmentation technology to obtain multiple\\nlocal regions of shrimp larvae, making feature extraction easier.\\nFigure 6 shows the feature maps selected from the 17th layer of the YOLOv5 model\\nfor both the full image and the local image. To facilitate comparison, we extracted the same\\nregion from the full image as that in the local image. From Figure 6, it is evident that the\\nfull image did not accurately identify local regions, while the local image allowed a more\\nprecise extraction of features from the shrimp larvae.Sensors 2024, 24, 6328 7 of 19\\nFigure 5. Image segmentation. (a) Original shrimp larvae image. (b) Segmented image.\\nFigure 6. The feature map referring to the shrimp larvae image.\\n3.3. The Improved YOLOv5\\nThe core idea of YOLOv5 is to convert the target detection problem into a regression\\nproblem, predicting the boundary boundaries and categories of the target in the image.\\nIt has high accuracy and a fast inference ability, making it one of the best-performing\\ntarget detection models available today. However, the tiny size of the shrimp larvae, along\\nwith various body positions such as their overlap, leads to a decrease in the identification\\naccuracy of the YOLOv5 model. Therefore, the recognition accuracy can be improved by\\nincreasing the classification performance of features.\\nAs shown in Figure 7, the C2f module (Figure 8) was used to replace the C3 module. In\\nthe backbone network of the YOLOv5 algorithm, the main function of the C3 module is to\\nextract image features and enhance the learning capacity of the convolutional network. The\\nC3 module cannot meet the requirements for the detection of small targets such as shrimp\\nlarvae and needs further improvement to enhance the feature extraction capabilities of the\\nmodel. The C2f module processes the input data using two convolutional layers, which\\nassist in extracting features at varying levels and degrees of abstraction. This improves the\\nfeature extraction efficiency and simultaneously reduces the network weight, facilitating\\nmore abundant gradient flow information [ 49]. In contrast to the C3 module, the C2f\\nmodule is more lightweight, has reduced computational demands, and demonstrates\\nrobust feature extraction capabilities.Sensors 2024, 24, 6328 8 of 19\\nFigure 7. The structure of the improved YOLOv5 model.\\nFigure 8. Structure of the C2f network.\\nTo further enhance the recognition capability of the model, the convolutional block\\nattention module (CBAM) [ 50] is introduced into the YOLOv5 framework. The CBAM\\nconsists of the channel attention and spatial attention modules, as shown in Figure 9. For\\nan input F, the global average and maximum pooling operations are first applied to obtain\\nglobal information for each channel. Subsequently, two fully connected layers are utilized\\nto produce the channel attention vector. These layers apply weights to the input feature F\\nper channel, which yields the refined feature F1. In the spatial attention module, the feature\\nmap is combined to capture spatial information. Furthermore, the feature F1 undergoes a\\nconvolution layer 3 × 3, resulting in the creation of a spatial attention map via the sigmoid\\nfunction. The spatial attention map is subsequently used to enhance the feature F1, leading\\nto production of the fused feature F2. The attention module improves the depiction of\\nshrimp larvae features across different conditions.\\nFigure 10 displays the feature map of both the enhanced YOLOv5 network and the\\noriginal YOLOv5 network. Clearly, the proposed approach provides more precise counting\\noutcomes even in instances of densely packed shrimp larvae.Sensors 2024, 24, 6328 9 of 19\\nFigure 9. Structure illustration of CBAM.\\nFigure 10. Feature map comparison. ( a) The original YOLOv5 network. ( b) The improved\\nYOLOv5 network.\\n3.4. Repeat Count Removal via Stitching\\nThe method described in Section 3.2 allows for segmentation of the entire image. The\\nmethod described in Section 3.3 can identify shrimp larvae in each segmented image block.\\nHowever, identical shrimp larvae can be segmented into neighboring image blocks and\\ndetected at the same time (in Figure 11), resulting in reduced counting accuracy. To address\\nthe issue of duplicate counts, an additional detection model was developed.\\nFigure 11. Segmentation of same shrimp larvae in adjacent image blocks (dashed lines indicate the\\nstitching positions). (a) Vertical segmentation. (b) Horizontal segmentation. (c) Both horizontal and\\nvertical segmentation.\\nFigure 11a shows a shrimp larva divided vertically into two sections and identified\\nin two separate subimages. Figure 11b presents two shrimp larvae segmented horizon-\\ntally into two subimages and identified. Figure 11c shows shrimp larvae cut horizontally\\nand vertically, resulting in four distinct subimages. Figure 12 shows a schematic illustra-\\ntion of the segmentation of shrimp larvae, helping in the evaluation of the segmentation\\nconfigurations.\\nIn Figure 12, the red boxes represent the detection boundary boxes of the detection\\noutput, and the black lines represent the cropping boundaries of the image. Taking intoSensors 2024, 24, 6328 10 of 19\\naccount the spatial relationship between the detection boundary boxes and the image\\ncropping boundaries in the output, nine distinct scenarios can be identified: undivided\\nshrimp larvae (N), top edge touching (U), bottom edge touching (D), right edge touching\\n(R), left edge touching (L), right and bottom edges touching (RD), right and top edges\\ntouching (RU), left and bottom edges touching(LD), and left and top edges touching(LU).\\nA set S, where S = {N, D, U, R, L, RD, RU, LD, LU}, was defined to store all detection\\noutput. By traversing the detection outputs and checking the boundaries, the detection\\nresults were placed in the corresponding sets.\\nFigure 12. Schematic diagram of shrimp larvae segmentation. (a) Vertical segmentation. (b) Horizon-\\ntal segmentation. (c) Both horizontal and vertical segmentation.\\nTo avoid duplication, we examined the shrimp larva detection boxes in neighboring\\nregions to determine whether the detected shrimp larvae were identical. If several detection\\nboxes corresponded to each other, as illustrated in Figure 13, then(xi, yi) are the coordinates\\nof the vertices of the detection frame, and the detection box at the bottom which intersects\\nthe cropping boundary coincides with the two detection boxes at the top, which also\\nintersect with the cropping boundary. Therefore, an overlap ratio must be implemented\\nto facilitate the selection process. We define a variable Ro to represent the percentage of\\noverlap between detection frames, whose formula is as follows:\\nRo = x4 − x1\\nx2 − x3\\n(2)\\nwhere x1, x2, x3, x4 are the horizontal coordinates of the detection frame.\\nFigure 13. Multiple detection box matching.\\nBy calculating the overlap ratio, the detection box with the highest overlap ratio\\nis selected. After determining the matching object, a minimum bounding rectangle is\\nused to completely enclose the two detection boxes, thus identifying the deduplication\\narea (indicated in Figure 14a, for example). The two red detection boxes in Figure 14bSensors 2024, 24, 6328 11 of 19\\ncorrespond to a typical repeat count case (according to Figure 11b). We determined whether\\nthey were connected by examining the coordinates of the two detection boxes. Once the\\nconnected boxes were identified, we used a minimum enclosing box (indicated by the green\\nrectangle in Figure 14a,c) to completely cover the two red detection boxes and define the\\ndeduplication area.\\nFigure 14. The deduplication area. (a) Schematic diagram of how to find the deduplication area (the\\ndashed line indicates region segmentation positions), where the two red rectangles are the detection boxes\\nand the corresponding minimum enclosing rectangle is drawn in green. (b) Original detection result with\\ntwo detection boxes detecting the same larva. (c) Minimum enclosing rectangle for the case in (b).\\nOnce the deduplication area is identified, it is fed back into the enhanced YOLOv5\\nnetwork mentioned in Section 3.3 to accurately count the shrimp larvae in this region. This\\nprocess ensures that duplicate counts are removed from the entire composite image of\\nshrimp larvae.\\n4. Experimental Results and Analysis\\n4.1. Implementation Details\\nIn the experiment, we collected 20,000 valid local images of shrimp larvae, with\\n15,000 allocated for training and 5000 for testing. For annotating the dataset, the conven-\\ntional labeling approach (Figure 15a) involves describing all the shrimp larvae, yet this\\ncan introduce redundant details and impede the model’s ability to learn the features of\\nshrimp larvae. We marked the heads of the shrimp larvae as shown in Figure 15b. This\\nannotation method concentrates on the feature of the shrimp larvae, making it easier to\\ndistinguish overlapping larvae. The comparison algorithms included the YOLOv5 algo-\\nrithm (which means using YOLOv5 without regional segmentation), the Mask R-CNN\\nalgorithm [17], and popular commercial software (Smart Shrimp Farm V2.1.3 [ 51]). In\\naddition, the proposed algorithm was performed on an NVIDIA RTX 3080Ti (NVIDIA,\\nSanta Clara, CA, USA). A total of 500 epochs were trained, with the learning rate set to\\n0.001. The other parameters were the default YOLOv5 parameters. The optimizer was the\\nstochastic gradient descent.\\nFigure 15. Dataset annotation. (a) The traditional labeling method. (b) The annotation methodwe used.Sensors 2024, 24, 6328 12 of 19\\nWe used images of shrimp larvae of different densities to evaluate the performance\\nof the proposed method. Under the fixed field of view provided (30 cm × 30 cm), we\\nexamined four different densities of shrimp larvae: approximately 1000 larvae for density\\nlevel 1, approximately 2300 larvae for density level 2, approximately 4000 larvae for density\\nlevel 3, and roughly 5000 larvae for density level 4. We chose 100 × 100 pixels for the\\nsegmentation image size in the counting experiments hereafter. The dataset and codes can\\nbe requested by email (Hongchao Duan: 2021024069@m.scnu.edu.cn).\\n4.2. Evaluation Criterion\\nTo quantitatively evaluate the performance of the proposed algorithm, we define A as\\nthe measurement for the counting accuracy of the formula as follows:\\nA = Ac − |Id − Ac|\\nAc\\n× 100% (3)\\nwhere Ac represents the actual number of shrimp larvae and Id denotes the number of\\nshrimp larvae.\\nFor accurate results, 15 images of shrimp larvae were analyzed at each density level,\\nwith the mean count accuracy serving as the statistical result for each level.\\n4.3. Experiment Comparison\\nTo verify the effectiveness of the above proposed method, we collected a large number\\nof shrimp images of different densities (four densities mentioned in Section 4.1) for the\\nvalidation experiment. Figure 16a–d shows the recognition effect of the proposed method\\nunder different densities. It can be seen in Figure 16 that the proposed algorithm had a\\ngood recognition effect for different densities, especially in places where shrimp larvae\\nwere densely populated.\\nFigure 16. Counting results at different densities. ( a) Density 1. ( b) Density 2. ( c) Density 3.\\n(d) Density 4.Sensors 2024, 24, 6328 13 of 19\\nFigure 17a shows a typical local image of the original high-density shrimp larvae\\n(approximately 5000 shrimp larvae in a 30 cm × 30 cm area), and Figure 17b shows the\\ncorresponding identification results. It shows that the proposed method exhibited high\\nrecognition accuracy even when the density of the shrimp larvae was high and there were\\noverlaps between larvae. This indicates that the regional segmentation approach adopted\\nin this study makes the receptive field more accurate, which makes the improved YOLOv5\\nmodel more accurate in the recognition of shrimp larvae targets.\\nFigure 17. A typical local area of Figure 16. (a) Original image. (b) Detection results.\\nAmong the various comparison algorithms, Smart Shrimp Farm only provided the\\nfinal count results without the detection images. We tested high-density shrimp larvae\\nimages (approximately 5000 shrimp larvae in a 30 cm × 30 cm area) using the remaining\\nalgorithms, and the comparison of the test results is shown in Figure 18. For Density Map\\nRegression, we provide the corresponding density map in Figure 18f.\\nFigure 18. Test result images for different algorithms. ( a) The proposed method. ( b) YOLOv5.\\n(c) Mask R-CNN. (d) YOLOv10. (e) Shrimpseed_Net. (f) Density map regression.\\nTable 1 presents the statistical results of the average counting accuracy A for the\\ncomparison algorithm.Sensors 2024, 24, 6328 14 of 19\\nTable 1. Average counting accuracy A for different shrimp larvae densities.\\nMethod Density 1 Density 2 Density 3 Density 4\\nThe Proposed Method 99.32 % 99.17% 98.53% 98.23%\\nSmart Shrimp Farm [51] 91.43% 90.35% 88.25% 82.99%\\nYOLOv5 [52] 87.79% 85.46% 83.66% 81.03%\\nMask R-CNN [17] 86.63% 84.52% 81.25% 80.32%\\nYOLOv10 [44] 91.41% 90.68% 89.29% 88.75%\\nShrimpseed_Net [53] 91.65% 90.23% 89.47% 87.62%\\nDensity Map Regression [23] 92.33% 91.58% 90.75% 88.61%\\nThe results clearly indicate that, compared with commercial software such as YOLOv5,\\nYOLOv10, Shrimpseed_Net, Density Map Regression, and Mask R-CNN, the proposed\\nalgorithm excelled in counting shrimp larvae at various densities, particularly in high-\\ndensity scenarios. Although the accuracy decreased somewhat as the density increased, the\\noverall recognition accuracy of the proposed method remained above 98%, significantly\\noutperforming the other three cases (just above 80%). Moreover, the accuracy, which\\nexceeded 98%, demonstrates that the proposed algorithm could potentially substitute the\\nmanual shrimp counting process on farms.\\n4.4. Ablation Study\\nTo validate the superiority of the improved YOLOv5 model proposed in this article,\\nwe compared the identification results in the segmentation images of100 × 100 pixels using\\nboth the original YOLOv5 model and the improved YOLOv5 model in Section 3.3 (typical\\nresults are shown in Figure 19).\\nFigure 19. Recognition performance of different algorithm models. ( a) Original YOLOv5 model.\\n(b) Improved YOLOv5 model.\\nAs shown in Figure 19, the improved YOLOv5 model demonstrated better recognition\\nperformance when the shrimp larvae were densely packed. To quantitatively analyze the\\ndifferences between the two models, we used the counting algorithm on all images, and\\nthe results of the average counting accuracy for the four density levels are summarized in\\nTable 2. For density 4, we collected images of shrimp larvae under three different lighting\\nconditions (bright, normal brightness, and dim) for testing. The test results are shown in\\nFigure 20, and the data are summarized in Table 2.\\nThe statistical results in Table 2 show that the improved YOLOv5 model achieved an\\napproximately 2% higher counting accuracy compared with the original YOLOv5 model,\\nvalidating the effectiveness of the proposed method. Under different lighting conditions,\\nthe counting accuracy of our proposed algorithm exceeded 98%, outperforming the original\\nYOLOv5 algorithm.Sensors 2024, 24, 6328 15 of 19\\nFigure 20. Test results under three different lighting conditions. (a) Bright. (b) Normal brightness.\\n(c) Dim.\\nTable 2. Average counting accuracy A for the two YOLOv5 models.\\nDensity The Original YOLOv5 The Improved YOLOv5\\nDensity 1 98.13% 99.32%\\nDensity 2 97.85% 99.17%\\nDensity 3 96.81% 98.53%\\nDensity 4 (Bright) 96.26% 98.23%\\nDensity 4 (Normal Brightness) 94.66% 98.21%\\nDensity 4 (Dim) 93.52% 98.17%\\nThe original image of the larvae was divided into many local areas to decrease the size\\nof the receptive field in Section 3.2. To evaluate the impact of segmentation image sizes on\\nthe accuracy of shrimp larvae detection, we cropped the original shrimp larvae images into\\nfive different sizes: 600 × 600 pixels, 300 × 300 pixels, 200 × 200 pixels, 100 × 100 pixels,\\nand 50 × 50 pixels (as shown in Figure 21). Subsequently, we used the improved YOLOv5\\nmodel to train and test these cropped images of different sizes separately.\\nFigure 21. Recognition results for different segmentation image sizes: ( a) 600 × 600 pixels,\\n(b) 300 × 3000 pixels, (c) 200 × 200 pixels, (d) 100 × 100 pixels, and (e) 50 × 50 pixels.\\nAs shown in Figure 21, when the segmentation image size was 600 × 600 pixels, many\\nshrimp larvae were not identified. As the segmentation size decreased, the number of\\nundetected shrimp larvae also decreased. When the segmentation image size was reducedSensors 2024, 24, 6328 16 of 19\\nto 100 × 100 pixels and 50 × 50 pixels, all shrimp larvae in the local images were detected.\\nTo quantitatively analyze the differences between the various segmentation image sizes,\\nwe tested the counting accuracy using images with four different densities of shrimp larvae\\nmentioned in Section 4.1, and the statistical results of the average counting accuracy A for\\neach level of density of shrimp larvae are presented in Table 3.\\nTable 3. Average counting accuracy A for different segmentation sizes.\\nDensity 600 × 600\\nPixels\\n300 × 300\\nPixels\\n200 × 200\\nPixels\\n100 × 100\\nPixels 50 × 50 Pixels\\nDensity 1 91.69% 94.27% 97.79% 99.32% 99.34%\\nDensity 2 90.83% 93.85% 97.24% 99.17% 99.20%\\nDensity 3 88.72% 93.31% 96.38% 98.53% 98.87%\\nDensity 4 88.15% 92.56% 95.93% 98.23% 98.21%\\nThe results indicate that the size of the image segmentation had a significant impact on\\nthe accuracy of the counting of shrimp larvae. As the segmentation image size decreased,\\nthe accuracy of the shrimp larvae count clearly increased. When the segmentation image\\nsize was reduced to 50 × 50 pixels, the counting accuracy was the best, but we also realize\\nthat the cost of computing increased exponentially. Considering the calculation cost and\\nthe counting accuracy, we chose 100 × 100 pixels for the segmentation image size in the\\ncounting experiments hereafter.\\n4.5. Discussion\\nComparative experiments showed that the proposed region segmentation algorithm\\nimproved the accuracy of counting shrimp larvae. In contrast to the standard YOLOv5\\nmodel, the enhanced YOLOv5 variant provides more accurate detection of small objects\\nsuch as shrimp larvae. Validation experiments indicated that the proposed algorithm\\nperformed better, with a counting accuracy of 98% even under conditions with high\\ndensities and large volumes of shrimp larvae. However, there are still aspects which need\\nto be improved. To begin with, the dataset for this experiment was gathered using an\\noptical platform which we designed. To improve the generalizability of the algorithm,\\nit is essential to include images of shrimp larvae obtained from various environments\\nand a range of imaging devices in the training dataset. In addition, the stitching and\\ndeduplication process occasionally overlooks a few shrimp larvae due to stitching errors,\\nsuggesting that this algorithm needs further improvement. In conclusion, the proposed\\nalgorithm can be integrated into shrimp larvae imaging devices to develop a comprehensive\\nshrimp larvae counting system.\\nThe image segmentation approach and the improved YOLOv5 model proposed in\\nthis paper not only solve the problem of shrimp larvae counting but also have significant\\nresearch potential in various fields, such as dense crowd counting and medical image\\nanalysis. Using segmentation algorithms, the proportion of the pixel of the target objects\\nin the input images increased, thereby improving the detection accuracy. However, the\\nalgorithm needs to be adjusted according to the specific problem at hand.\\n5. Conclusions\\nWe introduced a method for counting shrimp larvae which uses region segmentation\\nalong with an improved YOLOv5 model. By segmenting the regions, we can reduce the\\nreceptive field area, thereby enhancing the detection accuracy of shrimp larvae using the\\nimproved YOLOv5 model. Furthermore, the deduplication model can tackle the problem\\nof repeated counts. Based on this, the experimental results show that the number of larvae\\nwas about 5000, and the counting accuracy of our algorithm remained above 98%, which\\nwould be suitable for replacing the work of manually counting shrimp larvae.Sensors 2024, 24, 6328 17 of 19\\nAuthor Contributions: Conceptualization, Y.Z.; methodology, Y.Z., J.W. and H.D.; software, J.W. and\\nH.D.; validation, X.W., T.P ., X.L. and D.D.; formal analysis, J.W. and H.D.; investigation, J.W., X.W.\\nand H.D.; resources, T.P .; data curation, D.D.; writing—original draft preparation, H.D.; writing—\\nreview and editing, Y.Z.; visualization, Y.Z.; supervision, Y.Z.; project administration, Y.Z.; funding\\nacquisition, Y.Z. All authors have read and agreed to the published version of the manuscript.\\nFunding: This research received no external funding.\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: Data are available from the authors on request.\\nConflicts of Interest: The authors declare no conflicts of interest.\\nReferences\\n1. Racotta, I.S.; Palacios, E.; Ibarra, A.M. Shrimp larval quality in relation to broodstock condition. Aquaculture 2003, 227, 107–130.\\n[CrossRef]\\n2. Li, D.; Miao, Z.; Peng, F.; Wang, L.; Hao, Y.; Wang, Z.; Chen, T.; Li, H.; Zheng, Y. Automatic counting methods in aquaculture: A\\nreview. J. World Aquac. Soc.2021, 52, 269–283. [CrossRef]\\n3. Tierney, T.W.; Fleckenstein, L.J.; Ray, A.J. The effects of density and artificial substrate on intensive shrimp Litopenaeus vannamei\\nnursery production. Aquac. Eng.2020, 89, 102063. [CrossRef]\\n4. Naegel, L.C.; Gómez-Humarán, I.M. Effect of sample volume and population density on precision of larval population estimates.\\nAquac. Eng.1998, 17, 11–19. [CrossRef]\\n5. Hsieh, Y.K.; Hsieh, J.W.; Hu, W.C.; Tseng, Y.C. AIoT-Based Shrimp Larvae Counting System Using Scaled Multilayer Feature\\nFusion Network. IEEE Internet Things J.2024, early Access. [CrossRef]\\n6. Zhang, L.; Li, W.; Liu, C.; Zhou, X.; Duan, Q. Automatic fish counting method using image density grading and local regression.\\nComput. Electron. Agric.2020, 179, 105844. [CrossRef]\\n7. Yeh, C.T.; Ling, M.S. Portable Device for Ornamental Shrimp Counting Using Unsupervised Machine Learning. Sens. Mater.\\n2021, 33, 3027–3036. [CrossRef]\\n8. Martinez-Palacios, C.; Novoa, M.O.; Chavez-Martinez, C. A simple apparatus for self-separation of post-larval prawns,\\nMacrobrachium spp., in mixed larval rearing tanks. Aquac. Res.1985, 16, 341–348. [CrossRef]\\n9. Work, H.P .S. Photoelectric Sensors for Counting and Classifying Vehicles. InTransportation Research Record 1311; Transportation\\nResearch Board: Washington, DC, USA, 1991; p. 79.\\n10. Carmichael, H. Photoelectric Detection II. In An Open Systems Approach to Quantum Optics: Lectures Presented at the Université Libre\\nde Bruxelles October 28 to November 4, 1991; Springer: Berlin/Heidelberg, Germany, 1993; pp. 93–112.\\n11. Spratt, M.D. Preliminary results of a computer imaging system applied to estimating the quantity of larvae and fingerling fish for\\naquaculture. In Fish Quality Control by Computer Vision; Routledge: New York, NY, USA, 2017; pp. 263–282.\\n12. Awalludin, E.; Muhammad, W.W.; Arsad, T.; Yussof, W.H.W. Fish larvae counting system using image processing techniques. In\\nProceedings of the Journal of Physics Conference Series; IOP Publishing: Bandung, Indonesia, 2020; Volume 1529, p. 052040.\\n13. Hu, W.C.; Chen, L.B.; Hsieh, M.H.; Ting, Y.K. A deep-learning-based fast counting methodology using density estimation for\\ncounting shrimp larvae. IEEE Sens. J.2022, 23, 527–535. [CrossRef]\\n14. Sun, Y.; Lin, Y.; Zhao, G.; Svanberg, S. Identification of flying insects in the spatial, spectral, and time domains with focus on\\nmosquito imaging. Sensors 2021, 21, 3329. [CrossRef]\\n15. Patwal, A.; Diwakar, M.; Tripathi, V .; Singh, P . Crowd counting analysis using deep learning: A critical review.Procedia Comput.\\nSci. 2023, 218, 2448–2458. [CrossRef]\\n16. Kaewchote, J.; Janyong, S.; Limprasert, W. Image recognition method using Local Binary Pattern and the Random forest classifier\\nto count post larvae shrimp. Agric. Nat. Resour.2018, 52, 371–376. [CrossRef]\\n17. Nguyen, K.T.; Nguyen, C.N.; Wang, C.Y.; Wang, J.C. Two-phase instance segmentation for whiteleg shrimp larvae counting. In\\nProceedings of the 2020 IEEE International Conference on Consumer Electronics (ICCE), Las Vegas, NV , USA, 4–6 January 2020;\\npp. 1–3.\\n18. Qu, Y.; Jiang, S.; Li, D.; Zhong, P .; Shen, Z. SLCOBNet: Shrimp larvae counting network with overlapping splitting and\\nBayesian-DM-count loss. Biosyst. Eng.2024, 244, 200–210. [CrossRef]\\n19. Zhang, J.; Yang, G.; Sun, L.; Zhou, C.; Zhou, X.; Li, Q.; Bi, M.; Guo, J. Shrimp egg counting with fully convolutional regression\\nnetwork and generative adversarial network. Aquac. Eng.2021, 94, 102175. [CrossRef]\\n20. Kesvarakul, R.; Chianrabutra, C.; Chianrabutra, S. Baby shrimp counting via automated image processing. In Proceedings of the\\n9th International Conference on Machine Learning and Computing, Singapore, 24–26 February 2017; pp. 352–356.\\n21. Thai, T.T.N.; Nguyen, T.S.; Pham, V .C. Computer vision based estimation of shrimp population density and size. In Proceedings\\nof the 2021 International Symposium on Electrical and Electronics Engineering (ISEE), Ho Chi Minh, Vietnam, 15–16 April 2021;\\npp. 145–148.Sensors 2024, 24, 6328 18 of 19\\n22. Awalludin, E.A.; Yaziz, M.M.; Rahman, N.A.; Yussof, W.N.J.H.W.; Hitam, M.S.; Arsad, T.T. Combination of canny edge detection\\nand blob processing techniques for shrimp larvae counting. In Proceedings of the 2019 IEEE International Conference on Signal\\nand Image Processing Applications (ICSIPA), Kuala Lumpur, Malaysia, 17–19 September 2019; pp. 308–313.\\n23. Zhou, C.; Yang, G.; Sun, L.; Wang, S.; Song, W.; Guo, J. Counting, locating, and sizing of shrimp larvae based on density map\\nregression. Aquac. Int.2024, 32, 3147–3168. [CrossRef]\\n24. Zhang, L.; Zhou, X.; Li, B.; Zhang, H.; Duan, Q. Automatic shrimp counting method using local images and lightweight YOLOv4.\\nBiosyst. Eng.2022, 220, 39–54. [CrossRef]\\n25. Bereciartua-Pérez, A.; Gómez, L.; Picón, A.; Navarra-Mestre, R.; Klukas, C.; Eggers, T. Insect counting through deep learning-\\nbased density maps estimation. Comput. Electron. Agric.2022, 197, 106933. [CrossRef]\\n26. Yu, X.; Wang, J.; Wen, S.; Yang, J.; Zhang, F. A deep learning based feature extraction method on hyperspectral images for\\nnondestructive prediction of TVB-N content in Pacific white shrimp ( Litopenaeus vannamei). Biosyst. Eng. 2019, 178, 244–255.\\n[CrossRef]\\n27. Armalivia, S.; Zainuddin, Z.; Achmad, A.; Wicaksono, M.A. Automatic counting shrimp larvae based you only look once (YOLO).\\nIn Proceedings of the 2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS), Bandung,\\nIndonesia, 28–30 April 2021; pp. 1–4.\\n28. Liu, S.; Huang, D. Receptive field block net for accurate and fast object detection. In Proceedings of the European conference on\\nComputer Vision (ECCV), Munich, Germany, 8–14 September 2018; pp. 385–400.\\n29. Zhang, Y.; Zhao, D.; Zhang, J.; Xiong, R.; Gao, W. Interpolation-dependent image downsampling. IEEE Trans. Image Process.2011,\\n20, 3291–3296. [CrossRef]\\n30. Lan, Y.h.; Zhang, Y.; Li, C.h.; Zhao, X.f. A novel image segmentation method based on random walk. In Proceedings of the 2009\\nAsia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA), Wuhan, China, 28–29 November\\n2009; Volume 1, pp. 207–210.\\n31. Lin, W.; Chan, A.B. Optimal transport minimization: Crowd localization on density maps for semi-supervised counting. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 18–22 June 2023;\\npp. 21663–21673.\\n32. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV , USA, 27–30 June 2016; pp. 779–788.\\n33. Han, X.; Chang, J.; Wang, K. You only look once: Unified, real-time object detection. Procedia Comput. Sci.2021, 183, 61–72.\\n[CrossRef]\\n34. Redmon, J.; Farhadi, A. YOLO9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 7263–7271.\\n35. Redmon, J. Yolov3: An incremental improvement. arXiv 2018, arXiv:1804.02767.\\n36. Hussain, M. YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision. arXiv 2024, arXiv:2407.02988.\\n37. Fu, H.; Song, G.; Wang, Y. Improved YOLOv4 marine target detection combined with CBAM. Symmetry 2021, 13, 623. [CrossRef]\\n38. Wang, Z.; Jin, L.; Wang, S.; Xu, H. Apple stem/calyx real-time recognition using YOLO-v5 algorithm for fruit automatic loading\\nsystem. Postharvest Biol. Technol.2022, 185, 111808. [CrossRef]\\n39. Kim, J.H.; Kim, N.; Park, Y.W.; Won, C.S. Object detection and classification based on YOLO-V5 with improved maritime dataset.\\nJ. Mar. Sci. Eng.2022, 10, 377. [CrossRef]\\n40. Yang, R.; Li, W.; Shang, X.; Zhu, D.; Man, X. KPE-YOLOv5: An improved small target detection algorithm based on YOLOv5.\\nElectronics 2023, 12, 817. [CrossRef]\\n41. Wang, L.; Liu, X.; Ma, J.; Su, W.; Li, H. Real-time steel surface defect detection with improved multi-scale YOLO-v5. Processes\\n2023, 11, 1357. [CrossRef]\\n42. Sun, Q.; Li, P .; He, C.; Song, Q.; Chen, J.; Kong, X.; Luo, Z. A Lightweight and High-Precision Passion Fruit YOLO Detection\\nModel for Deployment in Embedded Devices. Sensors 2024, 24, 4942. [CrossRef]\\n43. Wang, C.Y.; Bochkovskiy, A.; Liao, H.Y.M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada,\\n18–22 June 2023; pp. 7464–7475.\\n44. Wang, A.; Chen, H.; Liu, L.; Chen, K.; Lin, Z.; Han, J.; Ding, G. Yolov10: Real-time end-to-end object detection. arXiv 2024,\\narXiv:2405.14458.\\n45. Dong, X.; Yan, S.; Duan, C. A lightweight vehicles detection network model based on YOLOv5. Eng. Appl. Artif. Intell.2022,\\n113, 104914. [CrossRef]\\n46. Chai, E.; Ta, L.; Ma, Z.; Zhi, M. ERF-YOLO: A YOLO algorithm compatible with fewer parameters and higher accuracy. Image Vis.\\nComput. 2021, 116, 104317. [CrossRef]\\n47. Kim, B.J.; Choi, H.; Jang, H.; Lee, D.G.; Jeong, W.; Kim, S.W. Dead pixel test using effective receptive field. Pattern Recognit. Lett.\\n2023, 167, 149–156. [CrossRef]\\n48. Wang, Q.; Qian, Y.; Hu, Y.; Wang, C.; Ye, X.; Wang, H. M2YOLOF: Based on effective receptive fields and multiple-in-single-out\\nencoder for object detection. Expert Syst. Appl.2023, 213, 118928. [CrossRef]Sensors 2024, 24, 6328 19 of 19\\n49. Chen, Y.; Zhan, S.; Cao, G.; Li, J.; Wu, Z.; Chen, X. C2f-Enhanced YOLOv5 for Lightweight Concrete Surface Crack Detection.\\nIn Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications, Wuhan, China,\\n18–20 November 2023; pp. 60–64.\\n50. Wang, W.; Tan, X.; Zhang, P .; Wang, X. A CBAM based multiscale transformer fusion approach for remote sensing image change\\ndetection. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.2022, 15, 6817–6825. [CrossRef]\\n51. Smart Shrimp Farm. 2024. Available online: http://m.ux6.com/app/222810.html (accessed on 1 March 2024).\\n52. Zhang, Y.; Guo, Z.; Wu, J.; Tian, Y.; Tang, H.; Guo, X. Real-time vehicle detection based on improved yolo v5. Sustainability 2022,\\n14, 12274. [CrossRef]\\n53. Liu, D.; Xu, B.; Cheng, Y.; Chen, H.; Dou, Y.; Bi, H.; Zhao, Y. Shrimpseed_Net: Counting of shrimp seed using deep learning on\\nsmartphones for aquaculture. IEEE Access2023, 11, 85441–85450. [CrossRef]\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267,
     "referenced_widgets": [
      "e81415e8e4cc453e9bc32de8a4a1f567",
      "57a1ad0ba0b04ef29a9886223fd889c9",
      "b5fe54e5760d44d0aa0d8476ad335ab2",
      "d0ece75094c948629efeafbf2506734f",
      "5104fe0fa1144f72a9d216932b8b3fb3",
      "a705286e388a4cb4ac3323934f99a0fa",
      "7fa7f9a88696449795df2309a4ddfe8b",
      "b98a79d88b884b10b6d4b2ba332c0ac0",
      "a3d80a91f5b54fff893e7a909b4644c0",
      "9c8c9367e95a4760a662493b1d2b38a5",
      "a8de03702c5b47e09af2f24c6e942124",
      "f666f6e2dce948478eba03bed880a5c4",
      "577719ed33314ad4b9ebe851c8469829",
      "8c49ede2caed452fb52c10087f50b445",
      "53b06af55eea46339e6fcf87c3cf72ca",
      "ae50dc6fa98247e6b77c861bf991cea6",
      "de6570c38cd749cb8c95d7c7e9495a43",
      "3865538eb2784928b34f2c63690a0eff",
      "1efc560ddf14432fbaeaa519acdfd1c9",
      "e522f33846f449bea629e1b0428f9448",
      "7a65ee1f601b4282a98e4a7d60b20429",
      "33e714e8c37b4c329705829fdf4ebd17",
      "068c1ac914964aa8959359f239537113",
      "b2d5c7ab8514404bbd085fd081ae66b6",
      "cca0a5dd926d41eaa2c6f3f744139026",
      "1185f5a11d7542549cf8a41c4acdd299",
      "74a99421e3a545f3b683629789d71ed4",
      "d1da84accb5e469a9ed60462de51f3af",
      "9e9c8704318644469be030855d0c957e",
      "8c5047dd6b794b1bb0fccb6ffcf3a7a2",
      "5d675c0fe78b4acd8e72e8a40a60cf22",
      "f021416b2548455fa473c8db203ba859",
      "21d269efb3974a75878b9e6509572977",
      "0b5d1a20b02e4e809016c9266b2bad0a",
      "fa0dd004fc8f48f6912bcc8e0457fb7e",
      "bfe10d44f2494bdc8718cac8e327fc15",
      "a2c5acd2a63545e995678980e9d8768e",
      "f4cbab3155e64acbb5cdcdfa8300c59f",
      "a409ce978edd4aa1bae2be48e9dfa0ff",
      "e56fe58ecf824fc582d44a76cef146ee",
      "45151a137aa94317a841c56a848fe341",
      "c049f4a3b77b4eb29a208d9d9da4eebb",
      "0f133181773c4a5d9c064e70490cb1a9",
      "e5b5bbfe393a4376b2b406735ea20f85"
     ]
    },
    "executionInfo": {
     "elapsed": 7042,
     "status": "ok",
     "timestamp": 1756902922497,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "ARADgz-KbUAu",
    "outputId": "63490bff-68dc-44f0-c742-6b4dcb05c2d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d\\generative AI\\Doc2Question\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13350 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import split_text_on_tokens, Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, max_length=512)\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text)\n",
    "\n",
    "def decode(tokens):\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_local = Tokenizer(\n",
    "    tokens_per_chunk=500,\n",
    "    chunk_overlap=50,\n",
    "    encode=encode,\n",
    "    decode=decode\n",
    ")\n",
    "\n",
    "chunk_ques_gen = split_text_on_tokens(text=question_gen, tokenizer=tokenizer_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1756902922989,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "MvInM2G7G_nh",
    "outputId": "6a0f170d-0953-4599-a587-f30422a3cd13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756902923009,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "M0v38IGuG_nh",
    "outputId": "d489bee2-2f72-4b6d-b21f-fc0b53548d81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_ques_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1756902925820,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "B1VP5XxZG_nh"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1756902926778,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "jpKq_lUfG_ni"
   },
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(page_content = t) for t in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "31Tyil3CG_ni"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Citation: Duan, H.; Wang, J.; Zhang, Y.; Wu, X.; Peng, T.; Liu, X.; Deng, D. Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation. Sensors 2024, 24, 6328. https://doi.org/10.3390/ s24196328 Academic Editor: Yongwha Chung Received: 10 September 2024 Revised: 26 September 2024 Accepted: 27 September 2024 Published: 30 September 2024 Copyright:  2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). sensors Article Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation Hongchao Duan, Jun Wang, Yuan Zhang *, Xiangyu Wu, Tao Peng, Xuhao Liu and Delong Deng Centre for Optical and Electromagnetic Research, South China Academy of Advanced Optoelectronics, South China Normal University, Guangzhou 510006, China; 2021024069@m.scnu.edu.cn (H.D.); 2020023787@m.scnu.edu.cn (J.W.); 2022024121@m.scnu.edu.cn (X.W.); 2023024193@m.scnu.edu.cn (T.P .); 2023024174@m.scnu.edu.cn (D.D.) * Correspondence: yuan.zhang@coer-scnu.org Abstract: Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5'),\n",
       " Document(metadata={}, page_content='larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter’s detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%. Keywords: shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal 1. Introduction Shrimp farming has become a vital economic sector within aquaculture, contributing significantly to the growth of the fish industry [1]. Counting shrimp larvae is an essential task in the shrimp farming process, as it assists farmers in determining the reproductive rate and accurately estimating the production potential [2]. In addition, it helps evaluate fertility, control the density of cultivation, and manage transport sales [ 3,4]. However, due to the tiny size, great flexibility, and dense numbers of shrimp larvae, counting them presents a significant challenge [5]. Currently, the process is predominantly carried out by hand, making it a time-consuming, labor-intensive, and somewhat imprecise task [6]. Thus, developing an innovative approach to overcome the current difficulties in counting shrimp larvae is of the utmost importance. The progress in the counting of shrimp larvae consists primarily of manual count- ing [7], photoelectric detector counting [ 8–11], counting based on conventional image processing [12], and deep learning-based counting [13]. The first technique is highly depen- dent on the operator’s expertise, resulting in substantial differences in the accuracy and efficiency of the counting process. In addition, the process of manual counting may harm shrimp larvae.'),\n",
       " Document(metadata={}, page_content='counting [13]. The first technique is highly depen- dent on the operator’s expertise, resulting in substantial differences in the accuracy and efficiency of the counting process. In addition, the process of manual counting may harm shrimp larvae. Photoelectric devices for counting shrimp larvae work by shining light on the larvae and determining their number based on the light reflected back to the sensors [8]. Although this method significantly improves both the speed and accuracy, these counters can be affected by various environmental factors, such as lighting conditions and water quality, resulting in considerable counting errors. Furthermore, photoelectric devices are generally effective for shrimp larvae of certain sizes, which reduces their accuracy when dealing with larvae of different sizes. Sensors 2024, 24, 6328. https://doi.org/10.3390/s24196328 https://www.mdpi.com/journal/sensorsSensors 2024, 24, 6328 2 of 19 Recently, computer vision technology has been applied in various fields [ 14], such as target detection, image super-resolution, and population counting [15]. Regarding the counting of shrimp larvae [16–19], traditional image processing techniques primarily utilize image segmentation and object detection methods to recognize and count target images. Kesvarakul et al. counted shrimp larvae in images by converting them into binary images with a threshold [20]. However, this technique is only effective when dealing with a small quantity of larvae and when their images are clear. Thai et al. used image segmentation and contour tracking methods for the counting of shrimp larvae [21]. This method is able to count individual shrimp larvae, although it struggles to accurately distinguish shrimp larvae which are stuck together. For populations with fewer shrimp larvae, counting was performed using algorithms, such as segmentation paired with the Canny edge detection method and blob processing approaches [22]. Although this technique offers an enhance- ment over previous algorithms, it still does not address the issue of shrimp and larvae adhering to each other. Traditional image processing-based counting techniques require prior knowledge to manually adjust image features, leading to poor accuracy in scenarios with complex backgrounds. Furthermore, the lack of generalization in these models hinders the accurate detection and counting of shrimp larvae in different scenarios. Subsequent'),\n",
       " Document(metadata={}, page_content='counting techniques require prior knowledge to manually adjust image features, leading to poor accuracy in scenarios with complex backgrounds. Furthermore, the lack of generalization in these models hinders the accurate detection and counting of shrimp larvae in different scenarios. Subsequently, deep learning-based [23–26] counting methods use trained object recog- nition models to identify targets within images. Armalivia et al. used the You Only Look Once version 3 (YOLOv3) algorithm to count shrimp larvae [ 27], achieving an average counting accuracy of 96% in low-density populations of around 100 larvae (in a circular area with a diameter of 40 cm), but a larger number of larvae could not be identified. Hu et al. used deep learning and a density map of shrimp larvae to estimate the populations of approximately 1000 larvae (in a 24 cm  33 cm area) [ 13]. However, this approach does not accurately identify each individual larva. Hence, devising an automated and accurate method for counting shrimp larvae in densely populated areas remains a difficult challenge. Object detection algorithms based on deep learning mainly include Regions with Convolutional Neural Networks (R-CNN) and multiple versions of YOLO, among which YOLOv5 is a mature algorithm among target recognition algorithms which has the advantages of being lightweight and having a fast inference ability. However, in the detection and recognition of small targets, YOLOv5 is not effective, and thus we need to improve YOLOv5. In this study, we investigate the enhancement of deep learning techniques to address the challenge of counting shrimp larvae in highly populated environments. In particular, we propose an algorithm which uses regional segmentation along with an improved YOLOv5 model to accurately count shrimp larvae, especially in highly dense environments. The remainder of this paper is structured as follows. In Section 2, we give the related work to highlight our motivation. In Section 3, we describe the details of the proposed shrimp larvae counting method. Section 4 presents the experimental results and further validates the superiority of the algorithm through controlled experiments. Section 5 gives our conclusions. The main contributions are described as follows: • W e build an automatic shrimp collecting platform to obtain the corresponding dataset and provide a counting'),\n",
       " Document(metadata={}, page_content='results and further validates the superiority of the algorithm through controlled experiments. Section 5 gives our conclusions. The main contributions are described as follows: • W e build an automatic shrimp collecting platform to obtain the corresponding dataset and provide a counting algorithm based on improved YOLOv5 with region segmentation. • The C2f and attention mechanism are embedded in the YOLOv5 model, improving the detection ability for small shrimp. • The segmentation of regions aims to boost the proportion of pixels representing shrimp larvae in a visual image. • Experimental results prove that the proposed algorithm can identify shrimp larvae with high accuracy under the difficult circumstance of overlap between large numbers of shrimp and different light intensities.Sensors 2024, 24, 6328 3 of 19 2. Related Work There are many challenges in the counting of shrimp larvae. First, shrimp larvae are small in size, making them relatively difficult to detect. In addition, shrimp larvae move rapidly, and their pixel values in images differ depending on their depth in the water. Conventional threshold segmentation techniques may easily overlook shrimp larvae which are at the bottom of the water. As shown in the Figure 1, there are three states, including isolated, clumped, and overlapping shrimp larvae. Isolated shrimp larvae remain unaffected by the presence of other larvae and can be detected through conventional image processing approaches or deep learning methods. Clumped shrimp larvae consist of several larvae attached to each other, resulting in an underestimated count when using connected component analysis. This problem can be resolved through the application of watershed algorithms and distance transformation. However, these techniques necessitate precise threshold configurations and do not generalize well. Overlapping shrimp larvae appear when larvae at varying water depths align at the same spot in the image. The aforementioned watershed algorithm is ineffective in addressing this scenario. Utilizing concave point algorithms for separation might erroneously fragment the shrimp tails, causing overcounting of the shrimp. Consequently, conventional image processing techniques prove inadequate for identifying these instances, necessitating the use of deep learning approaches to accurately discern and identify the various states of shrimp larvae. Figure 1. The states of shrimp larvae. ( a) Isolated shrimp larvae. ( b) C'),\n",
       " Document(metadata={}, page_content='necessitating the use of deep learning approaches to accurately discern and identify the various states of shrimp larvae. Figure 1. The states of shrimp larvae. ( a) Isolated shrimp larvae. ( b) Clumped shrimp larvae. (c) Overlapping shrimp larvae. Minimizing the dimensions of the input image can enhance the accuracy of receptive field detection [28]. Two widely used techniques for decreasing the size of an image are image scaling and cropping. Image scaling often employs downsampling [29], which may lead to data loss. Another approach is image segmentation, which can be performed either randomly or in a consistent manner. Random segmentation employs random functions to determine the coordinates for the top-left and bottom-left sections of the local image of shrimp larvae [30]. Then, the coordinates for the top-right and bottom-right sections are determined according to the dimensions of the local image. In addition, the cropping function is applied to extract the local image. Beginning in the upper-left corner of the original image, fixed segmentation continues with sequential segmentation based on the dimensions of the local image. For regions along the edges which are smaller than the segmented dimensions, the true size of the image is maintained. Random segmentation might result in repeated recognition of the same regions, thereby increasing the training burden on the model. Consequently, this research uses fixed segmentation to handle the initial images. The estimation of density maps is also a commonly used counting method [23] which estimates the number of objects by generating a density map. This is often applied in crowd counting [31]. However, this method cannot accurately provide the geometric and positional information of objects. When the detection of small objects does not require positional information, the method can provide good counting results. In shrimp larvae counting, when the larvae density is relatively low, positional and shape information may not be critical, and both density map estimation and object detection can achieve accurateSensors 2024, 24, 6328 4 of 19 counts. However, as the larvae density increases, with overlapping larvae at different water depths, positional and shape information become important. In high-density areas, density map estimation is prone to misjudgments, leading to a decrease in count accuracy. By'),\n",
       " Document(metadata={}, page_content=', with overlapping larvae at different water depths, positional and shape information become important. In high-density areas, density map estimation is prone to misjudgments, leading to a decrease in count accuracy. By incorporating position and shape information, object detection methods can achieve better counting accuracy. In terms of runtime, density map estimation models are more complex and require a longer processing time, while object detection methods are faster, making them more suitable for quick shrimp larvae counting. YOLO [32] is considered one of the best choices for object detection models due to its speed and superior accuracy compared with R-CNN [33]. YOLO pioneered an innovative approach by utilizing convolutional neural networks on a complete image to predict object classes and bounding boxes via direct feature regression. Over the years, YOLO has evolved into several versions. YOLOv1 uses a unified detection approach for object localization and classification tasks [ 32]. Subsequently, YOLOv2 improves on version 1 by having better accuracy, a faster speed, and the ability to recognize more objects [ 34]. YOLOv3 improves the object detection speed by implementing multiscale prediction [35], optimizing the core network, and refining the loss function. YOLOv4 introduces a fast and effective object detection model which substantially cuts down computational expenses, making it more compatible with general-purpose devices and those with hardware constraints. YOLOv5 brings significant advancements, including the CSPDarknet backbone and mosaic augmentation, balancing speed and accuracy [ 36–42]. YOLOv7 improves the structure of the extended efficient layer aggregation network, revises the model architecture, and introduces an efficient label assignment strategy, which increases detection performance while decreasing the number of model parameters [43]. Currently, researchers are constantly exploring the architectural design of YOLO, with the latest version being YOLOv10 [44]. It has achieved state-of-the-art performance and efficiency on various model scales. In the experimental section, we describe the performance of YOLOv10 and conduct shrimp larvae counting tests using this model. However, the YOLOv10 model is complex and has'),\n",
       " Document(metadata={}, page_content='performance and efficiency on various model scales. In the experimental section, we describe the performance of YOLOv10 and conduct shrimp larvae counting tests using this model. However, the YOLOv10 model is complex and has high hardware resource requirements, making its hardware deployment challenging. One of the primary objectives of this study is to deploy the algorithm on hardware to achieve an integrated system for shrimp larvae image acquisition and counting. YOLOv10 does not meet the requirements of our design. YOLOv5 stands out from its predecessors with its lighter architecture, faster inference times, and more developed ecosystem which offers enhanced compatibility [45]. Moreover, YOLOv5 has low hardware resource requirements, is relatively mature in terms of hardware deployment, and is easier to implement. Therefore, we selected YOLOv5 as the baseline for our study. However, in scenarios which involve the detection of small objects, such as shrimp larvae, the use of several convolutional layers may lead to missing small targets, resulting in reduced pixel occupancy for small objects. To address this problem, we upgraded the YOLOv5 backbone network to improve its ability to identify small targets. 3. Materials and Methods For a more precise counting of shrimp larvae in situations with large numbers and a high density, we suggest an algorithm based primarily on regional segmentation and an improved YOLOv5 model. The flow chart of the algorithm can be viewed in Figure 2. To begin with, we divide the whole original image into smaller segments using a region-based segmentation algorithm. Subsequently, we employ an enhanced YOLOv5 model to recognize shrimp larvae in each of these isolated images separately. Then, all segmented image blocks are reassembled into a complete image. A repeat count removal method is proposed to address the issue of repeat counting in the stitching position, ensuring that accurate counting of shrimp larvae can be achieved throughout the image.Sensors 2024, 24, 6328 5 of 19 Figure 2. The flow chart of the proposed counting algorithm. 3.1. Image Collection The quality of the original images, which is one of the key factors for computer vision, will affect the accuracy of the counting of shrimp larvae. For the best accuracy in counting the larvae, we designed and'),\n",
       " Document(metadata={}, page_content='algorithm. 3.1. Image Collection The quality of the original images, which is one of the key factors for computer vision, will affect the accuracy of the counting of shrimp larvae. For the best accuracy in counting the larvae, we designed and fabricated a shrimp larvae image acquisition device as shown in Figure 3. Shrimp larvae are placed in a semi-transparent plastic square bucket, and white LEDs with light guide plates are added around the side wall and the bottom to provide uniform illumination. Shrimp larvae images are captured using an industrial camera (MV-CE060- 10UM (Hikvision, Hangzhou, China)) with an 8 mm lens under supplementary white light. The resolution of the camera is 3072 2048 pixels for a field of view of about40 cm  30 cm, and the effective area we used for the shrimp larvae was 30 cm  30 cm. Figure 3. Shrimp larvae acquisition device. ( a) Schematic diagram. ( b) Photographs of the actual device. The shrimp larvae dataset used in this experiment was collected from on-site photog- raphy at the Hongkai Shrimp Larvae Farm in Zhuhai City, Guangdong Province, China. Figure 4a shows the real shrimp larvae farming environment. The staff used a fishing net to extract shrimp larvae from the pool (as shown in Figure 4b) and then placed the collected larvae in our custom-built shrimp larvae image acquisition device as shown in Figure 4c.Sensors 2024, 24, 6328 6 of 19 Figure 4. Acquisition of shrimp larvae datasets in a real aquaculture scenario. ( a) Shrimp culture tank. (b) Shrimp larvae collection. (c) Acquisition device. 3.2. Region Segmentation The receptive field refers to the size of the region in the input image which corresponds to each pixel in the output feature map [ 46]; that is, a single point on the feature map corresponds to a specific region on the input image. The size of the area of the receptive field directly affects the detection accuracy [47,48]. The formula for calculating the receptive field is as follows: Ri = (Ri+1  1)  Sti'),\n",
       " Document(metadata={}, page_content='size of the area of the receptive field directly affects the detection accuracy [47,48]. The formula for calculating the receptive field is as follows: Ri = (Ri+1  1)  Sti + Ksi (1) where Ri represents the receptive field on the ith convolutional layer, Ri+1 is the receptive field on the i + 1th layer, Sti is the stride of the convolution, and KS is the size of the convolutional kernel for the current layer. When the original image of shrimp larvae ( 3072  2048 pixels) is used as the input image, the corresponding receptive field size is38 38 pixels. This indicates that each point on the feature map corresponds to a area of 38  38 pixels on the original image, which is significantly larger than the size of the shrimp larvae (approximately 10  10 pixels). As a result, information on shrimp larvae would be overlooked or covered, greatly reducing the detection accuracy. To address this issue, it is necessary to reduce the size of the receptive field to improve the shrimp larvae detection performance. Therefore, we adopted the region segmentation method to divide the original image, reducing the size of the input image and thereby decreasing the receptive field area. For example, the original image of shrimp larvae (as shown in Figure 5a) was divided into multiple image blocks (100  100 pixels for each, as shown in Figure 5b) for subsequent labeling and training. In the whole image, the proportion of shrimp larvae in terms of pixels in the whole image is extremely small. Therefore, we used segmentation technology to obtain multiple local regions of shrimp larvae, making feature extraction easier. Figure 6 shows the feature maps selected from the 17th layer of the YOLOv5 model for both the full image and the local image. To facilitate comparison, we extracted the same region from the full image as that in the local image. From Figure 6, it is evident that the full image did not accurately identify local regions, while the local image allowed a more precise extraction of features from the shrimp larvae.Sensors 2024, 24, 6328 7 of 19 Figure 5. Image segmentation. (a) Original shrimp larva'),\n",
       " Document(metadata={}, page_content='not accurately identify local regions, while the local image allowed a more precise extraction of features from the shrimp larvae.Sensors 2024, 24, 6328 7 of 19 Figure 5. Image segmentation. (a) Original shrimp larvae image. (b) Segmented image. Figure 6. The feature map referring to the shrimp larvae image. 3.3. The Improved YOLOv5 The core idea of YOLOv5 is to convert the target detection problem into a regression problem, predicting the boundary boundaries and categories of the target in the image. It has high accuracy and a fast inference ability, making it one of the best-performing target detection models available today. However, the tiny size of the shrimp larvae, along with various body positions such as their overlap, leads to a decrease in the identification accuracy of the YOLOv5 model. Therefore, the recognition accuracy can be improved by increasing the classification performance of features. As shown in Figure 7, the C2f module (Figure 8) was used to replace the C3 module. In the backbone network of the YOLOv5 algorithm, the main function of the C3 module is to extract image features and enhance the learning capacity of the convolutional network. The C3 module cannot meet the requirements for the detection of small targets such as shrimp larvae and needs further improvement to enhance the feature extraction capabilities of the model. The C2f module processes the input data using two convolutional layers, which assist in extracting features at varying levels and degrees of abstraction. This improves the feature extraction efficiency and simultaneously reduces the network weight, facilitating more abundant gradient flow information [ 49]. In contrast to the C3 module, the C2f module is more lightweight, has reduced computational demands, and demonstrates robust feature extraction capabilities.Sensors 2024, 24, 6328 8 of 19 Figure 7. The structure of the improved YOLOv5 model. Figure 8. Structure of the C2f network. To further enhance the recognition capability of the model, the convolutional block attention module (CBAM) [ 50] is introduced into the YOLOv5 framework. The CBAM consists of the channel attention and spatial attention modules, as shown in Figure 9. For an input F, the global average and maximum pooling operations are first applied to'),\n",
       " Document(metadata={}, page_content='] is introduced into the YOLOv5 framework. The CBAM consists of the channel attention and spatial attention modules, as shown in Figure 9. For an input F, the global average and maximum pooling operations are first applied to obtain global information for each channel. Subsequently, two fully connected layers are utilized to produce the channel attention vector. These layers apply weights to the input feature F per channel, which yields the refined feature F1. In the spatial attention module, the feature map is combined to capture spatial information. Furthermore, the feature F1 undergoes a convolution layer 3  3, resulting in the creation of a spatial attention map via the sigmoid function. The spatial attention map is subsequently used to enhance the feature F1, leading to production of the fused feature F2. The attention module improves the depiction of shrimp larvae features across different conditions. Figure 10 displays the feature map of both the enhanced YOLOv5 network and the original YOLOv5 network. Clearly, the proposed approach provides more precise counting outcomes even in instances of densely packed shrimp larvae.Sensors 2024, 24, 6328 9 of 19 Figure 9. Structure illustration of CBAM. Figure 10. Feature map comparison. ( a) The original YOLOv5 network. ( b) The improved YOLOv5 network. 3.4. Repeat Count Removal via Stitching The method described in Section 3.2 allows for segmentation of the entire image. The method described in Section 3.3 can identify shrimp larvae in each segmented image block. However, identical shrimp larvae can be segmented into neighboring image blocks and detected at the same time (in Figure 11), resulting in reduced counting accuracy. To address the issue of duplicate counts, an additional detection model was developed. Figure 11. Segmentation of same shrimp larvae in adjacent image blocks (dashed lines indicate the stitching positions). (a) Vertical segmentation. (b) Horizontal segmentation. (c) Both horizontal and vertical segmentation. Figure 11a shows a shrimp larva divided vertically into two sections and identified in two separate subimages. Figure 11b presents two shrimp larvae segmented horizon- tally into two subimages and identified. Figure 11c shows shrimp larva'),\n",
       " Document(metadata={}, page_content='a shrimp larva divided vertically into two sections and identified in two separate subimages. Figure 11b presents two shrimp larvae segmented horizon- tally into two subimages and identified. Figure 11c shows shrimp larvae cut horizontally and vertically, resulting in four distinct subimages. Figure 12 shows a schematic illustra- tion of the segmentation of shrimp larvae, helping in the evaluation of the segmentation configurations. In Figure 12, the red boxes represent the detection boundary boxes of the detection output, and the black lines represent the cropping boundaries of the image. Taking intoSensors 2024, 24, 6328 10 of 19 account the spatial relationship between the detection boundary boxes and the image cropping boundaries in the output, nine distinct scenarios can be identified: undivided shrimp larvae (N), top edge touching (U), bottom edge touching (D), right edge touching (R), left edge touching (L), right and bottom edges touching (RD), right and top edges touching (RU), left and bottom edges touching(LD), and left and top edges touching(LU). A set S, where S = N, D, U, R, L, RD, RU, LD, LU, was defined to store all detection output. By traversing the detection outputs and checking the boundaries, the detection results were placed in the corresponding sets. Figure 12. Schematic diagram of shrimp larvae segmentation. (a) Vertical segmentation. (b) Horizon- tal segmentation. (c) Both horizontal and vertical segmentation. To avoid duplication, we examined the shrimp larva detection boxes in neighboring regions to determine whether the detected shrimp larvae were identical. If several detection boxes corresponded to each other, as illustrated in Figure 13, then(xi, yi) are the coordinates of the vertices of the detection frame, and the detection box at the bottom which intersects the cropping boundary coincides with the two detection boxes at the top, which also intersect with the cropping boundary. Therefore, an overlap ratio must be implemented to facilitate the selection process. We define a variable Ro to represent the percentage of overlap between detection frames, whose formula is as follows: Ro = x4  x1 x2  x3 (2) where '),\n",
       " Document(metadata={}, page_content='to facilitate the selection process. We define a variable Ro to represent the percentage of overlap between detection frames, whose formula is as follows: Ro = x4  x1 x2  x3 (2) where x1, x2, x3, x4 are the horizontal coordinates of the detection frame. Figure 13. Multiple detection box matching. By calculating the overlap ratio, the detection box with the highest overlap ratio is selected. After determining the matching object, a minimum bounding rectangle is used to completely enclose the two detection boxes, thus identifying the deduplication area (indicated in Figure 14a, for example). The two red detection boxes in Figure 14bSensors 2024, 24, 6328 11 of 19 correspond to a typical repeat count case (according to Figure 11b). We determined whether they were connected by examining the coordinates of the two detection boxes. Once the connected boxes were identified, we used a minimum enclosing box (indicated by the green rectangle in Figure 14a,c) to completely cover the two red detection boxes and define the deduplication area. Figure 14. The deduplication area. (a) Schematic diagram of how to find the deduplication area (the dashed line indicates region segmentation positions), where the two red rectangles are the detection boxes and the corresponding minimum enclosing rectangle is drawn in green. (b) Original detection result with two detection boxes detecting the same larva. (c) Minimum enclosing rectangle for the case in (b). Once the deduplication area is identified, it is fed back into the enhanced YOLOv5 network mentioned in Section 3.3 to accurately count the shrimp larvae in this region. This process ensures that duplicate counts are removed from the entire composite image of shrimp larvae. 4. Experimental Results and Analysis 4.1. Implementation Details In the experiment, we collected 20,000 valid local images of shrimp larvae, with 15,000 allocated for training and 5000 for testing. For annotating the dataset, the conven- tional labeling approach (Figure 15a) involves describing all the shrimp larvae, yet this can introduce redundant details and impede the model’s ability to learn the features of shrimp larvae. We marked the heads of the shrimp larvae'),\n",
       " Document(metadata={}, page_content='ional labeling approach (Figure 15a) involves describing all the shrimp larvae, yet this can introduce redundant details and impede the model’s ability to learn the features of shrimp larvae. We marked the heads of the shrimp larvae as shown in Figure 15b. This annotation method concentrates on the feature of the shrimp larvae, making it easier to distinguish overlapping larvae. The comparison algorithms included the YOLOv5 algo- rithm (which means using YOLOv5 without regional segmentation), the Mask R-CNN algorithm [17], and popular commercial software (Smart Shrimp Farm V2.1.3 [ 51]). In addition, the proposed algorithm was performed on an NVIDIA RTX 3080Ti (NVIDIA, Santa Clara, CA, USA). A total of 500 epochs were trained, with the learning rate set to 0.001. The other parameters were the default YOLOv5 parameters. The optimizer was the stochastic gradient descent. Figure 15. Dataset annotation. (a) The traditional labeling method. (b) The annotation methodwe used.Sensors 2024, 24, 6328 12 of 19 We used images of shrimp larvae of different densities to evaluate the performance of the proposed method. Under the fixed field of view provided (30 cm  30 cm), we examined four different densities of shrimp larvae: approximately 1000 larvae for density level 1, approximately 2300 larvae for density level 2, approximately 4000 larvae for density level 3, and roughly 5000 larvae for density level 4. We chose 100  100 pixels for the segmentation image size in the counting experiments hereafter. The dataset and codes can be requested by email (Hongchao Duan: 2021024069@m.scnu.edu.cn). 4.2. Evaluation Criterion To quantitatively evaluate the performance of the proposed algorithm, we define A as the measurement for the counting accuracy of the formula as follows: A = Ac  |Id  Ac| Ac  100% (3) where Ac represents the actual number of shrimp larvae and Id denotes the number of shrimp larvae. For accurate results, 15 images of shrimp larvae were analyzed at each density level, with the mean count accuracy serving as the statistical result for each level.'),\n",
       " Document(metadata={}, page_content='number of shrimp larvae and Id denotes the number of shrimp larvae. For accurate results, 15 images of shrimp larvae were analyzed at each density level, with the mean count accuracy serving as the statistical result for each level. 4.3. Experiment Comparison To verify the effectiveness of the above proposed method, we collected a large number of shrimp images of different densities (four densities mentioned in Section 4.1) for the validation experiment. Figure 16a–d shows the recognition effect of the proposed method under different densities. It can be seen in Figure 16 that the proposed algorithm had a good recognition effect for different densities, especially in places where shrimp larvae were densely populated. Figure 16. Counting results at different densities. ( a) Density 1. ( b) Density 2. ( c) Density 3. (d) Density 4.Sensors 2024, 24, 6328 13 of 19 Figure 17a shows a typical local image of the original high-density shrimp larvae (approximately 5000 shrimp larvae in a 30 cm  30 cm area), and Figure 17b shows the corresponding identification results. It shows that the proposed method exhibited high recognition accuracy even when the density of the shrimp larvae was high and there were overlaps between larvae. This indicates that the regional segmentation approach adopted in this study makes the receptive field more accurate, which makes the improved YOLOv5 model more accurate in the recognition of shrimp larvae targets. Figure 17. A typical local area of Figure 16. (a) Original image. (b) Detection results. Among the various comparison algorithms, Smart Shrimp Farm only provided the final count results without the detection images. We tested high-density shrimp larvae images (approximately 5000 shrimp larvae in a 30 cm  30 cm area) using the remaining algorithms, and the comparison of the test results is shown in Figure 18. For Density Map Regression, we provide the corresponding density map in Figure 18f. Figure 18. Test result images for different algorithms. ( a) The proposed method. ( b) YOLOv5. (c) Mask R-CNN. (d) YOLOv10. (e'),\n",
       " Document(metadata={}, page_content='18f. Figure 18. Test result images for different algorithms. ( a) The proposed method. ( b) YOLOv5. (c) Mask R-CNN. (d) YOLOv10. (e) Shrimpseed_Net. (f) Density map regression. Table 1 presents the statistical results of the average counting accuracy A for the comparison algorithm.Sensors 2024, 24, 6328 14 of 19 Table 1. Average counting accuracy A for different shrimp larvae densities. Method Density 1 Density 2 Density 3 Density 4 The Proposed Method 99.32 % 99.17% 98.53% 98.23% Smart Shrimp Farm [51] 91.43% 90.35% 88.25% 82.99% YOLOv5 [52] 87.79% 85.46% 83.66% 81.03% Mask R-CNN [17] 86.63% 84.52% 81.25% 80.32% YOLOv10 [44] 91.41% 90.68% 89.29% 88.75% Shrimpseed_Net [53] 91.65% 90.23% 89.47% 87.62% Density Map Regression [23] 92.33% 91.58% 90.75% 88.61% The results clearly indicate that, compared with commercial software such as YOLOv5, YOLOv10, Shrimpseed_Net, Density Map Regression, and Mask R-CNN, the proposed algorithm excelled in counting shrimp larvae at various densities, particularly in high- density scenarios. Although the accuracy decreased somewhat as the density increased, the overall recognition accuracy of the proposed method remained above 98%, significantly outperforming the other three cases (just above 80%). Moreover, the accuracy, which exceeded 98%, demonstrates that the proposed algorithm could potentially substitute the manual shrimp counting process on farms. 4.4. Ablation Study To validate the superiority of the improved YOLOv5 model proposed in this article, we compared the identification results in the segmentation images of100  100 pixels using both the original YOLOv5 model and the improved YOLOv5 model in Section 3.3 (typical results are shown in Figure'),\n",
       " Document(metadata={}, page_content=', we compared the identification results in the segmentation images of100  100 pixels using both the original YOLOv5 model and the improved YOLOv5 model in Section 3.3 (typical results are shown in Figure 19). Figure 19. Recognition performance of different algorithm models. ( a) Original YOLOv5 model. (b) Improved YOLOv5 model. As shown in Figure 19, the improved YOLOv5 model demonstrated better recognition performance when the shrimp larvae were densely packed. To quantitatively analyze the differences between the two models, we used the counting algorithm on all images, and the results of the average counting accuracy for the four density levels are summarized in Table 2. For density 4, we collected images of shrimp larvae under three different lighting conditions (bright, normal brightness, and dim) for testing. The test results are shown in Figure 20, and the data are summarized in Table 2. The statistical results in Table 2 show that the improved YOLOv5 model achieved an approximately 2% higher counting accuracy compared with the original YOLOv5 model, validating the effectiveness of the proposed method. Under different lighting conditions, the counting accuracy of our proposed algorithm exceeded 98%, outperforming the original YOLOv5 algorithm.Sensors 2024, 24, 6328 15 of 19 Figure 20. Test results under three different lighting conditions. (a) Bright. (b) Normal brightness. (c) Dim. Table 2. Average counting accuracy A for the two YOLOv5 models. Density The Original YOLOv5 The Improved YOLOv5 Density 1 98.13% 99.32% Density 2 97.85% 99.17% Density 3 96.81% 98.53% Density 4 (Bright) 96.26% 98.23% Density 4 (Normal Brightness) 94.66% 98.21% Density 4 (Dim) 93.52% 98.17% The original image of the larvae was divided into many local areas to decrease the size of the receptive field in Section 3.2. To evaluate the impact of segmentation image sizes on the accuracy of shrimp larvae detection, we cropped the original shrimp larvae images into five different sizes: 600  600'),\n",
       " Document(metadata={}, page_content='the size of the receptive field in Section 3.2. To evaluate the impact of segmentation image sizes on the accuracy of shrimp larvae detection, we cropped the original shrimp larvae images into five different sizes: 600  600 pixels, 300  300 pixels, 200  200 pixels, 100  100 pixels, and 50  50 pixels (as shown in Figure 21). Subsequently, we used the improved YOLOv5 model to train and test these cropped images of different sizes separately. Figure 21. Recognition results for different segmentation image sizes: ( a) 600  600 pixels, (b) 300  3000 pixels, (c) 200  200 pixels, (d) 100  100 pixels, and (e) 50  50 pixels. As shown in Figure 21, when the segmentation image size was 600  600 pixels, many shrimp larvae were not identified. As the segmentation size decreased, the number of undetected shrimp larvae also decreased. When the segmentation image size was reducedSensors 2024, 24, 6328 16 of 19 to 100  100 pixels and 50  50 pixels, all shrimp larvae in the local images were detected. To quantitatively analyze the differences between the various segmentation image sizes, we tested the counting accuracy using images with four different densities of shrimp larvae mentioned in Section 4.1, and the statistical results of the average counting accuracy A for each level of density of shrimp larvae are presented in Table 3. Table 3. Average counting accuracy A for different segmentation sizes. Density 600  600 Pixels 300  300 Pixels 200  200 Pixels 100  100 Pixels 50  50 Pixels Density 1 91.69% 94.27% 97.79% 99.32% 99.34% Density 2 90.83% 93.85% 97.24% 99.17% 99.20% Density 3 88.72% 93.31% 96.38% 98.53% 98.87% Density 4 88.15% 92.56% 95.93% 98.23% 98.21% The results indicate that the size of the image segmentation had a significant impact on the accuracy of the counting of shrimp larvae. As the segmentation image size decreased, the accuracy'),\n",
       " Document(metadata={}, page_content='.93% 98.23% 98.21% The results indicate that the size of the image segmentation had a significant impact on the accuracy of the counting of shrimp larvae. As the segmentation image size decreased, the accuracy of the shrimp larvae count clearly increased. When the segmentation image size was reduced to 50  50 pixels, the counting accuracy was the best, but we also realize that the cost of computing increased exponentially. Considering the calculation cost and the counting accuracy, we chose 100  100 pixels for the segmentation image size in the counting experiments hereafter. 4.5. Discussion Comparative experiments showed that the proposed region segmentation algorithm improved the accuracy of counting shrimp larvae. In contrast to the standard YOLOv5 model, the enhanced YOLOv5 variant provides more accurate detection of small objects such as shrimp larvae. Validation experiments indicated that the proposed algorithm performed better, with a counting accuracy of 98% even under conditions with high densities and large volumes of shrimp larvae. However, there are still aspects which need to be improved. To begin with, the dataset for this experiment was gathered using an optical platform which we designed. To improve the generalizability of the algorithm, it is essential to include images of shrimp larvae obtained from various environments and a range of imaging devices in the training dataset. In addition, the stitching and deduplication process occasionally overlooks a few shrimp larvae due to stitching errors, suggesting that this algorithm needs further improvement. In conclusion, the proposed algorithm can be integrated into shrimp larvae imaging devices to develop a comprehensive shrimp larvae counting system. The image segmentation approach and the improved YOLOv5 model proposed in this paper not only solve the problem of shrimp larvae counting but also have significant research potential in various fields, such as dense crowd counting and medical image analysis. Using segmentation algorithms, the proportion of the pixel of the target objects in the input images increased, thereby improving the detection accuracy. However, the algorithm needs to be adjusted according to the specific problem at hand. 5. Conclusions We introduced a method for counting shrimp larvae which uses region segmentation along with an improved YOLOv5 model. By segmenting the regions, we can reduce the receptive field area, thereby enhancing the detection accuracy of shrimp larvae using'),\n",
       " Document(metadata={}, page_content='counting shrimp larvae which uses region segmentation along with an improved YOLOv5 model. By segmenting the regions, we can reduce the receptive field area, thereby enhancing the detection accuracy of shrimp larvae using the improved YOLOv5 model. Furthermore, the deduplication model can tackle the problem of repeated counts. Based on this, the experimental results show that the number of larvae was about 5000, and the counting accuracy of our algorithm remained above 98%, which would be suitable for replacing the work of manually counting shrimp larvae.Sensors 2024, 24, 6328 17 of 19 Author Contributions: Conceptualization, Y.Z.; methodology, Y.Z., J.W. and H.D.; software, J.W. and H.D.; validation, X.W., T.P ., X.L. and D.D.; formal analysis, J.W. and H.D.; investigation, J.W., X.W. and H.D.; resources, T.P .; data curation, D.D.; writing—original draft preparation, H.D.; writing— review and editing, Y.Z.; visualization, Y.Z.; supervision, Y.Z.; project administration, Y.Z.; funding acquisition, Y.Z. All authors have read and agreed to the published version of the manuscript. Funding: This research received no external funding. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable. Data Availability Statement: Data are available from the authors on request. Conflicts of Interest: The authors declare no conflicts of interest. References 1. Racotta, I.S.; Palacios, E.; Ibarra, A.M. Shrimp larval quality in relation to broodstock condition. Aquaculture 2003, 227, 107–130. [CrossRef] 2. Li, D.; Miao, Z.; Peng, F.; Wang, L.; Hao, Y.; Wang, Z.; Chen, T.; Li, H.; Zheng, Y. Automatic counting methods in aquaculture: A review.'),\n",
       " Document(metadata={}, page_content='Peng, F.; Wang, L.; Hao, Y.; Wang, Z.; Chen, T.; Li, H.; Zheng, Y. Automatic counting methods in aquaculture: A review. J. World Aquac. Soc.2021, 52, 269–283. [CrossRef] 3. Tierney, T.W.; Fleckenstein, L.J.; Ray, A.J. The effects of density and artificial substrate on intensive shrimp Litopenaeus vannamei nursery production. Aquac. Eng.2020, 89, 102063. [CrossRef] 4. Naegel, L.C.; Gómez-Humarán, I.M. Effect of sample volume and population density on precision of larval population estimates. Aquac. Eng.1998, 17, 11–19. [CrossRef] 5. Hsieh, Y.K.; Hsieh, J.W.; Hu, W.C.; Tseng, Y.C. AIoT-Based Shrimp Larvae Counting System Using Scaled Multilayer Feature Fusion Network. IEEE Internet Things J.2024, early Access. [CrossRef] 6. Zhang, L.; Li, W.; Liu, C.; Zhou, X.; Duan, Q. Automatic fish counting method using image density grading and local regression. Comput. Electron. Agric.2020, 179, 105844. [CrossRef] 7. Yeh, C.T.; Ling, M.S. Portable Device for Ornamental Shrimp Counting Using Unsupervised Machine Learning. Sens. Mater. 2021, 33, 3027–3036. [CrossRef] 8. Martinez-Palacios, C.; Novoa, M.O.; Chavez-Martinez, C. A simple apparatus for self-separation of post-larval prawns, Macrobrachium spp., in mixed larval rearing tanks. Aquac. Res.1985, 16, 341–348. [CrossRef] 9. Work, H.P '),\n",
       " Document(metadata={}, page_content='prawns, Macrobrachium spp., in mixed larval rearing tanks. Aquac. Res.1985, 16, 341–348. [CrossRef] 9. Work, H.P .S. Photoelectric Sensors for Counting and Classifying Vehicles. InTransportation Research Record 1311; Transportation Research Board: Washington, DC, USA, 1991; p. 79. 10. Carmichael, H. Photoelectric Detection II. In An Open Systems Approach to Quantum Optics: Lectures Presented at the Université Libre de Bruxelles October 28 to November 4, 1991; Springer: Berlin/Heidelberg, Germany, 1993; pp. 93–112. 11. Spratt, M.D. Preliminary results of a computer imaging system applied to estimating the quantity of larvae and fingerling fish for aquaculture. In Fish Quality Control by Computer Vision; Routledge: New York, NY, USA, 2017; pp. 263–282. 12. Awalludin, E.; Muhammad, W.W.; Arsad, T.; Yussof, W.H.W. Fish larvae counting system using image processing techniques. In Proceedings of the Journal of Physics Conference Series; IOP Publishing: Bandung, Indonesia, 2020; Volume 1529, p. 052040. 13. Hu, W.C.; Chen, L.B.; Hsieh, M.H.; Ting, Y.K. A deep-learning-based fast counting methodology using density estimation for counting shrimp larvae. IEEE Sens. J.2022, 23, 527–535. [CrossRef] 14. Sun, Y.; Lin, Y.; Zhao, G.; Svanberg, S. Identification of flying insects in the spatial, spectral, and time domains with focus on mosquito imaging. Sensors 2021, 21, 3329. [CrossRef] 15. Patwal, A.; Diwakar, M.; Tripathi, V .; Singh, P . Crowd counting analysis using deep learning: A critical review.Procedia Comput. Sci. 2023, 218, 2448–2458. ['),\n",
       " Document(metadata={}, page_content='ar, M.; Tripathi, V .; Singh, P . Crowd counting analysis using deep learning: A critical review.Procedia Comput. Sci. 2023, 218, 2448–2458. [CrossRef] 16. Kaewchote, J.; Janyong, S.; Limprasert, W. Image recognition method using Local Binary Pattern and the Random forest classifier to count post larvae shrimp. Agric. Nat. Resour.2018, 52, 371–376. [CrossRef] 17. Nguyen, K.T.; Nguyen, C.N.; Wang, C.Y.; Wang, J.C. Two-phase instance segmentation for whiteleg shrimp larvae counting. In Proceedings of the 2020 IEEE International Conference on Consumer Electronics (ICCE), Las Vegas, NV , USA, 4–6 January 2020; pp. 1–3. 18. Qu, Y.; Jiang, S.; Li, D.; Zhong, P .; Shen, Z. SLCOBNet: Shrimp larvae counting network with overlapping splitting and Bayesian-DM-count loss. Biosyst. Eng.2024, 244, 200–210. [CrossRef] 19. Zhang, J.; Yang, G.; Sun, L.; Zhou, C.; Zhou, X.; Li, Q.; Bi, M.; Guo, J. Shrimp egg counting with fully convolutional regression network and generative adversarial network. Aquac. Eng.2021, 94, 102175. [CrossRef] 20. Kesvarakul, R.; Chianrabutra, C.; Chianrabutra, S. Baby shrimp counting via automated image processing. In Proceedings of the 9th International Conference on Machine Learning and Computing, Singapore, 24–26 February 2017; pp. 352–356. 21. Thai, T.T.N.; Nguyen, T.S.; Pham, V .C. Computer vision based estimation of shrimp population density and size. In Proceedings of the 2021 International Symposium on Electrical and Electronics Engineering (ISEE'),\n",
       " Document(metadata={}, page_content='N.; Nguyen, T.S.; Pham, V .C. Computer vision based estimation of shrimp population density and size. In Proceedings of the 2021 International Symposium on Electrical and Electronics Engineering (ISEE), Ho Chi Minh, Vietnam, 15–16 April 2021; pp. 145–148.Sensors 2024, 24, 6328 18 of 19 22. Awalludin, E.A.; Yaziz, M.M.; Rahman, N.A.; Yussof, W.N.J.H.W.; Hitam, M.S.; Arsad, T.T. Combination of canny edge detection and blob processing techniques for shrimp larvae counting. In Proceedings of the 2019 IEEE International Conference on Signal and Image Processing Applications (ICSIPA), Kuala Lumpur, Malaysia, 17–19 September 2019; pp. 308–313. 23. Zhou, C.; Yang, G.; Sun, L.; Wang, S.; Song, W.; Guo, J. Counting, locating, and sizing of shrimp larvae based on density map regression. Aquac. Int.2024, 32, 3147–3168. [CrossRef] 24. Zhang, L.; Zhou, X.; Li, B.; Zhang, H.; Duan, Q. Automatic shrimp counting method using local images and lightweight YOLOv4. Biosyst. Eng.2022, 220, 39–54. [CrossRef] 25. Bereciartua-Pérez, A.; Gómez, L.; Picón, A.; Navarra-Mestre, R.; Klukas, C.; Eggers, T. Insect counting through deep learning- based density maps estimation. Comput. Electron. Agric.2022, 197, 106933. [CrossRef] 26. Yu, X.; Wang, J.; Wen, S.; Yang, J.; Zhang, F. A deep learning based feature extraction method on hyperspectral images for nondestructive prediction of TVB-N content'),\n",
       " Document(metadata={}, page_content=', X.; Wang, J.; Wen, S.; Yang, J.; Zhang, F. A deep learning based feature extraction method on hyperspectral images for nondestructive prediction of TVB-N content in Pacific white shrimp ( Litopenaeus vannamei). Biosyst. Eng. 2019, 178, 244–255. [CrossRef] 27. Armalivia, S.; Zainuddin, Z.; Achmad, A.; Wicaksono, M.A. Automatic counting shrimp larvae based you only look once (YOLO). In Proceedings of the 2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS), Bandung, Indonesia, 28–30 April 2021; pp. 1–4. 28. Liu, S.; Huang, D. Receptive field block net for accurate and fast object detection. In Proceedings of the European conference on Computer Vision (ECCV), Munich, Germany, 8–14 September 2018; pp. 385–400. 29. Zhang, Y.; Zhao, D.; Zhang, J.; Xiong, R.; Gao, W. Interpolation-dependent image downsampling. IEEE Trans. Image Process.2011, 20, 3291–3296. [CrossRef] 30. Lan, Y.h.; Zhang, Y.; Li, C.h.; Zhao, X.f. A novel image segmentation method based on random walk. In Proceedings of the 2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA), Wuhan, China, 28–29 November 2009; Volume 1, pp. 207–210. 31. Lin, W.; Chan, A.B. Optimal transport minimization: Crowd localization on density maps for semi-supervised counting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 18–22 June 2023; pp. 21663–21673. 32. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once: Unified, real-'),\n",
       " Document(metadata={}, page_content='; pp. 21663–21673. 32. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV , USA, 27–30 June 2016; pp. 779–788. 33. Han, X.; Chang, J.; Wang, K. You only look once: Unified, real-time object detection. Procedia Comput. Sci.2021, 183, 61–72. [CrossRef] 34. Redmon, J.; Farhadi, A. YOLO9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 7263–7271. 35. Redmon, J. Yolov3: An incremental improvement. arXiv 2018, arXiv:1804.02767. 36. Hussain, M. YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision. arXiv 2024, arXiv:2407.02988. 37. Fu, H.; Song, G.; Wang, Y. Improved YOLOv4 marine target detection combined with CBAM. Symmetry 2021, 13, 623. [CrossRef] 38. Wang, Z.; Jin, L.; Wang, S.; Xu, H. Apple stem/calyx real-time recognition using YOLO-v5 algorithm for fruit automatic loading system. Postharvest Biol. Technol.2022, 185, 111808. [CrossRef] 39. Kim, J.H.; Kim, N.; Park, Y.W.; Won, C.S. Object detection and classification based on YOLO-V5 with improved maritime dataset. J. Mar. Sci. Eng.2022, 10, 377. [CrossRef] 40. Yang,'),\n",
       " Document(metadata={}, page_content=', C.S. Object detection and classification based on YOLO-V5 with improved maritime dataset. J. Mar. Sci. Eng.2022, 10, 377. [CrossRef] 40. Yang, R.; Li, W.; Shang, X.; Zhu, D.; Man, X. KPE-YOLOv5: An improved small target detection algorithm based on YOLOv5. Electronics 2023, 12, 817. [CrossRef] 41. Wang, L.; Liu, X.; Ma, J.; Su, W.; Li, H. Real-time steel surface defect detection with improved multi-scale YOLO-v5. Processes 2023, 11, 1357. [CrossRef] 42. Sun, Q.; Li, P .; He, C.; Song, Q.; Chen, J.; Kong, X.; Luo, Z. A Lightweight and High-Precision Passion Fruit YOLO Detection Model for Deployment in Embedded Devices. Sensors 2024, 24, 4942. [CrossRef] 43. Wang, C.Y.; Bochkovskiy, A.; Liao, H.Y.M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 18–22 June 2023; pp. 7464–7475. 44. Wang, A.; Chen, H.; Liu, L.; Chen, K.; Lin, Z.; Han, J.; Ding, G. Yolov10: Real-time end-to-end object detection. arXiv 2024, arXiv:2405.14458. 45. Dong, X.; Yan, S.; Duan, C. A lightweight vehicles detection network model based on YOLOv5. Eng. Appl. Artif. Intell.2022, 113, 104914. [CrossRef] 46'),\n",
       " Document(metadata={}, page_content='.; Duan, C. A lightweight vehicles detection network model based on YOLOv5. Eng. Appl. Artif. Intell.2022, 113, 104914. [CrossRef] 46. Chai, E.; Ta, L.; Ma, Z.; Zhi, M. ERF-YOLO: A YOLO algorithm compatible with fewer parameters and higher accuracy. Image Vis. Comput. 2021, 116, 104317. [CrossRef] 47. Kim, B.J.; Choi, H.; Jang, H.; Lee, D.G.; Jeong, W.; Kim, S.W. Dead pixel test using effective receptive field. Pattern Recognit. Lett. 2023, 167, 149–156. [CrossRef] 48. Wang, Q.; Qian, Y.; Hu, Y.; Wang, C.; Ye, X.; Wang, H. M2YOLOF: Based on effective receptive fields and multiple-in-single-out encoder for object detection. Expert Syst. Appl.2023, 213, 118928. [CrossRef]Sensors 2024, 24, 6328 19 of 19 49. Chen, Y.; Zhan, S.; Cao, G.; Li, J.; Wu, Z.; Chen, X. C2f-Enhanced YOLOv5 for Lightweight Concrete Surface Crack Detection. In Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications, Wuhan, China, 18–20 November 2023; pp. 60–64. 50. Wang, W.; Tan, X.; Zhang, P .; Wang, X. A CBAM based multiscale transformer fusion approach for remote sensing image change detection. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.2022, 15, 6817–6825. [CrossRef] 51. Smart Shrimp Farm. 2024. Available online: http://m.ux6.com/app/2228'),\n",
       " Document(metadata={}, page_content='Obs. Remote Sens.2022, 15, 6817–6825. [CrossRef] 51. Smart Shrimp Farm. 2024. Available online: http://m.ux6.com/app/222810.html (accessed on 1 March 2024). 52. Zhang, Y.; Guo, Z.; Wu, J.; Tian, Y.; Tang, H.; Guo, X. Real-time vehicle detection based on improved yolo v5. Sustainability 2022, 14, 12274. [CrossRef] 53. Liu, D.; Xu, B.; Cheng, Y.; Chen, H.; Dou, Y.; Bi, H.; Zhao, Y. Shrimpseed_Net: Counting of shrimp seed using deep learning on smartphones for aquaculture. IEEE Access2023, 11, 85441–85450. [CrossRef] Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_ques_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1756902930942,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "9qXgO_fpG_ni",
    "outputId": "da75e467-866d-4fec-caa5-b7a976e2074c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_ques_gen[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "8774c5c350fb4025a904381cc4279d75",
      "f4e755663307409c964ecc36a035bc31",
      "29bf2c450d194d2f87474872aa678428",
      "b92bf32c6a6f485ebbd618aa3ed1a4fa",
      "6de677c870a8439a968de2ae0ee92516",
      "072172f5a0974fc79b43977e284eec66",
      "d9bc5c8b9869497da6ce1322c8a42010",
      "9e5471fe4cc540638361ed5b7a8751dc",
      "81ec678ad05f441eb97fd69311129888",
      "7276ee4304b1420cbb9e9662d8115b6e",
      "20085efd569c4472bab8e83c564e3cae",
      "d773a45cdd0e478492d66400c4fe46fd",
      "818802eb2866440e98f05057ef485964",
      "ec872adf833c42dd86c354ca10b25c8a",
      "88c0cf0084af4e96b6c0b82725a9689c",
      "554e29fbfb874af7a0ae6ff85a4a19a2",
      "a35c3693d5a64ff2ae71938b61e2c7d0",
      "80884ff5b14e492d80035ee516517157",
      "e2e1c33b4bd94810b5159a23f35f4e39",
      "ab40da8759e346c1954afd317d0cdb11",
      "d7541383256e4e428ac407c4d4249d0c",
      "da9e7e87b742467283c7db476b909672",
      "4d1d415f52a147ec90197eeba1533f8c",
      "aa6d28a0dca443229e40ca6eec98ac6d",
      "97817ff6586543968b0c2b47b38cce17",
      "5f0b0dbc284f4ac4b234b962ce8bd719",
      "602874d9b1cb45cdafe8547cefdb301e",
      "1b9d2ac8f45f4d62bbcb4b307e6f5ac7",
      "21970613bfcc44f08a9df53e6248e659",
      "bd97861f615541d884f20bc467e421dc",
      "8bbf43fdd7094d9ea7e47135269912ee",
      "e3ecb98fec034b6aa5c0b18aae264f3e",
      "ce861c38dd06484e854ead8fa98acf13"
     ]
    },
    "executionInfo": {
     "elapsed": 85282,
     "status": "ok",
     "timestamp": 1756903025897,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "5tLzU0jFhXb-",
    "outputId": "d29a0a5e-b0d2-49a3-f0b0-e9207d309099"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\d\\generative AI\\Doc2Question\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_16168\\236193542.py:9: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm_ques_gen_pipeline = HuggingFacePipeline(pipeline=pipeline)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto')\n",
    "pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=450)\n",
    "llm_ques_gen_pipeline = HuggingFacePipeline(pipeline=pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1756903026406,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "5K8gYb2RG_nj"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1756903026651,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "LScxMwgeG_nj"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1756903026672,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "vRV0B2_YG_nj"
   },
   "outputs": [],
   "source": [
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756903026692,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "7bCOwRJFG_nj"
   },
   "outputs": [],
   "source": [
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756903026695,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "XpuGWQULG_nj"
   },
   "outputs": [],
   "source": [
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1756903026836,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "cHpc6VHlG_nj"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1756903026888,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "624beEPrG_nk"
   },
   "outputs": [],
   "source": [
    "ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline,\n",
    "                                          chain_type = \"refine\",\n",
    "                                          verbose = True,\n",
    "                                          question_prompt=PROMPT_QUESTIONS,\n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1756903773595,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "_si9jv47oAdF",
    "outputId": "0886dcc7-f808-4b0d-defc-2e0a0d4f9baa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8X1T61b4G_nk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_16168\\105993891.py:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  ques = ques_gen_chain.run([chunk])\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "Citation: Duan, H.; Wang, J.; Zhang, Y.; Wu, X.; Peng, T.; Liu, X.; Deng, D. Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation. Sensors 2024, 24, 6328. https://doi.org/10.3390/ s24196328 Academic Editor: Yongwha Chung Received: 10 September 2024 Revised: 26 September 2024 Accepted: 27 September 2024 Published: 30 September 2024 Copyright:  2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). sensors Article Shrimp Larvae Counting Based on Improved YOLOv5 Model with Regional Segmentation Hongchao Duan, Jun Wang, Yuan Zhang *, Xiangyu Wu, Tao Peng, Xuhao Liu and Delong Deng Centre for Optical and Electromagnetic Research, South China Academy of Advanced Optoelectronics, South China Normal University, Guangzhou 510006, China; 2021024069@m.scnu.edu.cn (H.D.); 2020023787@m.scnu.edu.cn (J.W.); 2022024121@m.scnu.edu.cn (X.W.); 2023024193@m.scnu.edu.cn (T.P .); 2023024174@m.scnu.edu.cn (D.D.) * Correspondence: yuan.zhang@coer-scnu.org Abstract: Counting shrimp larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "larvae is an essential part of shrimp farming. Due to their tiny size and high density, this task is exceedingly difficult. Thus, we introduce an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach. First, the C2f and convolutional block attention modules are used to improve the capabilities of YOLOv5 in recognizing the small shrimp. Moreover, employing a regional segmentation technique can decrease the receptive field area, thereby enhancing the shrimp counter’s detection performance. Finally, a strategy for stitching and deduplication is implemented to tackle the problem of double counting across various segments. The findings from the experiments indicate that the suggested algorithm surpasses several other shrimp counting techniques in terms of accuracy. Notably, for high-density shrimp larvae in large quantities, this algorithm attained an accuracy exceeding 98%. Keywords: shrimp larvae counting; YOLOv5; regional segmentation; attention mechanism; repeat shrimp removal 1. Introduction Shrimp farming has become a vital economic sector within aquaculture, contributing significantly to the growth of the fish industry [1]. Counting shrimp larvae is an essential task in the shrimp farming process, as it assists farmers in determining the reproductive rate and accurately estimating the production potential [2]. In addition, it helps evaluate fertility, control the density of cultivation, and manage transport sales [ 3,4]. However, due to the tiny size, great flexibility, and dense numbers of shrimp larvae, counting them presents a significant challenge [5]. Currently, the process is predominantly carried out by hand, making it a time-consuming, labor-intensive, and somewhat imprecise task [6]. Thus, developing an innovative approach to overcome the current difficulties in counting shrimp larvae is of the utmost importance. The progress in the counting of shrimp larvae consists primarily of manual count- ing [7], photoelectric detector counting [ 8–11], counting based on conventional image processing [12], and deep learning-based counting [13]. The first technique is highly depen- dent on the operator’s expertise, resulting in substantial differences in the accuracy and efficiency of the counting process. In addition, the process of manual counting may harm shrimp larvae.\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "counting [13]. The first technique is highly depen- dent on the operator’s expertise, resulting in substantial differences in the accuracy and efficiency of the counting process. In addition, the process of manual counting may harm shrimp larvae. Photoelectric devices for counting shrimp larvae work by shining light on the larvae and determining their number based on the light reflected back to the sensors [8]. Although this method significantly improves both the speed and accuracy, these counters can be affected by various environmental factors, such as lighting conditions and water quality, resulting in considerable counting errors. Furthermore, photoelectric devices are generally effective for shrimp larvae of certain sizes, which reduces their accuracy when dealing with larvae of different sizes. Sensors 2024, 24, 6328. https://doi.org/10.3390/s24196328 https://www.mdpi.com/journal/sensorsSensors 2024, 24, 6328 2 of 19 Recently, computer vision technology has been applied in various fields [ 14], such as target detection, image super-resolution, and population counting [15]. Regarding the counting of shrimp larvae [16–19], traditional image processing techniques primarily utilize image segmentation and object detection methods to recognize and count target images. Kesvarakul et al. counted shrimp larvae in images by converting them into binary images with a threshold [20]. However, this technique is only effective when dealing with a small quantity of larvae and when their images are clear. Thai et al. used image segmentation and contour tracking methods for the counting of shrimp larvae [21]. This method is able to count individual shrimp larvae, although it struggles to accurately distinguish shrimp larvae which are stuck together. For populations with fewer shrimp larvae, counting was performed using algorithms, such as segmentation paired with the Canny edge detection method and blob processing approaches [22]. Although this technique offers an enhance- ment over previous algorithms, it still does not address the issue of shrimp and larvae adhering to each other. Traditional image processing-based counting techniques require prior knowledge to manually adjust image features, leading to poor accuracy in scenarios with complex backgrounds. Furthermore, the lack of generalization in these models hinders the accurate detection and counting of shrimp larvae in different scenarios. Subsequent\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "counting techniques require prior knowledge to manually adjust image features, leading to poor accuracy in scenarios with complex backgrounds. Furthermore, the lack of generalization in these models hinders the accurate detection and counting of shrimp larvae in different scenarios. Subsequently, deep learning-based [23–26] counting methods use trained object recog- nition models to identify targets within images. Armalivia et al. used the You Only Look Once version 3 (YOLOv3) algorithm to count shrimp larvae [ 27], achieving an average counting accuracy of 96% in low-density populations of around 100 larvae (in a circular area with a diameter of 40 cm), but a larger number of larvae could not be identified. Hu et al. used deep learning and a density map of shrimp larvae to estimate the populations of approximately 1000 larvae (in a 24 cm  33 cm area) [ 13]. However, this approach does not accurately identify each individual larva. Hence, devising an automated and accurate method for counting shrimp larvae in densely populated areas remains a difficult challenge. Object detection algorithms based on deep learning mainly include Regions with Convolutional Neural Networks (R-CNN) and multiple versions of YOLO, among which YOLOv5 is a mature algorithm among target recognition algorithms which has the advantages of being lightweight and having a fast inference ability. However, in the detection and recognition of small targets, YOLOv5 is not effective, and thus we need to improve YOLOv5. In this study, we investigate the enhancement of deep learning techniques to address the challenge of counting shrimp larvae in highly populated environments. In particular, we propose an algorithm which uses regional segmentation along with an improved YOLOv5 model to accurately count shrimp larvae, especially in highly dense environments. The remainder of this paper is structured as follows. In Section 2, we give the related work to highlight our motivation. In Section 3, we describe the details of the proposed shrimp larvae counting method. Section 4 presents the experimental results and further validates the superiority of the algorithm through controlled experiments. Section 5 gives our conclusions. The main contributions are described as follows: • W e build an automatic shrimp collecting platform to obtain the corresponding dataset and provide a counting\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "results and further validates the superiority of the algorithm through controlled experiments. Section 5 gives our conclusions. The main contributions are described as follows: • W e build an automatic shrimp collecting platform to obtain the corresponding dataset and provide a counting algorithm based on improved YOLOv5 with region segmentation. • The C2f and attention mechanism are embedded in the YOLOv5 model, improving the detection ability for small shrimp. • The segmentation of regions aims to boost the proportion of pixels representing shrimp larvae in a visual image. • Experimental results prove that the proposed algorithm can identify shrimp larvae with high accuracy under the difficult circumstance of overlap between large numbers of shrimp and different light intensities.Sensors 2024, 24, 6328 3 of 19 2. Related Work There are many challenges in the counting of shrimp larvae. First, shrimp larvae are small in size, making them relatively difficult to detect. In addition, shrimp larvae move rapidly, and their pixel values in images differ depending on their depth in the water. Conventional threshold segmentation techniques may easily overlook shrimp larvae which are at the bottom of the water. As shown in the Figure 1, there are three states, including isolated, clumped, and overlapping shrimp larvae. Isolated shrimp larvae remain unaffected by the presence of other larvae and can be detected through conventional image processing approaches or deep learning methods. Clumped shrimp larvae consist of several larvae attached to each other, resulting in an underestimated count when using connected component analysis. This problem can be resolved through the application of watershed algorithms and distance transformation. However, these techniques necessitate precise threshold configurations and do not generalize well. Overlapping shrimp larvae appear when larvae at varying water depths align at the same spot in the image. The aforementioned watershed algorithm is ineffective in addressing this scenario. Utilizing concave point algorithms for separation might erroneously fragment the shrimp tails, causing overcounting of the shrimp. Consequently, conventional image processing techniques prove inadequate for identifying these instances, necessitating the use of deep learning approaches to accurately discern and identify the various states of shrimp larvae. Figure 1. The states of shrimp larvae. ( a) Isolated shrimp larvae. ( b) C\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "necessitating the use of deep learning approaches to accurately discern and identify the various states of shrimp larvae. Figure 1. The states of shrimp larvae. ( a) Isolated shrimp larvae. ( b) Clumped shrimp larvae. (c) Overlapping shrimp larvae. Minimizing the dimensions of the input image can enhance the accuracy of receptive field detection [28]. Two widely used techniques for decreasing the size of an image are image scaling and cropping. Image scaling often employs downsampling [29], which may lead to data loss. Another approach is image segmentation, which can be performed either randomly or in a consistent manner. Random segmentation employs random functions to determine the coordinates for the top-left and bottom-left sections of the local image of shrimp larvae [30]. Then, the coordinates for the top-right and bottom-right sections are determined according to the dimensions of the local image. In addition, the cropping function is applied to extract the local image. Beginning in the upper-left corner of the original image, fixed segmentation continues with sequential segmentation based on the dimensions of the local image. For regions along the edges which are smaller than the segmented dimensions, the true size of the image is maintained. Random segmentation might result in repeated recognition of the same regions, thereby increasing the training burden on the model. Consequently, this research uses fixed segmentation to handle the initial images. The estimation of density maps is also a commonly used counting method [23] which estimates the number of objects by generating a density map. This is often applied in crowd counting [31]. However, this method cannot accurately provide the geometric and positional information of objects. When the detection of small objects does not require positional information, the method can provide good counting results. In shrimp larvae counting, when the larvae density is relatively low, positional and shape information may not be critical, and both density map estimation and object detection can achieve accurateSensors 2024, 24, 6328 4 of 19 counts. However, as the larvae density increases, with overlapping larvae at different water depths, positional and shape information become important. In high-density areas, density map estimation is prone to misjudgments, leading to a decrease in count accuracy. By\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      ", with overlapping larvae at different water depths, positional and shape information become important. In high-density areas, density map estimation is prone to misjudgments, leading to a decrease in count accuracy. By incorporating position and shape information, object detection methods can achieve better counting accuracy. In terms of runtime, density map estimation models are more complex and require a longer processing time, while object detection methods are faster, making them more suitable for quick shrimp larvae counting. YOLO [32] is considered one of the best choices for object detection models due to its speed and superior accuracy compared with R-CNN [33]. YOLO pioneered an innovative approach by utilizing convolutional neural networks on a complete image to predict object classes and bounding boxes via direct feature regression. Over the years, YOLO has evolved into several versions. YOLOv1 uses a unified detection approach for object localization and classification tasks [ 32]. Subsequently, YOLOv2 improves on version 1 by having better accuracy, a faster speed, and the ability to recognize more objects [ 34]. YOLOv3 improves the object detection speed by implementing multiscale prediction [35], optimizing the core network, and refining the loss function. YOLOv4 introduces a fast and effective object detection model which substantially cuts down computational expenses, making it more compatible with general-purpose devices and those with hardware constraints. YOLOv5 brings significant advancements, including the CSPDarknet backbone and mosaic augmentation, balancing speed and accuracy [ 36–42]. YOLOv7 improves the structure of the extended efficient layer aggregation network, revises the model architecture, and introduces an efficient label assignment strategy, which increases detection performance while decreasing the number of model parameters [43]. Currently, researchers are constantly exploring the architectural design of YOLO, with the latest version being YOLOv10 [44]. It has achieved state-of-the-art performance and efficiency on various model scales. In the experimental section, we describe the performance of YOLOv10 and conduct shrimp larvae counting tests using this model. However, the YOLOv10 model is complex and has\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "performance and efficiency on various model scales. In the experimental section, we describe the performance of YOLOv10 and conduct shrimp larvae counting tests using this model. However, the YOLOv10 model is complex and has high hardware resource requirements, making its hardware deployment challenging. One of the primary objectives of this study is to deploy the algorithm on hardware to achieve an integrated system for shrimp larvae image acquisition and counting. YOLOv10 does not meet the requirements of our design. YOLOv5 stands out from its predecessors with its lighter architecture, faster inference times, and more developed ecosystem which offers enhanced compatibility [45]. Moreover, YOLOv5 has low hardware resource requirements, is relatively mature in terms of hardware deployment, and is easier to implement. Therefore, we selected YOLOv5 as the baseline for our study. However, in scenarios which involve the detection of small objects, such as shrimp larvae, the use of several convolutional layers may lead to missing small targets, resulting in reduced pixel occupancy for small objects. To address this problem, we upgraded the YOLOv5 backbone network to improve its ability to identify small targets. 3. Materials and Methods For a more precise counting of shrimp larvae in situations with large numbers and a high density, we suggest an algorithm based primarily on regional segmentation and an improved YOLOv5 model. The flow chart of the algorithm can be viewed in Figure 2. To begin with, we divide the whole original image into smaller segments using a region-based segmentation algorithm. Subsequently, we employ an enhanced YOLOv5 model to recognize shrimp larvae in each of these isolated images separately. Then, all segmented image blocks are reassembled into a complete image. A repeat count removal method is proposed to address the issue of repeat counting in the stitching position, ensuring that accurate counting of shrimp larvae can be achieved throughout the image.Sensors 2024, 24, 6328 5 of 19 Figure 2. The flow chart of the proposed counting algorithm. 3.1. Image Collection The quality of the original images, which is one of the key factors for computer vision, will affect the accuracy of the counting of shrimp larvae. For the best accuracy in counting the larvae, we designed and\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "algorithm. 3.1. Image Collection The quality of the original images, which is one of the key factors for computer vision, will affect the accuracy of the counting of shrimp larvae. For the best accuracy in counting the larvae, we designed and fabricated a shrimp larvae image acquisition device as shown in Figure 3. Shrimp larvae are placed in a semi-transparent plastic square bucket, and white LEDs with light guide plates are added around the side wall and the bottom to provide uniform illumination. Shrimp larvae images are captured using an industrial camera (MV-CE060- 10UM (Hikvision, Hangzhou, China)) with an 8 mm lens under supplementary white light. The resolution of the camera is 3072 2048 pixels for a field of view of about40 cm  30 cm, and the effective area we used for the shrimp larvae was 30 cm  30 cm. Figure 3. Shrimp larvae acquisition device. ( a) Schematic diagram. ( b) Photographs of the actual device. The shrimp larvae dataset used in this experiment was collected from on-site photog- raphy at the Hongkai Shrimp Larvae Farm in Zhuhai City, Guangdong Province, China. Figure 4a shows the real shrimp larvae farming environment. The staff used a fishing net to extract shrimp larvae from the pool (as shown in Figure 4b) and then placed the collected larvae in our custom-built shrimp larvae image acquisition device as shown in Figure 4c.Sensors 2024, 24, 6328 6 of 19 Figure 4. Acquisition of shrimp larvae datasets in a real aquaculture scenario. ( a) Shrimp culture tank. (b) Shrimp larvae collection. (c) Acquisition device. 3.2. Region Segmentation The receptive field refers to the size of the region in the input image which corresponds to each pixel in the output feature map [ 46]; that is, a single point on the feature map corresponds to a specific region on the input image. The size of the area of the receptive field directly affects the detection accuracy [47,48]. The formula for calculating the receptive field is as follows: Ri = (Ri+1  1)  Sti\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "size of the area of the receptive field directly affects the detection accuracy [47,48]. The formula for calculating the receptive field is as follows: Ri = (Ri+1  1)  Sti + Ksi (1) where Ri represents the receptive field on the ith convolutional layer, Ri+1 is the receptive field on the i + 1th layer, Sti is the stride of the convolution, and KS is the size of the convolutional kernel for the current layer. When the original image of shrimp larvae ( 3072  2048 pixels) is used as the input image, the corresponding receptive field size is38 38 pixels. This indicates that each point on the feature map corresponds to a area of 38  38 pixels on the original image, which is significantly larger than the size of the shrimp larvae (approximately 10  10 pixels). As a result, information on shrimp larvae would be overlooked or covered, greatly reducing the detection accuracy. To address this issue, it is necessary to reduce the size of the receptive field to improve the shrimp larvae detection performance. Therefore, we adopted the region segmentation method to divide the original image, reducing the size of the input image and thereby decreasing the receptive field area. For example, the original image of shrimp larvae (as shown in Figure 5a) was divided into multiple image blocks (100  100 pixels for each, as shown in Figure 5b) for subsequent labeling and training. In the whole image, the proportion of shrimp larvae in terms of pixels in the whole image is extremely small. Therefore, we used segmentation technology to obtain multiple local regions of shrimp larvae, making feature extraction easier. Figure 6 shows the feature maps selected from the 17th layer of the YOLOv5 model for both the full image and the local image. To facilitate comparison, we extracted the same region from the full image as that in the local image. From Figure 6, it is evident that the full image did not accurately identify local regions, while the local image allowed a more precise extraction of features from the shrimp larvae.Sensors 2024, 24, 6328 7 of 19 Figure 5. Image segmentation. (a) Original shrimp larva\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "not accurately identify local regions, while the local image allowed a more precise extraction of features from the shrimp larvae.Sensors 2024, 24, 6328 7 of 19 Figure 5. Image segmentation. (a) Original shrimp larvae image. (b) Segmented image. Figure 6. The feature map referring to the shrimp larvae image. 3.3. The Improved YOLOv5 The core idea of YOLOv5 is to convert the target detection problem into a regression problem, predicting the boundary boundaries and categories of the target in the image. It has high accuracy and a fast inference ability, making it one of the best-performing target detection models available today. However, the tiny size of the shrimp larvae, along with various body positions such as their overlap, leads to a decrease in the identification accuracy of the YOLOv5 model. Therefore, the recognition accuracy can be improved by increasing the classification performance of features. As shown in Figure 7, the C2f module (Figure 8) was used to replace the C3 module. In the backbone network of the YOLOv5 algorithm, the main function of the C3 module is to extract image features and enhance the learning capacity of the convolutional network. The C3 module cannot meet the requirements for the detection of small targets such as shrimp larvae and needs further improvement to enhance the feature extraction capabilities of the model. The C2f module processes the input data using two convolutional layers, which assist in extracting features at varying levels and degrees of abstraction. This improves the feature extraction efficiency and simultaneously reduces the network weight, facilitating more abundant gradient flow information [ 49]. In contrast to the C3 module, the C2f module is more lightweight, has reduced computational demands, and demonstrates robust feature extraction capabilities.Sensors 2024, 24, 6328 8 of 19 Figure 7. The structure of the improved YOLOv5 model. Figure 8. Structure of the C2f network. To further enhance the recognition capability of the model, the convolutional block attention module (CBAM) [ 50] is introduced into the YOLOv5 framework. The CBAM consists of the channel attention and spatial attention modules, as shown in Figure 9. For an input F, the global average and maximum pooling operations are first applied to\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "] is introduced into the YOLOv5 framework. The CBAM consists of the channel attention and spatial attention modules, as shown in Figure 9. For an input F, the global average and maximum pooling operations are first applied to obtain global information for each channel. Subsequently, two fully connected layers are utilized to produce the channel attention vector. These layers apply weights to the input feature F per channel, which yields the refined feature F1. In the spatial attention module, the feature map is combined to capture spatial information. Furthermore, the feature F1 undergoes a convolution layer 3  3, resulting in the creation of a spatial attention map via the sigmoid function. The spatial attention map is subsequently used to enhance the feature F1, leading to production of the fused feature F2. The attention module improves the depiction of shrimp larvae features across different conditions. Figure 10 displays the feature map of both the enhanced YOLOv5 network and the original YOLOv5 network. Clearly, the proposed approach provides more precise counting outcomes even in instances of densely packed shrimp larvae.Sensors 2024, 24, 6328 9 of 19 Figure 9. Structure illustration of CBAM. Figure 10. Feature map comparison. ( a) The original YOLOv5 network. ( b) The improved YOLOv5 network. 3.4. Repeat Count Removal via Stitching The method described in Section 3.2 allows for segmentation of the entire image. The method described in Section 3.3 can identify shrimp larvae in each segmented image block. However, identical shrimp larvae can be segmented into neighboring image blocks and detected at the same time (in Figure 11), resulting in reduced counting accuracy. To address the issue of duplicate counts, an additional detection model was developed. Figure 11. Segmentation of same shrimp larvae in adjacent image blocks (dashed lines indicate the stitching positions). (a) Vertical segmentation. (b) Horizontal segmentation. (c) Both horizontal and vertical segmentation. Figure 11a shows a shrimp larva divided vertically into two sections and identified in two separate subimages. Figure 11b presents two shrimp larvae segmented horizon- tally into two subimages and identified. Figure 11c shows shrimp larva\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "a shrimp larva divided vertically into two sections and identified in two separate subimages. Figure 11b presents two shrimp larvae segmented horizon- tally into two subimages and identified. Figure 11c shows shrimp larvae cut horizontally and vertically, resulting in four distinct subimages. Figure 12 shows a schematic illustra- tion of the segmentation of shrimp larvae, helping in the evaluation of the segmentation configurations. In Figure 12, the red boxes represent the detection boundary boxes of the detection output, and the black lines represent the cropping boundaries of the image. Taking intoSensors 2024, 24, 6328 10 of 19 account the spatial relationship between the detection boundary boxes and the image cropping boundaries in the output, nine distinct scenarios can be identified: undivided shrimp larvae (N), top edge touching (U), bottom edge touching (D), right edge touching (R), left edge touching (L), right and bottom edges touching (RD), right and top edges touching (RU), left and bottom edges touching(LD), and left and top edges touching(LU). A set S, where S = N, D, U, R, L, RD, RU, LD, LU, was defined to store all detection output. By traversing the detection outputs and checking the boundaries, the detection results were placed in the corresponding sets. Figure 12. Schematic diagram of shrimp larvae segmentation. (a) Vertical segmentation. (b) Horizon- tal segmentation. (c) Both horizontal and vertical segmentation. To avoid duplication, we examined the shrimp larva detection boxes in neighboring regions to determine whether the detected shrimp larvae were identical. If several detection boxes corresponded to each other, as illustrated in Figure 13, then(xi, yi) are the coordinates of the vertices of the detection frame, and the detection box at the bottom which intersects the cropping boundary coincides with the two detection boxes at the top, which also intersect with the cropping boundary. Therefore, an overlap ratio must be implemented to facilitate the selection process. We define a variable Ro to represent the percentage of overlap between detection frames, whose formula is as follows: Ro = x4  x1 x2  x3 (2) where \n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "to facilitate the selection process. We define a variable Ro to represent the percentage of overlap between detection frames, whose formula is as follows: Ro = x4  x1 x2  x3 (2) where x1, x2, x3, x4 are the horizontal coordinates of the detection frame. Figure 13. Multiple detection box matching. By calculating the overlap ratio, the detection box with the highest overlap ratio is selected. After determining the matching object, a minimum bounding rectangle is used to completely enclose the two detection boxes, thus identifying the deduplication area (indicated in Figure 14a, for example). The two red detection boxes in Figure 14bSensors 2024, 24, 6328 11 of 19 correspond to a typical repeat count case (according to Figure 11b). We determined whether they were connected by examining the coordinates of the two detection boxes. Once the connected boxes were identified, we used a minimum enclosing box (indicated by the green rectangle in Figure 14a,c) to completely cover the two red detection boxes and define the deduplication area. Figure 14. The deduplication area. (a) Schematic diagram of how to find the deduplication area (the dashed line indicates region segmentation positions), where the two red rectangles are the detection boxes and the corresponding minimum enclosing rectangle is drawn in green. (b) Original detection result with two detection boxes detecting the same larva. (c) Minimum enclosing rectangle for the case in (b). Once the deduplication area is identified, it is fed back into the enhanced YOLOv5 network mentioned in Section 3.3 to accurately count the shrimp larvae in this region. This process ensures that duplicate counts are removed from the entire composite image of shrimp larvae. 4. Experimental Results and Analysis 4.1. Implementation Details In the experiment, we collected 20,000 valid local images of shrimp larvae, with 15,000 allocated for training and 5000 for testing. For annotating the dataset, the conven- tional labeling approach (Figure 15a) involves describing all the shrimp larvae, yet this can introduce redundant details and impede the model’s ability to learn the features of shrimp larvae. We marked the heads of the shrimp larvae\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "ional labeling approach (Figure 15a) involves describing all the shrimp larvae, yet this can introduce redundant details and impede the model’s ability to learn the features of shrimp larvae. We marked the heads of the shrimp larvae as shown in Figure 15b. This annotation method concentrates on the feature of the shrimp larvae, making it easier to distinguish overlapping larvae. The comparison algorithms included the YOLOv5 algo- rithm (which means using YOLOv5 without regional segmentation), the Mask R-CNN algorithm [17], and popular commercial software (Smart Shrimp Farm V2.1.3 [ 51]). In addition, the proposed algorithm was performed on an NVIDIA RTX 3080Ti (NVIDIA, Santa Clara, CA, USA). A total of 500 epochs were trained, with the learning rate set to 0.001. The other parameters were the default YOLOv5 parameters. The optimizer was the stochastic gradient descent. Figure 15. Dataset annotation. (a) The traditional labeling method. (b) The annotation methodwe used.Sensors 2024, 24, 6328 12 of 19 We used images of shrimp larvae of different densities to evaluate the performance of the proposed method. Under the fixed field of view provided (30 cm  30 cm), we examined four different densities of shrimp larvae: approximately 1000 larvae for density level 1, approximately 2300 larvae for density level 2, approximately 4000 larvae for density level 3, and roughly 5000 larvae for density level 4. We chose 100  100 pixels for the segmentation image size in the counting experiments hereafter. The dataset and codes can be requested by email (Hongchao Duan: 2021024069@m.scnu.edu.cn). 4.2. Evaluation Criterion To quantitatively evaluate the performance of the proposed algorithm, we define A as the measurement for the counting accuracy of the formula as follows: A = Ac  |Id  Ac| Ac  100% (3) where Ac represents the actual number of shrimp larvae and Id denotes the number of shrimp larvae. For accurate results, 15 images of shrimp larvae were analyzed at each density level, with the mean count accuracy serving as the statistical result for each level.\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "number of shrimp larvae and Id denotes the number of shrimp larvae. For accurate results, 15 images of shrimp larvae were analyzed at each density level, with the mean count accuracy serving as the statistical result for each level. 4.3. Experiment Comparison To verify the effectiveness of the above proposed method, we collected a large number of shrimp images of different densities (four densities mentioned in Section 4.1) for the validation experiment. Figure 16a–d shows the recognition effect of the proposed method under different densities. It can be seen in Figure 16 that the proposed algorithm had a good recognition effect for different densities, especially in places where shrimp larvae were densely populated. Figure 16. Counting results at different densities. ( a) Density 1. ( b) Density 2. ( c) Density 3. (d) Density 4.Sensors 2024, 24, 6328 13 of 19 Figure 17a shows a typical local image of the original high-density shrimp larvae (approximately 5000 shrimp larvae in a 30 cm  30 cm area), and Figure 17b shows the corresponding identification results. It shows that the proposed method exhibited high recognition accuracy even when the density of the shrimp larvae was high and there were overlaps between larvae. This indicates that the regional segmentation approach adopted in this study makes the receptive field more accurate, which makes the improved YOLOv5 model more accurate in the recognition of shrimp larvae targets. Figure 17. A typical local area of Figure 16. (a) Original image. (b) Detection results. Among the various comparison algorithms, Smart Shrimp Farm only provided the final count results without the detection images. We tested high-density shrimp larvae images (approximately 5000 shrimp larvae in a 30 cm  30 cm area) using the remaining algorithms, and the comparison of the test results is shown in Figure 18. For Density Map Regression, we provide the corresponding density map in Figure 18f. Figure 18. Test result images for different algorithms. ( a) The proposed method. ( b) YOLOv5. (c) Mask R-CNN. (d) YOLOv10. (e\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "18f. Figure 18. Test result images for different algorithms. ( a) The proposed method. ( b) YOLOv5. (c) Mask R-CNN. (d) YOLOv10. (e) Shrimpseed_Net. (f) Density map regression. Table 1 presents the statistical results of the average counting accuracy A for the comparison algorithm.Sensors 2024, 24, 6328 14 of 19 Table 1. Average counting accuracy A for different shrimp larvae densities. Method Density 1 Density 2 Density 3 Density 4 The Proposed Method 99.32 % 99.17% 98.53% 98.23% Smart Shrimp Farm [51] 91.43% 90.35% 88.25% 82.99% YOLOv5 [52] 87.79% 85.46% 83.66% 81.03% Mask R-CNN [17] 86.63% 84.52% 81.25% 80.32% YOLOv10 [44] 91.41% 90.68% 89.29% 88.75% Shrimpseed_Net [53] 91.65% 90.23% 89.47% 87.62% Density Map Regression [23] 92.33% 91.58% 90.75% 88.61% The results clearly indicate that, compared with commercial software such as YOLOv5, YOLOv10, Shrimpseed_Net, Density Map Regression, and Mask R-CNN, the proposed algorithm excelled in counting shrimp larvae at various densities, particularly in high- density scenarios. Although the accuracy decreased somewhat as the density increased, the overall recognition accuracy of the proposed method remained above 98%, significantly outperforming the other three cases (just above 80%). Moreover, the accuracy, which exceeded 98%, demonstrates that the proposed algorithm could potentially substitute the manual shrimp counting process on farms. 4.4. Ablation Study To validate the superiority of the improved YOLOv5 model proposed in this article, we compared the identification results in the segmentation images of100  100 pixels using both the original YOLOv5 model and the improved YOLOv5 model in Section 3.3 (typical results are shown in Figure\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      ", we compared the identification results in the segmentation images of100  100 pixels using both the original YOLOv5 model and the improved YOLOv5 model in Section 3.3 (typical results are shown in Figure 19). Figure 19. Recognition performance of different algorithm models. ( a) Original YOLOv5 model. (b) Improved YOLOv5 model. As shown in Figure 19, the improved YOLOv5 model demonstrated better recognition performance when the shrimp larvae were densely packed. To quantitatively analyze the differences between the two models, we used the counting algorithm on all images, and the results of the average counting accuracy for the four density levels are summarized in Table 2. For density 4, we collected images of shrimp larvae under three different lighting conditions (bright, normal brightness, and dim) for testing. The test results are shown in Figure 20, and the data are summarized in Table 2. The statistical results in Table 2 show that the improved YOLOv5 model achieved an approximately 2% higher counting accuracy compared with the original YOLOv5 model, validating the effectiveness of the proposed method. Under different lighting conditions, the counting accuracy of our proposed algorithm exceeded 98%, outperforming the original YOLOv5 algorithm.Sensors 2024, 24, 6328 15 of 19 Figure 20. Test results under three different lighting conditions. (a) Bright. (b) Normal brightness. (c) Dim. Table 2. Average counting accuracy A for the two YOLOv5 models. Density The Original YOLOv5 The Improved YOLOv5 Density 1 98.13% 99.32% Density 2 97.85% 99.17% Density 3 96.81% 98.53% Density 4 (Bright) 96.26% 98.23% Density 4 (Normal Brightness) 94.66% 98.21% Density 4 (Dim) 93.52% 98.17% The original image of the larvae was divided into many local areas to decrease the size of the receptive field in Section 3.2. To evaluate the impact of segmentation image sizes on the accuracy of shrimp larvae detection, we cropped the original shrimp larvae images into five different sizes: 600  600\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "the size of the receptive field in Section 3.2. To evaluate the impact of segmentation image sizes on the accuracy of shrimp larvae detection, we cropped the original shrimp larvae images into five different sizes: 600  600 pixels, 300  300 pixels, 200  200 pixels, 100  100 pixels, and 50  50 pixels (as shown in Figure 21). Subsequently, we used the improved YOLOv5 model to train and test these cropped images of different sizes separately. Figure 21. Recognition results for different segmentation image sizes: ( a) 600  600 pixels, (b) 300  3000 pixels, (c) 200  200 pixels, (d) 100  100 pixels, and (e) 50  50 pixels. As shown in Figure 21, when the segmentation image size was 600  600 pixels, many shrimp larvae were not identified. As the segmentation size decreased, the number of undetected shrimp larvae also decreased. When the segmentation image size was reducedSensors 2024, 24, 6328 16 of 19 to 100  100 pixels and 50  50 pixels, all shrimp larvae in the local images were detected. To quantitatively analyze the differences between the various segmentation image sizes, we tested the counting accuracy using images with four different densities of shrimp larvae mentioned in Section 4.1, and the statistical results of the average counting accuracy A for each level of density of shrimp larvae are presented in Table 3. Table 3. Average counting accuracy A for different segmentation sizes. Density 600  600 Pixels 300  300 Pixels 200  200 Pixels 100  100 Pixels 50  50 Pixels Density 1 91.69% 94.27% 97.79% 99.32% 99.34% Density 2 90.83% 93.85% 97.24% 99.17% 99.20% Density 3 88.72% 93.31% 96.38% 98.53% 98.87% Density 4 88.15% 92.56% 95.93% 98.23% 98.21% The results indicate that the size of the image segmentation had a significant impact on the accuracy of the counting of shrimp larvae. As the segmentation image size decreased, the accuracy\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      ".93% 98.23% 98.21% The results indicate that the size of the image segmentation had a significant impact on the accuracy of the counting of shrimp larvae. As the segmentation image size decreased, the accuracy of the shrimp larvae count clearly increased. When the segmentation image size was reduced to 50  50 pixels, the counting accuracy was the best, but we also realize that the cost of computing increased exponentially. Considering the calculation cost and the counting accuracy, we chose 100  100 pixels for the segmentation image size in the counting experiments hereafter. 4.5. Discussion Comparative experiments showed that the proposed region segmentation algorithm improved the accuracy of counting shrimp larvae. In contrast to the standard YOLOv5 model, the enhanced YOLOv5 variant provides more accurate detection of small objects such as shrimp larvae. Validation experiments indicated that the proposed algorithm performed better, with a counting accuracy of 98% even under conditions with high densities and large volumes of shrimp larvae. However, there are still aspects which need to be improved. To begin with, the dataset for this experiment was gathered using an optical platform which we designed. To improve the generalizability of the algorithm, it is essential to include images of shrimp larvae obtained from various environments and a range of imaging devices in the training dataset. In addition, the stitching and deduplication process occasionally overlooks a few shrimp larvae due to stitching errors, suggesting that this algorithm needs further improvement. In conclusion, the proposed algorithm can be integrated into shrimp larvae imaging devices to develop a comprehensive shrimp larvae counting system. The image segmentation approach and the improved YOLOv5 model proposed in this paper not only solve the problem of shrimp larvae counting but also have significant research potential in various fields, such as dense crowd counting and medical image analysis. Using segmentation algorithms, the proportion of the pixel of the target objects in the input images increased, thereby improving the detection accuracy. However, the algorithm needs to be adjusted according to the specific problem at hand. 5. Conclusions We introduced a method for counting shrimp larvae which uses region segmentation along with an improved YOLOv5 model. By segmenting the regions, we can reduce the receptive field area, thereby enhancing the detection accuracy of shrimp larvae using\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "counting shrimp larvae which uses region segmentation along with an improved YOLOv5 model. By segmenting the regions, we can reduce the receptive field area, thereby enhancing the detection accuracy of shrimp larvae using the improved YOLOv5 model. Furthermore, the deduplication model can tackle the problem of repeated counts. Based on this, the experimental results show that the number of larvae was about 5000, and the counting accuracy of our algorithm remained above 98%, which would be suitable for replacing the work of manually counting shrimp larvae.Sensors 2024, 24, 6328 17 of 19 Author Contributions: Conceptualization, Y.Z.; methodology, Y.Z., J.W. and H.D.; software, J.W. and H.D.; validation, X.W., T.P ., X.L. and D.D.; formal analysis, J.W. and H.D.; investigation, J.W., X.W. and H.D.; resources, T.P .; data curation, D.D.; writing—original draft preparation, H.D.; writing— review and editing, Y.Z.; visualization, Y.Z.; supervision, Y.Z.; project administration, Y.Z.; funding acquisition, Y.Z. All authors have read and agreed to the published version of the manuscript. Funding: This research received no external funding. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable. Data Availability Statement: Data are available from the authors on request. Conflicts of Interest: The authors declare no conflicts of interest. References 1. Racotta, I.S.; Palacios, E.; Ibarra, A.M. Shrimp larval quality in relation to broodstock condition. Aquaculture 2003, 227, 107–130. [CrossRef] 2. Li, D.; Miao, Z.; Peng, F.; Wang, L.; Hao, Y.; Wang, Z.; Chen, T.; Li, H.; Zheng, Y. Automatic counting methods in aquaculture: A review.\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "Peng, F.; Wang, L.; Hao, Y.; Wang, Z.; Chen, T.; Li, H.; Zheng, Y. Automatic counting methods in aquaculture: A review. J. World Aquac. Soc.2021, 52, 269–283. [CrossRef] 3. Tierney, T.W.; Fleckenstein, L.J.; Ray, A.J. The effects of density and artificial substrate on intensive shrimp Litopenaeus vannamei nursery production. Aquac. Eng.2020, 89, 102063. [CrossRef] 4. Naegel, L.C.; Gómez-Humarán, I.M. Effect of sample volume and population density on precision of larval population estimates. Aquac. Eng.1998, 17, 11–19. [CrossRef] 5. Hsieh, Y.K.; Hsieh, J.W.; Hu, W.C.; Tseng, Y.C. AIoT-Based Shrimp Larvae Counting System Using Scaled Multilayer Feature Fusion Network. IEEE Internet Things J.2024, early Access. [CrossRef] 6. Zhang, L.; Li, W.; Liu, C.; Zhou, X.; Duan, Q. Automatic fish counting method using image density grading and local regression. Comput. Electron. Agric.2020, 179, 105844. [CrossRef] 7. Yeh, C.T.; Ling, M.S. Portable Device for Ornamental Shrimp Counting Using Unsupervised Machine Learning. Sens. Mater. 2021, 33, 3027–3036. [CrossRef] 8. Martinez-Palacios, C.; Novoa, M.O.; Chavez-Martinez, C. A simple apparatus for self-separation of post-larval prawns, Macrobrachium spp., in mixed larval rearing tanks. Aquac. Res.1985, 16, 341–348. [CrossRef] 9. Work, H.P \n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "prawns, Macrobrachium spp., in mixed larval rearing tanks. Aquac. Res.1985, 16, 341–348. [CrossRef] 9. Work, H.P .S. Photoelectric Sensors for Counting and Classifying Vehicles. InTransportation Research Record 1311; Transportation Research Board: Washington, DC, USA, 1991; p. 79. 10. Carmichael, H. Photoelectric Detection II. In An Open Systems Approach to Quantum Optics: Lectures Presented at the Université Libre de Bruxelles October 28 to November 4, 1991; Springer: Berlin/Heidelberg, Germany, 1993; pp. 93–112. 11. Spratt, M.D. Preliminary results of a computer imaging system applied to estimating the quantity of larvae and fingerling fish for aquaculture. In Fish Quality Control by Computer Vision; Routledge: New York, NY, USA, 2017; pp. 263–282. 12. Awalludin, E.; Muhammad, W.W.; Arsad, T.; Yussof, W.H.W. Fish larvae counting system using image processing techniques. In Proceedings of the Journal of Physics Conference Series; IOP Publishing: Bandung, Indonesia, 2020; Volume 1529, p. 052040. 13. Hu, W.C.; Chen, L.B.; Hsieh, M.H.; Ting, Y.K. A deep-learning-based fast counting methodology using density estimation for counting shrimp larvae. IEEE Sens. J.2022, 23, 527–535. [CrossRef] 14. Sun, Y.; Lin, Y.; Zhao, G.; Svanberg, S. Identification of flying insects in the spatial, spectral, and time domains with focus on mosquito imaging. Sensors 2021, 21, 3329. [CrossRef] 15. Patwal, A.; Diwakar, M.; Tripathi, V .; Singh, P . Crowd counting analysis using deep learning: A critical review.Procedia Comput. Sci. 2023, 218, 2448–2458. [\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "ar, M.; Tripathi, V .; Singh, P . Crowd counting analysis using deep learning: A critical review.Procedia Comput. Sci. 2023, 218, 2448–2458. [CrossRef] 16. Kaewchote, J.; Janyong, S.; Limprasert, W. Image recognition method using Local Binary Pattern and the Random forest classifier to count post larvae shrimp. Agric. Nat. Resour.2018, 52, 371–376. [CrossRef] 17. Nguyen, K.T.; Nguyen, C.N.; Wang, C.Y.; Wang, J.C. Two-phase instance segmentation for whiteleg shrimp larvae counting. In Proceedings of the 2020 IEEE International Conference on Consumer Electronics (ICCE), Las Vegas, NV , USA, 4–6 January 2020; pp. 1–3. 18. Qu, Y.; Jiang, S.; Li, D.; Zhong, P .; Shen, Z. SLCOBNet: Shrimp larvae counting network with overlapping splitting and Bayesian-DM-count loss. Biosyst. Eng.2024, 244, 200–210. [CrossRef] 19. Zhang, J.; Yang, G.; Sun, L.; Zhou, C.; Zhou, X.; Li, Q.; Bi, M.; Guo, J. Shrimp egg counting with fully convolutional regression network and generative adversarial network. Aquac. Eng.2021, 94, 102175. [CrossRef] 20. Kesvarakul, R.; Chianrabutra, C.; Chianrabutra, S. Baby shrimp counting via automated image processing. In Proceedings of the 9th International Conference on Machine Learning and Computing, Singapore, 24–26 February 2017; pp. 352–356. 21. Thai, T.T.N.; Nguyen, T.S.; Pham, V .C. Computer vision based estimation of shrimp population density and size. In Proceedings of the 2021 International Symposium on Electrical and Electronics Engineering (ISEE\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "N.; Nguyen, T.S.; Pham, V .C. Computer vision based estimation of shrimp population density and size. In Proceedings of the 2021 International Symposium on Electrical and Electronics Engineering (ISEE), Ho Chi Minh, Vietnam, 15–16 April 2021; pp. 145–148.Sensors 2024, 24, 6328 18 of 19 22. Awalludin, E.A.; Yaziz, M.M.; Rahman, N.A.; Yussof, W.N.J.H.W.; Hitam, M.S.; Arsad, T.T. Combination of canny edge detection and blob processing techniques for shrimp larvae counting. In Proceedings of the 2019 IEEE International Conference on Signal and Image Processing Applications (ICSIPA), Kuala Lumpur, Malaysia, 17–19 September 2019; pp. 308–313. 23. Zhou, C.; Yang, G.; Sun, L.; Wang, S.; Song, W.; Guo, J. Counting, locating, and sizing of shrimp larvae based on density map regression. Aquac. Int.2024, 32, 3147–3168. [CrossRef] 24. Zhang, L.; Zhou, X.; Li, B.; Zhang, H.; Duan, Q. Automatic shrimp counting method using local images and lightweight YOLOv4. Biosyst. Eng.2022, 220, 39–54. [CrossRef] 25. Bereciartua-Pérez, A.; Gómez, L.; Picón, A.; Navarra-Mestre, R.; Klukas, C.; Eggers, T. Insect counting through deep learning- based density maps estimation. Comput. Electron. Agric.2022, 197, 106933. [CrossRef] 26. Yu, X.; Wang, J.; Wen, S.; Yang, J.; Zhang, F. A deep learning based feature extraction method on hyperspectral images for nondestructive prediction of TVB-N content\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      ", X.; Wang, J.; Wen, S.; Yang, J.; Zhang, F. A deep learning based feature extraction method on hyperspectral images for nondestructive prediction of TVB-N content in Pacific white shrimp ( Litopenaeus vannamei). Biosyst. Eng. 2019, 178, 244–255. [CrossRef] 27. Armalivia, S.; Zainuddin, Z.; Achmad, A.; Wicaksono, M.A. Automatic counting shrimp larvae based you only look once (YOLO). In Proceedings of the 2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS), Bandung, Indonesia, 28–30 April 2021; pp. 1–4. 28. Liu, S.; Huang, D. Receptive field block net for accurate and fast object detection. In Proceedings of the European conference on Computer Vision (ECCV), Munich, Germany, 8–14 September 2018; pp. 385–400. 29. Zhang, Y.; Zhao, D.; Zhang, J.; Xiong, R.; Gao, W. Interpolation-dependent image downsampling. IEEE Trans. Image Process.2011, 20, 3291–3296. [CrossRef] 30. Lan, Y.h.; Zhang, Y.; Li, C.h.; Zhao, X.f. A novel image segmentation method based on random walk. In Proceedings of the 2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA), Wuhan, China, 28–29 November 2009; Volume 1, pp. 207–210. 31. Lin, W.; Chan, A.B. Optimal transport minimization: Crowd localization on density maps for semi-supervised counting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 18–22 June 2023; pp. 21663–21673. 32. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once: Unified, real-\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "; pp. 21663–21673. 32. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV , USA, 27–30 June 2016; pp. 779–788. 33. Han, X.; Chang, J.; Wang, K. You only look once: Unified, real-time object detection. Procedia Comput. Sci.2021, 183, 61–72. [CrossRef] 34. Redmon, J.; Farhadi, A. YOLO9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 7263–7271. 35. Redmon, J. Yolov3: An incremental improvement. arXiv 2018, arXiv:1804.02767. 36. Hussain, M. YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision. arXiv 2024, arXiv:2407.02988. 37. Fu, H.; Song, G.; Wang, Y. Improved YOLOv4 marine target detection combined with CBAM. Symmetry 2021, 13, 623. [CrossRef] 38. Wang, Z.; Jin, L.; Wang, S.; Xu, H. Apple stem/calyx real-time recognition using YOLO-v5 algorithm for fruit automatic loading system. Postharvest Biol. Technol.2022, 185, 111808. [CrossRef] 39. Kim, J.H.; Kim, N.; Park, Y.W.; Won, C.S. Object detection and classification based on YOLO-V5 with improved maritime dataset. J. Mar. Sci. Eng.2022, 10, 377. [CrossRef] 40. Yang,\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      ", C.S. Object detection and classification based on YOLO-V5 with improved maritime dataset. J. Mar. Sci. Eng.2022, 10, 377. [CrossRef] 40. Yang, R.; Li, W.; Shang, X.; Zhu, D.; Man, X. KPE-YOLOv5: An improved small target detection algorithm based on YOLOv5. Electronics 2023, 12, 817. [CrossRef] 41. Wang, L.; Liu, X.; Ma, J.; Su, W.; Li, H. Real-time steel surface defect detection with improved multi-scale YOLO-v5. Processes 2023, 11, 1357. [CrossRef] 42. Sun, Q.; Li, P .; He, C.; Song, Q.; Chen, J.; Kong, X.; Luo, Z. A Lightweight and High-Precision Passion Fruit YOLO Detection Model for Deployment in Embedded Devices. Sensors 2024, 24, 4942. [CrossRef] 43. Wang, C.Y.; Bochkovskiy, A.; Liao, H.Y.M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 18–22 June 2023; pp. 7464–7475. 44. Wang, A.; Chen, H.; Liu, L.; Chen, K.; Lin, Z.; Han, J.; Ding, G. Yolov10: Real-time end-to-end object detection. arXiv 2024, arXiv:2405.14458. 45. Dong, X.; Yan, S.; Duan, C. A lightweight vehicles detection network model based on YOLOv5. Eng. Appl. Artif. Intell.2022, 113, 104914. [CrossRef] 46\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      ".; Duan, C. A lightweight vehicles detection network model based on YOLOv5. Eng. Appl. Artif. Intell.2022, 113, 104914. [CrossRef] 46. Chai, E.; Ta, L.; Ma, Z.; Zhi, M. ERF-YOLO: A YOLO algorithm compatible with fewer parameters and higher accuracy. Image Vis. Comput. 2021, 116, 104317. [CrossRef] 47. Kim, B.J.; Choi, H.; Jang, H.; Lee, D.G.; Jeong, W.; Kim, S.W. Dead pixel test using effective receptive field. Pattern Recognit. Lett. 2023, 167, 149–156. [CrossRef] 48. Wang, Q.; Qian, Y.; Hu, Y.; Wang, C.; Ye, X.; Wang, H. M2YOLOF: Based on effective receptive fields and multiple-in-single-out encoder for object detection. Expert Syst. Appl.2023, 213, 118928. [CrossRef]Sensors 2024, 24, 6328 19 of 19 49. Chen, Y.; Zhan, S.; Cao, G.; Li, J.; Wu, Z.; Chen, X. C2f-Enhanced YOLOv5 for Lightweight Concrete Surface Crack Detection. In Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications, Wuhan, China, 18–20 November 2023; pp. 60–64. 50. Wang, W.; Tan, X.; Zhang, P .; Wang, X. A CBAM based multiscale transformer fusion approach for remote sensing image change detection. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.2022, 15, 6817–6825. [CrossRef] 51. Smart Shrimp Farm. 2024. Available online: http://m.ux6.com/app/2228\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "Obs. Remote Sens.2022, 15, 6817–6825. [CrossRef] 51. Smart Shrimp Farm. 2024. Available online: http://m.ux6.com/app/222810.html (accessed on 1 March 2024). 52. Zhang, Y.; Guo, Z.; Wu, J.; Tian, Y.; Tang, H.; Guo, X. Real-time vehicle detection based on improved yolo v5. Sustainability 2022, 14, 12274. [CrossRef] 53. Liu, D.; Xu, B.; Cheng, Y.; Chen, H.; Dou, Y.; Bi, H.; Zhao, Y. Shrimpseed_Net: Counting of shrimp seed using deep learning on smartphones for aquaculture. IEEE Access2023, 11, 85441–85450. [CrossRef] Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ques = ques_gen_chain.run(document_ques_gen)\n",
    "\n",
    "results = []\n",
    "for chunk in document_ques_gen:\n",
    "    ques = ques_gen_chain.run([chunk])\n",
    "    results.append(ques)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp0bFvfEmUhe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1756904211400,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "FhizpKXYG_nk"
   },
   "outputs": [],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 1357,
     "status": "ok",
     "timestamp": 1756904826663,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "wXtueUeEG_nk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d\\generative AI\\Doc2Question\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# embeddings = OpenAIEmbeddings()\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756904875115,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "liASzaR2rWjl"
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db'\n",
    "\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=document_ques_gen,\n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1756904886808,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "ank9vjVwsGne",
    "outputId": "840fb0c7-fc41-4b26-aa39-3b3feeb686fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_16168\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17731,
     "status": "ok",
     "timestamp": 1756904920556,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "hEeFlCsMlKKS",
    "outputId": "5b8db8cc-8d9a-42a5-d43c-4fb406d936b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto')\n",
    "pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=450)\n",
    "llm_answer_gen = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1756903414828,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "oWguXPdtG_nv"
   },
   "outputs": [],
   "source": [
    "ques_list = ques.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1756904194763,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "jqY8tPIeG_nv",
    "outputId": "89db85b6-c35e-438a-f37f-077834038fed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the name of the algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5?',\n",
       " 'What is the name of the algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach?',\n",
       " 'How are photoelectric devices for counting shrimp larvae affected by various environmental factors?',\n",
       " 'What is the name of the deep learning method that Armalivia et al. used to count shrimp larvae in a circular area with a diameter of 40 cm?',\n",
       " 'What are the three states of shrimp larvae?',\n",
       " 'What are two widely used techniques for decreasing the size of an image?',\n",
       " 'YOLO pioneered an innovative approach by utilizing convolutional neural networks on a complete image to predict object classes and bounding boxes via direct feature regression. YOLO pioneered an innovative approach by utilizing convolutional neural networks on a complete image to predict object classes and bounding boxes via direct feature regression. YOLO pioneered an innovative approach by utilizing convolutional neural networks on a complete image to predict object classes and bounding boxes via direct feature regression. Over the years, YOLO has evolved into several versions. YOLOv1 uses a unified detection approach for object localization and classification tasks [ 32]. Subsequently, YOLOv2 improves on version 1 by having better accuracy, a faster speed, and the ability to recognize more objects [ 34]. YOLOv3 improves the object detection speed by implementing multiscale prediction [ 35], optimizing the core network, and refining the loss function. YOLOv4 introduces a fast and effective object detection model which substantially cuts down computational expenses, making it more compatible with general-purpose devices and those with hardware constraints. YOLOv5 brings significant advancements, including the CSPDarknet backbone and mosaic augmentation, balancing speed and accuracy [ 36–42]. YOLOv7 improves the structure of the extended efficient layer aggregation network, revises the model architecture, and introduces an efficient label assignment strategy, which increases detection performance while decreasing the number of model parameters [ 43]. ------------',\n",
       " 'What is the purpose of this paper? ------------',\n",
       " 'How are shrimp larvae images captured?',\n",
       " 'What is the size of the area of the receptive field?',\n",
       " 'What is the main function of the C3 module in the backbone network of the YOLOv5 algorithm?',\n",
       " 'What is the name of the YOLOv5 framework?',\n",
       " 'What are the four distinct subimages of a shrimp larvae?',\n",
       " 'What are the two red detection boxes in Figure 14bSensors 2024, 24, 6328 11 of 19?',\n",
       " 'How many pixels were used for the segmentation image size in the counting experiments?',\n",
       " 'How many images of shrimp larvae were analyzed at each density level?',\n",
       " 'What is the name of the method used to count shrimp larvae at various densities?',\n",
       " 'What is the difference between the original YOLOv5 model and the improved YOLOv5 model?',\n",
       " 'What is the size of the receptive field in Section 3.2?',\n",
       " 'What is the name of the YOLOv5 model? ------------',\n",
       " 'How does the deduplication model tackle the problem of repeated counts?',\n",
       " 'What is the name of the fish that can be counted using the AIoT-based Shrimp Larvae Counting System Using Scaled Multilayer Feature Fusion Network?',\n",
       " 'What is the name of the book that describes a deep-learning-based fast counting methodology using density estimation for counting shrimp larvae?',\n",
       " 'What is the name of the method used to count post larvae shrimp?',\n",
       " 'What is the name of the paper that is based on computer vision based estimation of shrimp population density and size?',\n",
       " 'What is the name of the paper that presents a deep learning based feature extraction method on hyperspectral images for nondestructive prediction of TVB-N content in Pacific white shrimp ( Litopenaeus vannamei )?',\n",
       " 'What is the name of the algorithm used for apple stem/calyx real-time recognition using YOLO-v5 algorithm for fruit automatic loading system?',\n",
       " 'Object detection and classification based on YOLO-V5 with improved maritime dataset. J. Mar. Sci. Eng.2022, 10, 377. [CrossRef] 40. Yang, R.; Li, W.; Shang, X.; Zhu, D.; Man, X. KPE-YOLOv5: An improved small target detection algorithm based on YOLOv5. Electronics 2023, 12, 817. [CrossRef] 41. Wang, L.; Liu, X.; Ma, J.; Su, W.; Li, H. Real-time steel surface defect detection with improved multi-scale YOLO-v5. Processes 2023, 11, 1357. [CrossRef] 42. Sun, Q.; Li, P .; He, C.; Song, Q.; Chen, J.; Kong, X.; Luo, Z. A Lightweight and High-Precision Passion Fruit YOLO Detection Model for Deployment in Embedded Devices. Sensors 2024, 24, 4942. [CrossRef] 43. Wang, C.Y.; Bochkovskiy, A.; Liao, H.Y.M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 18–22 June 2023; pp. 7464–7475. 44. Wang, A.; Chen, H.; Liu, L.; Chen, K.; Lin, Z.; Han, J.; Ding, G. Yolov10: Real-time end-to-end object detection. arXiv 2024, arXiv:2405.14458. 45. Dong, X.; Yan, S.; Duan',\n",
       " 'What is the name of the YOLO algorithm that is compatible with fewer parameters and higher accuracy?',\n",
       " 'What is the purpose of Shrimpseed_Net?']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_list = results\n",
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1756904954287,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "cdVIFAanG_nv"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1756904955317,
     "user": {
      "displayName": "vinh nguyen",
      "userId": "17414799588517278299"
     },
     "user_tz": -420
    },
    "id": "E8CPJjQlG_ny"
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen,\n",
    "                                               chain_type=\"stuff\",\n",
    "                                               retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0RW_O_1G_ny",
    "outputId": "d8909c20-98c8-4e3a-cd84-3fdfb6e90473"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2073 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is the name of the algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5?\n",
      "Answer:  YOLOv5\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  What is the name of the algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach?\n",
      "Answer:  an algorithm for counting densely packed shrimp larvae utilizing an enhanced You Only Look Once version 5 (YOLOv5) model through a regional segmentation approach\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  How are photoelectric devices for counting shrimp larvae affected by various environmental factors?\n",
      "Answer:  These counters can be affected by various environmental factors, such as lighting conditions and water quality, resulting in considerable counting errors. Furthermore, photoelectric devices are generally effective for shrimp larvae of certain sizes, which reduces their accuracy when dealing with larvae of different sizes.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  What is the name of the deep learning method that Armalivia et al. used to count shrimp larvae in a circular area with a diameter of 40 cm?\n",
      "Answer:  You Only Look Once version 3\n",
      "--------------------------------------------------\\n\\n\n"
     ]
    }
   ],
   "source": [
    "# Answer each question and save to a file\n",
    "for question in ques_list[:4]:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
    "        f.write(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cepdMy1_G_ny"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "068c1ac914964aa8959359f239537113": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b2d5c7ab8514404bbd085fd081ae66b6",
       "IPY_MODEL_cca0a5dd926d41eaa2c6f3f744139026",
       "IPY_MODEL_1185f5a11d7542549cf8a41c4acdd299"
      ],
      "layout": "IPY_MODEL_74a99421e3a545f3b683629789d71ed4"
     }
    },
    "072172f5a0974fc79b43977e284eec66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b5d1a20b02e4e809016c9266b2bad0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa0dd004fc8f48f6912bcc8e0457fb7e",
       "IPY_MODEL_bfe10d44f2494bdc8718cac8e327fc15",
       "IPY_MODEL_a2c5acd2a63545e995678980e9d8768e"
      ],
      "layout": "IPY_MODEL_f4cbab3155e64acbb5cdcdfa8300c59f"
     }
    },
    "0f133181773c4a5d9c064e70490cb1a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1185f5a11d7542549cf8a41c4acdd299": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f021416b2548455fa473c8db203ba859",
      "placeholder": "​",
      "style": "IPY_MODEL_21d269efb3974a75878b9e6509572977",
      "value": " 2.42M/? [00:00&lt;00:00, 25.8MB/s]"
     }
    },
    "1b9d2ac8f45f4d62bbcb4b307e6f5ac7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1efc560ddf14432fbaeaa519acdfd1c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20085efd569c4472bab8e83c564e3cae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21970613bfcc44f08a9df53e6248e659": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21d269efb3974a75878b9e6509572977": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29bf2c450d194d2f87474872aa678428": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e5471fe4cc540638361ed5b7a8751dc",
      "max": 662,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_81ec678ad05f441eb97fd69311129888",
      "value": 662
     }
    },
    "33e714e8c37b4c329705829fdf4ebd17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3865538eb2784928b34f2c63690a0eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45151a137aa94317a841c56a848fe341": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "4d1d415f52a147ec90197eeba1533f8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa6d28a0dca443229e40ca6eec98ac6d",
       "IPY_MODEL_97817ff6586543968b0c2b47b38cce17",
       "IPY_MODEL_5f0b0dbc284f4ac4b234b962ce8bd719"
      ],
      "layout": "IPY_MODEL_602874d9b1cb45cdafe8547cefdb301e"
     }
    },
    "5104fe0fa1144f72a9d216932b8b3fb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53b06af55eea46339e6fcf87c3cf72ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a65ee1f601b4282a98e4a7d60b20429",
      "placeholder": "​",
      "style": "IPY_MODEL_33e714e8c37b4c329705829fdf4ebd17",
      "value": " 792k/792k [00:00&lt;00:00, 347kB/s]"
     }
    },
    "554e29fbfb874af7a0ae6ff85a4a19a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "577719ed33314ad4b9ebe851c8469829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de6570c38cd749cb8c95d7c7e9495a43",
      "placeholder": "​",
      "style": "IPY_MODEL_3865538eb2784928b34f2c63690a0eff",
      "value": "spiece.model: 100%"
     }
    },
    "57a1ad0ba0b04ef29a9886223fd889c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a705286e388a4cb4ac3323934f99a0fa",
      "placeholder": "​",
      "style": "IPY_MODEL_7fa7f9a88696449795df2309a4ddfe8b",
      "value": "tokenizer_config.json: "
     }
    },
    "5d675c0fe78b4acd8e72e8a40a60cf22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f0b0dbc284f4ac4b234b962ce8bd719": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3ecb98fec034b6aa5c0b18aae264f3e",
      "placeholder": "​",
      "style": "IPY_MODEL_ce861c38dd06484e854ead8fa98acf13",
      "value": " 147/147 [00:00&lt;00:00, 3.67kB/s]"
     }
    },
    "602874d9b1cb45cdafe8547cefdb301e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de677c870a8439a968de2ae0ee92516": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7276ee4304b1420cbb9e9662d8115b6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74a99421e3a545f3b683629789d71ed4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a65ee1f601b4282a98e4a7d60b20429": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fa7f9a88696449795df2309a4ddfe8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80884ff5b14e492d80035ee516517157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "818802eb2866440e98f05057ef485964": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a35c3693d5a64ff2ae71938b61e2c7d0",
      "placeholder": "​",
      "style": "IPY_MODEL_80884ff5b14e492d80035ee516517157",
      "value": "model.safetensors: 100%"
     }
    },
    "81ec678ad05f441eb97fd69311129888": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8774c5c350fb4025a904381cc4279d75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4e755663307409c964ecc36a035bc31",
       "IPY_MODEL_29bf2c450d194d2f87474872aa678428",
       "IPY_MODEL_b92bf32c6a6f485ebbd618aa3ed1a4fa"
      ],
      "layout": "IPY_MODEL_6de677c870a8439a968de2ae0ee92516"
     }
    },
    "88c0cf0084af4e96b6c0b82725a9689c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7541383256e4e428ac407c4d4249d0c",
      "placeholder": "​",
      "style": "IPY_MODEL_da9e7e87b742467283c7db476b909672",
      "value": " 3.13G/3.13G [01:00&lt;00:00, 76.0MB/s]"
     }
    },
    "8bbf43fdd7094d9ea7e47135269912ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8c49ede2caed452fb52c10087f50b445": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1efc560ddf14432fbaeaa519acdfd1c9",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e522f33846f449bea629e1b0428f9448",
      "value": 791656
     }
    },
    "8c5047dd6b794b1bb0fccb6ffcf3a7a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "97817ff6586543968b0c2b47b38cce17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd97861f615541d884f20bc467e421dc",
      "max": 147,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8bbf43fdd7094d9ea7e47135269912ee",
      "value": 147
     }
    },
    "9c8c9367e95a4760a662493b1d2b38a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e5471fe4cc540638361ed5b7a8751dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e9c8704318644469be030855d0c957e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2c5acd2a63545e995678980e9d8768e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f133181773c4a5d9c064e70490cb1a9",
      "placeholder": "​",
      "style": "IPY_MODEL_e5b5bbfe393a4376b2b406735ea20f85",
      "value": " 2.20k/? [00:00&lt;00:00, 42.0kB/s]"
     }
    },
    "a35c3693d5a64ff2ae71938b61e2c7d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3d80a91f5b54fff893e7a909b4644c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a409ce978edd4aa1bae2be48e9dfa0ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a705286e388a4cb4ac3323934f99a0fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8de03702c5b47e09af2f24c6e942124": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa6d28a0dca443229e40ca6eec98ac6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9d2ac8f45f4d62bbcb4b307e6f5ac7",
      "placeholder": "​",
      "style": "IPY_MODEL_21970613bfcc44f08a9df53e6248e659",
      "value": "generation_config.json: 100%"
     }
    },
    "ab40da8759e346c1954afd317d0cdb11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae50dc6fa98247e6b77c861bf991cea6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2d5c7ab8514404bbd085fd081ae66b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1da84accb5e469a9ed60462de51f3af",
      "placeholder": "​",
      "style": "IPY_MODEL_9e9c8704318644469be030855d0c957e",
      "value": "tokenizer.json: "
     }
    },
    "b5fe54e5760d44d0aa0d8476ad335ab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b98a79d88b884b10b6d4b2ba332c0ac0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a3d80a91f5b54fff893e7a909b4644c0",
      "value": 1
     }
    },
    "b92bf32c6a6f485ebbd618aa3ed1a4fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7276ee4304b1420cbb9e9662d8115b6e",
      "placeholder": "​",
      "style": "IPY_MODEL_20085efd569c4472bab8e83c564e3cae",
      "value": " 662/662 [00:00&lt;00:00, 68.8kB/s]"
     }
    },
    "b98a79d88b884b10b6d4b2ba332c0ac0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "bd97861f615541d884f20bc467e421dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfe10d44f2494bdc8718cac8e327fc15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45151a137aa94317a841c56a848fe341",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c049f4a3b77b4eb29a208d9d9da4eebb",
      "value": 1
     }
    },
    "c049f4a3b77b4eb29a208d9d9da4eebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cca0a5dd926d41eaa2c6f3f744139026": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c5047dd6b794b1bb0fccb6ffcf3a7a2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d675c0fe78b4acd8e72e8a40a60cf22",
      "value": 1
     }
    },
    "ce861c38dd06484e854ead8fa98acf13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0ece75094c948629efeafbf2506734f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c8c9367e95a4760a662493b1d2b38a5",
      "placeholder": "​",
      "style": "IPY_MODEL_a8de03702c5b47e09af2f24c6e942124",
      "value": " 2.54k/? [00:00&lt;00:00, 43.4kB/s]"
     }
    },
    "d1da84accb5e469a9ed60462de51f3af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7541383256e4e428ac407c4d4249d0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d773a45cdd0e478492d66400c4fe46fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_818802eb2866440e98f05057ef485964",
       "IPY_MODEL_ec872adf833c42dd86c354ca10b25c8a",
       "IPY_MODEL_88c0cf0084af4e96b6c0b82725a9689c"
      ],
      "layout": "IPY_MODEL_554e29fbfb874af7a0ae6ff85a4a19a2"
     }
    },
    "d9bc5c8b9869497da6ce1322c8a42010": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da9e7e87b742467283c7db476b909672": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de6570c38cd749cb8c95d7c7e9495a43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2e1c33b4bd94810b5159a23f35f4e39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3ecb98fec034b6aa5c0b18aae264f3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e522f33846f449bea629e1b0428f9448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e56fe58ecf824fc582d44a76cef146ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5b5bbfe393a4376b2b406735ea20f85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e81415e8e4cc453e9bc32de8a4a1f567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57a1ad0ba0b04ef29a9886223fd889c9",
       "IPY_MODEL_b5fe54e5760d44d0aa0d8476ad335ab2",
       "IPY_MODEL_d0ece75094c948629efeafbf2506734f"
      ],
      "layout": "IPY_MODEL_5104fe0fa1144f72a9d216932b8b3fb3"
     }
    },
    "ec872adf833c42dd86c354ca10b25c8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2e1c33b4bd94810b5159a23f35f4e39",
      "max": 3132668804,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab40da8759e346c1954afd317d0cdb11",
      "value": 3132668804
     }
    },
    "f021416b2548455fa473c8db203ba859": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4cbab3155e64acbb5cdcdfa8300c59f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e755663307409c964ecc36a035bc31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_072172f5a0974fc79b43977e284eec66",
      "placeholder": "​",
      "style": "IPY_MODEL_d9bc5c8b9869497da6ce1322c8a42010",
      "value": "config.json: 100%"
     }
    },
    "f666f6e2dce948478eba03bed880a5c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_577719ed33314ad4b9ebe851c8469829",
       "IPY_MODEL_8c49ede2caed452fb52c10087f50b445",
       "IPY_MODEL_53b06af55eea46339e6fcf87c3cf72ca"
      ],
      "layout": "IPY_MODEL_ae50dc6fa98247e6b77c861bf991cea6"
     }
    },
    "fa0dd004fc8f48f6912bcc8e0457fb7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a409ce978edd4aa1bae2be48e9dfa0ff",
      "placeholder": "​",
      "style": "IPY_MODEL_e56fe58ecf824fc582d44a76cef146ee",
      "value": "special_tokens_map.json: "
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
